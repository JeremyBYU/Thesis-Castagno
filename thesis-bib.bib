%% If I want to change to IEEE style references, I need this line to allow repeated authors names in citation list
%%  ------------ Use AT symbol here
%%  |
%%  â–¼
%%  %IEEEtranBSTCTL{IEEEexample:BSTcontrol,CTLdash_repeated_names = "no"}
%% Dont forget to clear cache as well, Look at log file and look for trash icon


@incollection{castagno_map-based_2021,
  title = {Map-{{Based Planning}} for {{Small Unmanned Aircraft Rooftop Landing}}},
  booktitle = {Handbook on {{Reinforcement Learning}} and {{Control}}},
  author = {Castagno, Jeremy and Atkins, Ella},
  year = {2021},
  publisher = {{Springer}},
  doi = {10.1007/978-3-030-60990-0},
}


@inproceedings{10.1145/37401.37422,
  title = {Marching Cubes: {{A}} High Resolution {{3D}} Surface Construction Algorithm},
  booktitle = {Proceedings of the 14th Annual Conference on Computer Graphics and Interactive Techniques},
  author = {Lorensen, William E. and Cline, Harvey E.},
  year = {1987},
  pages = {163--169},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/37401.37422},
  abstract = {We present a new algorithm, called marching cubes, that creates triangle models of constant density surfaces from 3D medical data. Using a divide-and-conquer approach to generate inter-slice connectivity, we create a case table that defines triangle topology. The algorithm processes the 3D medical data in scan-line order and calculates triangle vertices using linear interpolation. We find the gradient of the original data, normalize it, and use it as a basis for shading the models. The detail in images produced from the generated surface models is the result of maintaining the inter-slice connectivity, surface data, and gradient information present in the original 3D data. Results from computed tomography (CT), magnetic resonance (MR), and single-photon emission computed tomography (SPECT) illustrate the quality and functionality of marching cubes. We also discuss improvements that decrease processing time and add solid modeling capabilities.},
  isbn = {0-89791-227-6},
  series = {{{SIGGRAPH}} '87}
}




@inproceedings{10.1145/237170.237269,
  title = {A Volumetric Method for Building Complex Models from Range Images},
  booktitle = {Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques},
  author = {Curless, Brian and Levoy, Marc},
  year = {1996},
  pages = {303--312},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/237170.237269},
  isbn = {0-89791-746-4},
  keywords = {isosurface extraction,range image integration,surface fitting,three-dimensional shape recovery},
  series = {{{SIGGRAPH}} '96}
}




@article{castagno_polylidar3d_2020,
  title = {{{Polylidar3D}} - {{Fast Polygon Extraction}} from {{3D Data}}},
  author = {Castagno, Jeremy and Atkins, Ella},
  year = {2020},
  month = jan,
  volume = {20},
  pages = {4819},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/s20174819},
  abstract = {Flat surfaces captured by 3D point clouds are often used for localization, mapping, and modeling. Dense point cloud processing has high computation and memory costs making low-dimensional representations of flat surfaces such as polygons desirable. We present Polylidar3D, a non-convex polygon extraction algorithm which takes as input unorganized 3D point clouds (e.g., LiDAR data), organized point clouds (e.g., range images), or user-provided meshes. Non-convex polygons represent flat surfaces in an environment with interior cutouts representing obstacles or holes. The Polylidar3D front-end transforms input data into a half-edge triangular mesh. This representation provides a common level of abstraction for subsequent back-end processing. The Polylidar3D back-end is composed of four core algorithms: mesh smoothing, dominant plane normal estimation, planar segment extraction, and finally polygon extraction. Polylidar3D is shown to be quite fast, making use of CPU multi-threading and GPU acceleration when available. We demonstrate Polylidar3D\&rsquo;s versatility and speed with real-world datasets including aerial LiDAR point clouds for rooftop mapping, autonomous driving LiDAR point clouds for road surface detection, and RGBD cameras for indoor floor/wall detection. We also evaluate Polylidar3D on a challenging planar segmentation benchmark dataset. Results consistently show excellent speed and accuracy.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\TYKIGHBT\\Castagno and Atkins - 2020 - Polylidar3D-Fast Polygon Extraction from 3D Data.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\JPLQPZIJ\\htm.html},
  journal = {Sensors},
  keywords = {geometry,LiDAR,mapping,point cloud,polygon},
  language = {en},
  number = {17}
}




@article{bleier_risk_2015,
  title = {Risk {{Assessment}} of {{Flight Paths}} for {{Automatic Emergency Parachute Deployment}} in {{UAVs}}},
  author = {Bleier, Michael and Settele, Ferdinand and Krauss, Markus and Knoll, Alexander and Schilling, Klaus},
  year = {2015},
  month = jan,
  volume = {48},
  pages = {180--185},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2015.08.080},
  abstract = {This paper presents models for the risk assessment and generation of flight paths, which support the automatic deployment of emergency parachutes for unmanned aerial vehicles in emergencies due to loss of propulsion. Based on a risk analysis of the area underneath the flight path, suitable deployment positions are identified, which minimize the chance of endangering humans on the ground, property damage and loss of the air vehicle. Additionally, the flight path selection is guided by constraints, such as control data link availability or aircraft performance.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\Y88RFIJ4\\S2405896315009477.html},
  journal = {IFAC-PapersOnLine},
  keywords = {Autonomy,Decision making,Emergency planning,Trajectory generation,UAV},
  language = {en},
  number = {9},
  series = {1st {{IFAC Workshop}} on {{Advanced Control}} and {{Navigation}} for {{Autonomous Aerospace Vehicles ACNAAV}}'15}
}




@article{agarwal2002polygon,
  title = {Polygon Decomposition for Efficient Construction of {{Minkowski}} Sums},
  author = {Agarwal, Pankaj K and Flato, Eyal and Halperin, Dan},
  year = {2002},
  volume = {21},
  pages = {39--61},
  publisher = {{Elsevier}},
  journal = {Computational Geometry},
  number = {1-2}
}

@inproceedings{ahn_analysis_2019,
  title = {Analysis and {{Noise Modeling}} of the {{Intel RealSense D435}} for {{Mobile Robots}}},
  booktitle = {2019 16th {{International Conference}} on {{Ubiquitous Robots}} ({{UR}})},
  author = {Ahn, Min Sung and Chae, Hosik and Noh, Donghun and Nam, Hyunwoo and Hong, Dennis},
  year = {2019},
  month = jun,
  pages = {707--711},
  issn = {2325-033X},
  doi = {10.1109/URAI.2019.8768489},
  abstract = {Cameras that provide distance measurement along with RGB data have increasingly been appearing in the market as alternatives to the more expensive setup of LIDARs and webcams. While products such as the Kinect have existed in the past, its weight and form factor have been demanding constraints for mobile robots, specifically legged robots that are sensitive to payload. Recently Intel released a new lineup of Intel RealSense RGB-D cameras that have favorable characteristics for legged robots, specifically in terms of resolution, frames per second, form factor, weight, and price range. However, because these active stereo sensors are noisy for reasons such as non-overlapping image regions or lack of texture, it is beneficial to empirically model the noise. Systematic errors, specifically the distance inhomogeneity and depth bias, are observed to recognize and verify the limitations of the camera. We also analyze the non-systematic error by modeling both the axial and lateral noise as a function of distance and angle of incidence using a Gaussian distribution for its versatile applicability for mobile robots in mapping.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\UK9DGEAS\\Ahn et al. - 2019 - Analysis and Noise Modeling of the Intel RealSense.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\8FB669NG\\8768489.html},
  keywords = {axial noise,cameras,Cameras,d435,depth bias,distance inhomogeneity,distance measurement,form factor,Gaussian distribution,Image edge detection,image resolution,Intel RealSense D435,Intel RealSense RGB-D,lateral noise,legged locomotion,legged robots,mobile robots,Mobile robots,noise,noise modeling,nonoverlapping image regions,nonsystematic error,realsense,RGB data,robot vision,Robot vision systems,sensor,Sensor phenomena and characterization,stereo image processing,Webcams}
}

@article{alidoost_knowledge_2016,
  title = {Knowledge {{Based 3D Building Model Recognition Using Convolutional Neural Networks From Lidar}} and {{Aerial Imageries}}},
  author = {Alidoost, F. and Arefi, H.},
  year = {2016},
  volume = {XLI-B3},
  pages = {833--840},
  doi = {10.5194/isprs-archives-XLI-B3-833-2016},
  journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences}
}

@inproceedings{amdahl_validity_1967,
  title = {Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities},
  booktitle = {Proceedings of the {{April}} 18-20, 1967, Spring Joint Computer Conference},
  author = {Amdahl, Gene M.},
  year = {1967},
  month = apr,
  pages = {483--485},
  publisher = {{Association for Computing Machinery}},
  address = {{Atlantic City, New Jersey}},
  doi = {10.1145/1465482.1465560},
  abstract = {For over a decade prophets have voiced the contention that the organization of a single computer has reached its limits and that truly significant advances can be made only by interconnection of a multiplicity of computers in such a manner as to permit cooperative solution. Variously the proper direction has been pointed out as general purpose computers with a generalized interconnection of memories, or as specialized computers with geometrically related memory interconnections and controlled by one or more instruction streams.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\5FRI8MKU\\Amdahl - 1967 - Validity of the single processor approach to achie.pdf},
  isbn = {978-1-4503-7895-6},
  keywords = {parallelism},
  series = {{{AFIPS}} '67 ({{Spring}})}
}

@inproceedings{ancel_real-time_2017,
  title = {Real-Time {{Risk Assessment Framework}} for {{Unmanned Aircraft System}} ({{UAS}}) {{Traffic Management}} ({{UTM}})},
  booktitle = {17th {{AIAA Aviation Technology}}, {{Integration}}, and {{Operations Conference}}},
  author = {Ancel, Ersin and Capristan, Francisco M. and Foster, John V. and Condotta, Ryan C.},
  year = {2017},
  month = jun,
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/6.2017-3273}
}

@inproceedings{assouline_building_2017,
  title = {Building Rooftop Classification Using Random Forests for Large-Scale {{PV}} Deployment},
  booktitle = {Earth {{Resources}} and {{Environmental Remote Sensing}}/{{GIS Applications VIII}}},
  author = {Assouline, Dan and Mohajeri, Nahid and Scartezzini, Jean-Louis},
  year = {2017},
  month = oct,
  volume = {10428},
  pages = {1042806},
  publisher = {{International Society for Optics and Photonics}},
  doi = {10.1117/12.2277692},
  abstract = {Large scale solar Photovoltaic (PV) deployment on existing building rooftops has proven to be one of the most efficient and viable sources of renewable energy in urban areas. As it usually requires a potential analysis over the area of interest, a crucial step is to estimate the geometric characteristics of the building rooftops. In this paper, we introduce a multi-layer machine learning methodology to classify 6 roof types, 9 aspect (azimuth) classes and 5 slope (tilt) classes for all building rooftops in Switzerland, using GIS processing. We train Random Forests (RF), an ensemble learning algorithm, to build the classifiers. We use (2 \&times; 2) [m\textsuperscript{2} ] LiDAR data (considering buildings and vegetation) to extract several rooftop features, and a generalised footprint polygon data to localize buildings. The roof classifier is trained and tested with 1252 labeled roofs from three different urban areas, namely Baden, Luzern, and Winterthur. The results for roof type classification show an average accuracy of 67\%. The aspect and slope classifiers are trained and tested with 11449 labeled roofs in the Zurich periphery area. The results for aspect and slope classification show different accuracies depending on the classes: while some classes are well identified, other under-represented classes remain challenging to detect.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\JTI6LA5F\\Assouline et al. - 2017 - Building rooftop classification using random fores.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\9XW44LZR\\12.2277692.html}
}

@article{atkins_emergency_2006,
  title = {Emergency {{Flight Planning Applied}} to {{Total Loss}} of {{Thrust}}},
  author = {Atkins, Ella M. and Portillo, Igor Alonso and Strube, Matthew J.},
  year = {2006},
  month = jul,
  volume = {43},
  pages = {1205--1216},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/1.18816},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\R7DU3GUD\\Atkins et al. - 2006 - Emergency Flight Planning Applied to Total Loss of.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\D9VEFGEC\\1.html},
  journal = {Journal of Aircraft},
  number = {4}
}

@article{balsa-barreiro_airborne_2012,
  title = {Airborne Light Detection and Ranging ({{LiDAR}}) Point Density Analysis},
  author = {{Balsa-Barreiro}, Jos{\'e} and Avariento, Joan P. and Lerma, Jos{\'e} Luis},
  year = {2012},
  volume = {7},
  pages = {3010--3019},
  doi = {10.5897/SRE12.278},
  abstract = {The point density is a preeminent parameter on airborne laser scanner surveys. It is not only related to accuracy but costs and savings. The lack of uniformity of the point density across the survey is well-known in the scientific community. This paper analyzes the behaviour of the point density derived by an oscillating mirror laser scanner on different single strips on flat bare ground in order to estimate a meaningful mean density value. The variation of the point density at both extreme ends of the oscillating mirror scan is meaningful. It will be demonstrated that excluding the extreme sectors across the strip corresponding to 1/8 of the swath width (12.5\% of the sampling area, half in each side) for the computation of the mean density value is enough to satisfy light detection and ranging (LiDAR) specifications for national level surveys.     ~     Key words:~Light detection and ranging (LiDAR), point density, point distribution.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\WBI6DVYU\\Balsa-Barreiro et al. - 2012 - Airborne light detection and ranging (LiDAR) point.pdf},
  journal = {Academic Journals}
}

@article{balsa-barreiro_generation_2018,
  title = {Generation of Visually Aesthetic and Detailed {{3D}} Models of Historical Cities by Using Laser Scanning and Digital Photogrammetry},
  author = {{Balsa-Barreiro}, Jos{\'e} and Fritsch, Dieter},
  year = {2018},
  month = mar,
  volume = {8},
  pages = {57--64},
  issn = {2212-0548},
  doi = {10.1016/j.daach.2017.12.001},
  abstract = {Historical cities (or towns) are inherently valuable for their uniqueness. Although reflecting the past, these cities offer significant opportunities. For this reason, it is important to know how to preserve them and assess their exploitation potential from different perspectives. An increased knowledge about them is continuously required and demanded. Three-dimensional (henceforth 3D) virtual models are an excellent way for introducing these cities to people. However, some particular characteristics derived from irregular urban structures and specific human dynamics in historical urban environments can constrain the use of some technologies for 3D data collection. This research paper proposes an own working methodology for the generation of 3D virtual models of historical cities. This methodology is based on the combined use of laser scanning and photogrammetric techniques, which complement each other. As a result, a 3D virtual model with high geometric accuracy and visual completeness is obtained and integrated into a web-based application. Resulting virtual models can be used for touristic promotion, navigation, urban planning, documentation and preservation of cultural heritage, etc.},
  journal = {Digital Applications in Archaeology and Cultural Heritage},
  keywords = {3D virtual city,Historical city,Laser scanning,Level of detail; photogrammetry,Photorealistic visualization},
  language = {en}
}

@inproceedings{bergelt_improving_2017,
  title = {Improving the Intrinsic Calibration of a {{Velodyne LiDAR}} Sensor},
  booktitle = {2017 {{IEEE SENSORS}}},
  author = {Bergelt, Rene and Khan, Owes and Hardt, Wolfram},
  year = {2017},
  month = oct,
  pages = {1--3},
  doi = {10.1109/ICSENS.2017.8234357},
  abstract = {LiDAR (Light detection and ranging) sensors are widely used in research and development. As such, they build the base for the evaluation of newly developed ADAS (Advanced Driver Assistance Systems) functions in the automotive field where they are used for ground truth establishment. However, the factory calibration provided for the sensors is not able to satisfy the high accuracy requirements by such applications. In this paper we propose a concept to easily improve the existing calibration of a Velodyne LiDAR sensor without the need for special calibration setups which can even be used to enhance already recorded data.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\M2ERDDAQ\\Bergelt et al. - 2017 - Improving the intrinsic calibration of a Velodyne .pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\JM2ZMHLH\\8234357.html},
  keywords = {ADAS,Advanced Driver Assistance Systems,automotive field,calibration,Calibration,crispness,driver information systems,ground truth establishment,intrinsic calibration,Laser radar,LiDAR,light detection-and-ranging sensors,Minimization,optical radar,optical sensors,Optimization,point cloud,Production facilities,Robot sensing systems,Three-dimensional displays,Velodyne LiDAR sensor}
}

@article{bernardini_ball-pivoting_1999,
  title = {The Ball-Pivoting Algorithm for Surface Reconstruction},
  author = {Bernardini, F. and Mittleman, J. and Rushmeier, H. and Silva, C. and Taubin, G.},
  year = {1999},
  month = oct,
  volume = {5},
  pages = {349--359},
  issn = {1941-0506},
  doi = {10.1109/2945.817351},
  abstract = {The Ball-Pivoting Algorithm (BPA) computes a triangle mesh interpolating a given point cloud. Typically, the points are surface samples acquired with multiple range scans of an object. The principle of the BPA is very simple: Three points form a triangle if a ball of a user-specified radius p touches them without containing any other point. Starting with a seed triangle, the ball pivots around an edge (i.e., it revolves around the edge while keeping in contact with the edge's endpoints) until it touches another point, forming another triangle. The process continues until all reachable edges have been tried, and then starts from another seed triangle, until all points have been considered. The process can then be repeated with a ball of larger radius to handle uneven sampling densities. We applied the BPA to datasets of millions of points representing actual scans of complex 3D objects. The relatively small amount of memory required by the BPA, its time efficiency, and the quality of the results obtained compare favorably with existing techniques.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\4CXCKY77\\Bernardini et al. - 1999 - The ball-pivoting algorithm for surface reconstruc.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\R6W5F39I\\817351.html},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  keywords = {ball-pivoting algorithm,computational geometry,Data acquisition,Geometrical optics,image reconstruction,Image reconstruction,mesh,mesh generation,Product design,Sampling methods,seed triangle,Shape,surface reconstruction,Surface reconstruction,surface samples,Three-dimensional displays,triangle mesh},
  number = {4}
}

@inproceedings{biswas_planar_2012,
  title = {Planar Polygon Extraction and Merging from Depth Images},
  booktitle = {2012 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Biswas, Joydeep and Veloso, Manuela},
  year = {2012},
  month = oct,
  pages = {3859--3864},
  issn = {2153-0858},
  doi = {10.1109/IROS.2012.6385841},
  abstract = {There has been considerable interest recently in building 3D maps of environments using inexpensive depth cameras like the Microsoft Kinect sensor. We exploit the fact that typical indoor scenes have an abundance of planar features by modeling environments as sets of plane polygons. To this end, we build upon the Fast Sampling Plane Filtering (FSPF) algorithm that extracts points belonging to local neighborhoods of planes from depth images, even in the presence of clutter. We introduce an algorithm that uses the FSPF-generated plane filtered point clouds to generate convex polygons from individual observed depth images. We then contribute an approach of merging these detected polygons across successive frames while accounting for a complete history of observed plane filtered points without explicitly maintaining a list of all observed points. The FSPF and polygon merging algorithms run in real time at full camera frame rates with low CPU requirements: in a real world indoor environment scene, the FSPF and polygon merging algorithms take 2.5 ms on average to process a single 640 \texttimes{} 480 depth image. We provide experimental results demonstrating the computational efficiency of the algorithm and the accuracy of the detected plane polygons by comparing with ground truth.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\V8VUWFXM\\Biswas and Veloso - 2012 - Planar polygon extraction and merging from depth i.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\8RDKQN64\\6385841.html},
  keywords = {3D map,cameras,Cameras,Clutter,convex polygon generation,depth camera,depth image,fast sampling plane filtering algorithm,feature extraction,Feature extraction,filtering theory,image sampling,Indoor environments,indoor scene,Merging,mesh,Microsoft Kinect sensor,planar polygon extraction,planar polygon merging,plane polygon,range images,sensors,Simultaneous localization and mapping,Solid modeling}
}

@misc{blanco_nanoflann_2014,
  title = {Nanoflann: A {{C}}++ Header-Only Fork of {{FLANN}}, a Library for {{Nearest Neighbor}} ({{NN}}) with {{KD}}-Trees},
  author = {Blanco, Jose Luis and Rai, Pranjal Kumar},
  year = {2014}
}

@article{borrmann_3d_2011,
  title = {The {{3D Hough Transform}} for Plane Detection in Point Clouds: {{A}} Review and a New Accumulator Design},
  shorttitle = {The {{3D Hough Transform}} for Plane Detection in Point Clouds},
  author = {Borrmann, Dorit and Elseberg, Jan and Lingemann, Kai and N{\"u}chter, Andreas},
  year = {2011},
  month = jun,
  volume = {2},
  pages = {3},
  issn = {2092-6731},
  doi = {10.1007/3DRes.02(2011)3},
  abstract = {The Hough Transform is a well-known method for detecting parameterized objects. It is the de facto standard for detecting lines and circles in 2-dimensional data sets. For 3D it has attained little attention so far. Even for the 2D case high computational costs have lead to the development of numerous variations for the Hough Transform. In this article we evaluate different variants of the Hough Transform with respect to their applicability to detect planes in 3D point clouds reliably. Apart from computational costs, the main problem is the representation of the accumulator. Usual implementations favor geometrical objects with certain parameters due to uneven sampling of the parameter space. We present a novel approach to design the accumulator focusing on achieving the same size for each cell and compare it to existing designs.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\BVWMZTQH\\Borrmann et al. - 2011 - The 3D Hough Transform for plane detection in poin.pdf},
  journal = {3D Research},
  language = {en},
  number = {2}
}

@article{canziani_analysis_2017,
  title = {An {{Analysis}} of {{Deep Neural Network Models}} for {{Practical Applications}}},
  author = {Canziani, Alfredo and Paszke, Adam and Culurciello, Eugenio},
  year = {2017},
  month = apr,
  abstract = {Since the emergence of Deep Neural Networks (DNNs) as a prominent technique in the field of computer vision, the ImageNet classification challenge has played a major role in advancing the state-of-the-art. While accuracy figures have steadily increased, the resource utilisation of winning models has not been properly taken into account. In this work, we present a comprehensive analysis of important metrics in practical applications: accuracy, memory footprint, parameters, operations count, inference time and power consumption. Key findings are: (1) power consumption is independent of batch size and architecture; (2) accuracy and inference time are in a hyperbolic relationship; (3) energy constraint is an upper bound on the maximum achievable accuracy and model complexity; (4) the number of operations is a reliable estimate of the inference time. We believe our analysis provides a compelling set of information that helps design and engineer efficient DNNs.},
  archivePrefix = {arXiv},
  eprint = {1605.07678},
  eprinttype = {arxiv},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\45QMWWWY\\Canziani et al. - 2017 - An Analysis of Deep Neural Network Models for Prac.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\PAAIIU2K\\1605.html},
  journal = {arXiv:1605.07678 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{cao_roof_2017,
  title = {Roof Plane Extraction from Airborne Lidar Point Clouds},
  author = {Cao, Rujun and Zhang, Yongjun and Liu, Xinyi and Zhao, Zongze},
  year = {2017},
  month = jun,
  volume = {38},
  pages = {3684--3703},
  issn = {0143-1161},
  doi = {10.1080/01431161.2017.1302112},
  abstract = {Planar patches are important primitives for polyhedral building models. One of the key challenges for successful reconstruction of three-dimensional (3D) building models from airborne lidar point clouds is achieving high quality recognition and segmentation of the roof planar points. Unfortunately, the current automatic extraction processes for planar surfaces continue to suffer from limitations such as sensitivity to the selection of seed points and the lack of computational efficiency. In order to address these drawbacks, a new fully automatic segmentation method is proposed in this article, which is capable of the following: (1) processing a roof point dataset with an arbitrary shape; (2) robustly selecting the seed points in a parameter space with reduced dimensions; and (3) segmenting the planar patches in a sub-dataset with similar attributes when region growing in the object space. The detection of seed points in the parameter space was improved by mapping the accumulator array to a 1D space. The range for region growing in the object space was reduced by an attribute similarity measure that split the roof dataset into candidate and non-candidate subsets. The experimental results confirmed that the proposed approach can extract planar patches of building roofs robustly and efficiently.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\AJRT4ZWR\\Cao et al. - 2017 - Roof plane extraction from airborne lidar point cl.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\23UFS5YS\\01431161.2017.html},
  journal = {International Journal of Remote Sensing},
  keywords = {plane,point cloud,roof},
  number = {12}
}

@incollection{castagno_automatic_2018,
  title = {Automatic {{Classification}} of {{Roof Shapes}} for {{Multicopter Emergency Landing Site Selection}}},
  booktitle = {2018 {{Aviation Technology}}, {{Integration}}, and {{Operations Conference}}},
  author = {Castagno, Jeremy and Atkins, Ella M.},
  year = {2018},
  month = jun,
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/6.2018-3977},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\G9WIKSTP\\Castagno and Atkins - 2018 - Automatic Classification of Roof Shapes for Multic.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\5FHHSGAU\\6.html},
  series = {{{AIAA AVIATION Forum}}}
}

@inproceedings{castagno_comprehensive_2018,
  title = {Comprehensive {{Risk}}-Based {{Planning}} for {{Small Unmanned Aircraft System Rooftop Landing}}},
  booktitle = {2018 {{International Conference}} on {{Unmanned Aircraft Systems}} ({{ICUAS}})},
  author = {Castagno, Jeremy and Ochoa, Cosme and Atkins, Ella},
  year = {2018},
  month = jun,
  pages = {1031--1040},
  issn = {2575-7296},
  doi = {10.1109/ICUAS.2018.8453483},
  abstract = {The expected proliferation of Unmanned Aircraft Systems (UAS) has prompted many to question their safety and reliability, particularly in urban areas. Failures and anomalies can lead to the need for emergency landing, which in turn requires the UAS operator or autonomy to rapidly identify and evaluate the risks for possible landing sites and trajectories to reach these sites. This paper proposes a method to optimize the overall emergency landing site and flight path risks. Although sensors can scan an immediate area, no safe site might be observable in which case pre-processed data on more distant safe sites is required. For example, in urban regions, out of sight flat rooftops may pose less risk to people and property than landing in streets or sidewalks. This paper proposes the offline construction of a landing site database using a variety of public data sources, uniquely allowing for the assessment of risk associated with a rooftop landing. A real-time map-based planner is presented that demonstrates a novel trade-off between landing site risk and path risk and provides a heuristic to improve decision-making efficiency.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\EW67FTSP\\Castagno et al. - 2018 - Comprehensive Risk-based Planning for Small Unmann.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\KDKDKP3A\\8453483.html},
  keywords = {Aircraft,autonomous aerial vehicles,Buildings,case pre-processed data,comprehensive risk-based,Databases,decision making,distant safe sites,emergency landing site,expected proliferation,flight path risks,landing site database,path planning,path risk,Planning,possible landing sites,real-time map-based planner,risk management,safe site,Sensors,Shape,sight flat rooftops,Sociology,UAS,unmanned aircraft system rooftop landing,unmanned aircraft systems,urban areas,urban regions}
}

@misc{Castagno_Github_fastga,
  title = {Github - {{Fast Gaussian Sphere Accumulator}}},
  author = {Castagno, Jeremy},
  year = {2020},
  howpublished = {[Online] Available: \url{https://github.com/JeremyBYU/FastGaussianAccumulator}},
  note = {Accessed: 2020-06-05}
}

@misc{Castagno_Github_opf,
  title = {Github - {{Organized Point Filters}}},
  author = {Castagno, Jeremy},
  year = {2020},
  howpublished = {[Online] Available: \url{https://github.com/JeremyBYU/OrganizedPointFilters}},
  note = {Accessed: 2020-06-05}
}

@misc{Castagno_Github_Polylidar,
  title = {Github - {{Polylidar}}},
  author = {Castagno, Jeremy},
  year = {2020},
  howpublished = {[Online] Available: \url{https://github.com/JeremyBYU/polylidar}},
  note = {Accessed: 2019-01-05}
}

@misc{Castagno_Github_Polylidar_Synpeb,
  title = {Github - {{Polylidard3D}} and {{SynPEB}}},
  author = {Castagno, Jeremy},
  year = {2020},
  howpublished = {[Online] Available: \url{https://github.com/JeremyBYU/polylidar-plane-benchmark}},
  note = {Accessed: 2020-06-05}
}

@misc{Castagno_Github_Polylidar3D_Kitti,
  title = {Github - {{Polylidard3D}} and {{KITTI}}},
  author = {Castagno, Jeremy},
  year = {2020},
  howpublished = {[Online] Available: \url{https://github.com/JeremyBYU/polylidar-kitti}},
  note = {Accessed: 2020-06-05}
}

@misc{Castagno_Github_Polylidar3D_RealSense,
  title = {Github - {{Polylidar3D}} with {{RealSense}}},
  author = {Castagno, Jeremy},
  year = {2020},
  howpublished = {[Online] Available: \url{https://github.com/JeremyBYU/polylidar-realsense}},
  note = {Accessed: 2020-06-05}
}

@article{castagno_multi-uav_nodate,
  title = {Multi-{{UAV Wildire Boundary Estimation}} Using a {{Semantic Segmentation Neural Network}}},
  author = {Castagno, Jeremy and Romano, Matthew and Kuevor, Prince and Atkins, Ella},
  journal = {Journal of Aerospace Information Systems},
  note = {Submitted and under review}
}

@article{castagno_polylidar_2020,
  title = {Polylidar - {{Polygons From Triangular Meshes}}},
  author = {Castagno, Jeremy and Atkins, Ella},
  year = {2020},
  month = jul,
  volume = {5},
  pages = {4634--4641},
  issn = {2377-3766},
  doi = {10.1109/LRA.2020.3002212},
  abstract = {This letter presents Polylidar, an efficient algorithm to extract non-convex polygons from 2D point sets, including interior holes. Plane segmented point clouds can be input into Polylidar to extract their polygonal counterpart, thereby reducing map size and improving visualization. The algorithm begins by triangulating the point set and filtering triangles by user configurable parameters such as triangle edge length. Next, connected triangles are extracted into triangular mesh regions representing the shape of the point set. Finally each region is converted to a polygon through a novel boundary following method which accounts for holes. Real-world and synthetic benchmarks are presented to comparatively evaluate Polylidar speed and accuracy. Results show comparable accuracy and more than four times speedup compared to other concave polygon extraction methods.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\TG4K383P\\Castagno and Atkins - 2020 - Polylidar - Polygons From Triangular Meshes.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\BP5PTJD8\\9117017.html},
  journal = {IEEE Robotics and Automation Letters},
  keywords = {2D point sets,Aerial systems: perception and autonomy,Benchmark testing,boundary following method,computational geometry,concave polygon extraction methods,connected triangles,Data mining,Data structures,data visualisation,improving visualization,Indexes,interior holes,map size,mesh generation,nonconvex polygons,point clouds,polygonal counterpart,polylidar,reactive and sensor-based planning,Shape,Three-dimensional displays,triangle edge length,triangular mesh regions,triangular meshes,Two dimensional displays,user configurable parameters},
  number = {3}
}

@article{castagno_roof_2018,
  title = {Roof {{Shape Classification}} from {{LiDAR}} and {{Satellite Image Data Fusion Using Supervised Learning}}},
  author = {Castagno, Jeremy and Atkins, Ella},
  year = {2018},
  month = nov,
  volume = {18},
  pages = {3960},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/s18113960},
  abstract = {Geographic information systems (GIS) provide accurate maps of terrain, roads, waterways, and building footprints and heights. Aircraft, particularly small unmanned aircraft systems (UAS), can exploit this and additional information such as building roof structure to improve navigation accuracy and safely perform contingency landings particularly in urban regions. However, building roof structure is not fully provided in maps. This paper proposes a method to automatically label building roof shape from publicly available GIS data. Satellite imagery and airborne LiDAR data are processed and manually labeled to create a diverse annotated roof image dataset for small to large urban cities. Multiple convolutional neural network (CNN) architectures are trained and tested, with the best performing networks providing a condensed feature set for support vector machine and decision tree classifiers. Satellite image and LiDAR data fusion is shown to provide greater classification accuracy than using either data type alone. Model confidence thresholds are adjusted leading to significant increases in models precision. Networks trained from roof data in Witten, Germany and Manhattan (New York City) are evaluated on independent data from these cities and Ann Arbor, Michigan.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\IW93TSG4\\Castagno and Atkins - 2018 - Roof Shape Classification from LiDAR and Satellite.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\JJSQN3DG\\3960.html},
  journal = {Sensors},
  keywords = {drones,geographical information system (GIS),LiDAR,machine learning,machine vision,maps,safety,unmanned aircraft systems (UAS)},
  language = {en},
  number = {11}
}

@misc{chollet_keras_2015,
  title = {Github - Keras},
  author = {Chollet, Francois and others},
  year = {2015},
  howpublished = {[Online] Available: \url{https://github.com/keras-team/keras}},
  note = {Accessed: 2018-01-01}
}

@article{cohen_gauge_2019,
  title = {Gauge {{Equivariant Convolutional Networks}} and the {{Icosahedral CNN}}},
  author = {Cohen, Taco S. and Weiler, Maurice and Kicanaoglu, Berkay and Welling, Max},
  year = {2019},
  month = may,
  abstract = {The principle of equivariance to symmetry transformations enables a theoretically grounded approach to neural network architecture design. Equivariant networks have shown excellent performance and data efficiency on vision and medical imaging problems that exhibit symmetries. Here we show how this principle can be extended beyond global symmetries to local gauge transformations. This enables the development of a very general class of convolutional neural networks on manifolds that depend only on the intrinsic geometry, and which includes many popular methods from equivariant and geometric deep learning.},
  archivePrefix = {arXiv},
  eprint = {1902.04615},
  eprinttype = {arxiv},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\ECEJBS7C\\Cohen et al. - 2019 - Gauge Equivariant Convolutional Networks and the I.pdf},
  journal = {arXiv:1902.04615 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  note = {Comment: Proceedings of the International Conference on Machine Learning (ICML), 2019},
  primaryClass = {cs, stat}
}

@article{cohen_spherical_2018-1,
  title = {Spherical {{CNNs}}},
  author = {Cohen, Taco S. and Geiger, Mario and Koehler, Jonas and Welling, Max},
  year = {2018},
  month = feb,
  abstract = {Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest have created a demand for models that can analyze spherical images. Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective. In this paper we introduce the building blocks for constructing spherical CNNs. We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant. The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression.},
  archivePrefix = {arXiv},
  eprint = {1801.10130},
  eprinttype = {arxiv},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\JKS798L3\\Cohen et al. - 2018 - Spherical CNNs.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\FX52WH4U\\1801.html},
  journal = {arXiv:1801.10130 [cs, stat]},
  keywords = {cnn,Computer Science - Machine Learning,harmonics,Statistics - Machine Learning},
  note = {Comment: Proceedings of the 6th International Conference on Learning Representations (ICLR), 2018},
  primaryClass = {cs, stat}
}

@article{coombes_reachability_2014,
  title = {Reachability {{Analysis}} of {{Landing Sites}} for {{Forced Landing}} of a {{UAS}}},
  author = {Coombes, Matthew and Chen, Wen-Hua and Render, Peter},
  year = {2014},
  month = jan,
  volume = {73},
  pages = {635--653},
  issn = {1573-0409},
  doi = {10.1007/s10846-013-9920-9},
  abstract = {This paper details a method to ascertain the reachability of known emergency landing sites for any fixed wing aircraft in a forced landing situation. With a knowledge of the aircraft's state and parameters, as well as a known wind profile, the area of maximum glide range can be calculated using aircraft equations of motion for gliding flight. A landing descent circuit technique used by human pilots carrying out forced landings called high key low key is employed to account for the extra glide distance required for an approach and landing. By combining maximum glide range analysis with the descent circuit, all the reachable landing sites can be determined. X-Plane flight simulator is used to demonstrate and validate the techniques presented.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\9L6S2FFI\\Coombes et al. - 2014 - Reachability Analysis of Landing Sites for Forced .pdf},
  journal = {Journal of Intelligent \& Robotic Systems},
  language = {en},
  number = {1}
}

@article{dagum_openmp_1998,
  title = {{{OpenMP}}: An Industry Standard {{API}} for Shared-Memory Programming},
  author = {Dagum, Leonardo and Menon, Ramesh},
  year = {1998},
  volume = {5},
  pages = {46--55},
  publisher = {{IEEE}},
  journal = {Computational Science \& Engineering, IEEE},
  number = {1}
}

@article{de_berg_delaunay_2008,
  title = {Delaunay {{Triangulations}}: {{Height Interpolation}}},
  author = {{de Berg}, Mark and Cheong, Otfried and {van Kreveld}, Marc and Overmars, Mark},
  year = {2008},
  pages = {191--218},
  journal = {Computational Geometry: Algorithms and Applications}
}

@techreport{degarmo_issues_2013,
  title = {Issues {{Concerning Integration}} of {{Unmanned Aerial Vehicles}} in {{Civil Airspace}}},
  author = {DeGarmo, Matthew T.},
  year = {2013},
  month = sep,
  institution = {{The Mitre Corporation}},
  abstract = {Interest in Unmanned Aerial Vehicles (UAVs) is growing worldwide and several efforts are underway to integrate UAV operations routinely and safely into civil airspace. Currently, UAV operations are confined to special-use airspace or are limited in their access, for safety reasons, by a restrictive authorization process. This document provides a context of UAV developments, describes current initiatives, and frames and assesses the issues associated with the integration of UAVs in civil airspace.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\KKS8JB6F\\DeGarmo - 2013 - Issues Concerning Integration of Unmanned Aerial V.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\67PYWZ2P\\issues-concerning-integration-of-unmanned-aerial-vehicles-in-civil-airspace.html},
  language = {en}
}

@article{desaraju_vision-based_2015,
  title = {Vision-Based Landing Site Evaluation and Informed Optimal Trajectory Generation toward Autonomous Rooftop Landing},
  author = {Desaraju, Vishnu R. and Michael, Nathan and Humenberger, Martin and Brockers, Roland and Weiss, Stephan and Nash, Jeremy and Matthies, Larry},
  year = {2015},
  month = oct,
  volume = {39},
  pages = {445--463},
  issn = {1573-7527},
  doi = {10.1007/s10514-015-9456-x},
  abstract = {Autonomous landing is an essential function for micro air vehicles (MAVs) for many scenarios. We pursue an active perception strategy that enables MAVs with limited onboard sensing and processing capabilities to concurrently assess feasible rooftop landing sites with a vision-based perception system while generating trajectories that balance continued landing site assessment and the requirement to provide visual monitoring of an interest point. The contributions of the work are twofold: (1) a perception system that employs a dense motion stereo approach that determines the 3D model of the captured scene without the need of geo-referenced images, scene geometry constraints, or external navigation aids; and (2) an online trajectory generation approach that balances the need to concurrently explore available rooftop vantages of an interest point while ensuring confidence in the landing site suitability by considering the impact of landing site uncertainty as assessed by the perception system. Simulation and experimental evaluation of the performance of the perception and trajectory generation methodologies are analyzed independently and jointly in order to establish the efficacy and robustness of the proposed approach.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\K9DI52ME\\Desaraju et al. - 2015 - Vision-based landing site evaluation and informed .pdf},
  journal = {Autonomous Robots},
  language = {en},
  number = {3}
}

@article{di_donato_evaluating_2017,
  title = {Evaluating {{Risk}} to {{People}} and {{Property}} for {{Aircraft Emergency Landing Planning}}},
  author = {Di Donato, Pedro F. A. and Atkins, Ella M.},
  year = {2017},
  volume = {14},
  pages = {259--278},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/1.I010513},
  abstract = {General aviation and small unmanned aircraft systems are less redundant, may be less thoroughly tested, and are flown at lower cruise altitudes than commercial aviation counterparts. These factors result in a higher probability of a forced or emergency landing scenario. Currently, general aviation relies on the pilot to select a landing site and plan a trajectory, even though workload in an emergency is typically high, and decisions must be made rapidly. Although sensors can provide local real-time information, awareness of more distant or occluded regions requires database and/or offboard data sources. This paper considers different data sources and how to process these data to inform an emergency landing planner regarding risks posed to property, people on the ground, and the aircraft itself. Detailed terrain data are used for selection of candidate emergency landing sites. Mobile phone activity is evaluated as a means of real-time occupancy estimation. Occupancy estimates are combined with population census data to estimate emergency landing risk to people on the ground. Openly available databases are identified and mined as part of an emergency landing planning case study.},
  annotation = {\_eprint: https://doi.org/10.2514/1.I010513},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\2QUG9GZQ\\Di Donato and Atkins - 2017 - Evaluating Risk to People and Property for Aircraf.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\CPKWQ2PE\\1.html;C\:\\Users\\Jeremy\\Zotero\\storage\\RP5L8A6V\\1.html},
  journal = {Journal of Aerospace Information Systems},
  number = {5}
}

@article{dmowska_high_2017-1,
  title = {A {{High Resolution Population Grid}} for the {{Conterminous United States}}: {{The}} 2010 Edition},
  shorttitle = {A High Resolution Population Grid for the Conterminous {{United States}}},
  author = {Dmowska, Anna and Stepinski, Tomasz F.},
  year = {2017},
  month = jan,
  volume = {61},
  pages = {13--23},
  issn = {0198-9715},
  doi = {10.1016/j.compenvurbsys.2016.08.006},
  abstract = {Readily available high resolution data on population distribution is an important resource for monitoring human-environment interactions and for supporting planning and management decisions. Using a grid that approximates population density over the entire country seems like the most practical approach to exploring and distributing detailed population data but instead data based on census aggregation units is still the most widely used method. In this paper we describe the construction of 30m resolution grid representing the distribution of population in 2010 over the entire conterminous United States. The grid is computed using 2010 U.S. Census block level population counts disaggregated by a dasymetric model that uses land cover (2011 NLCD) and land use (2010 NLUD) as ancillary data. Detailed descriptions of the ancillary data and dasymetric model are given. Methods of computing the grid are presented followed by an extensive assessment of model accuracy. Overall the expected value for relative error of the model is 44\% which is at the lower limit of errors reported for other continental-sized, high resolution population grids. We also offer a more specific error estimate for areas with specified value of population density. Using two example areas, one highly urbanized and another rural, we demonstrate the advantages of using the gridded population data over the census block-based data. Our 30m population grid is available for online exploration and for download from the custom-made GeoWeb application SocScape at http://sil.uc.edu.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\QHE3GZ7C\\Dmowska and Stepinski - 2017 - A high resolution population grid for the contermi.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\6D5E57V6\\S0198971516301983.html},
  journal = {Computers, Environment and Urban Systems},
  keywords = {census,Census,Dasymetric modeling,Gridded population data,Land use,NLCD},
  language = {en}
}

@article{douglas_algorithms_1973,
  title = {Algorithms for the Reduction of the Number of Points Required to Represent a Digitized Line or Its Caricature},
  author = {Douglas, David H and Peucker, Thomas K},
  year = {1973},
  month = dec,
  volume = {10},
  pages = {112--122},
  publisher = {{University of Toronto Press}},
  issn = {0317-7173},
  doi = {10.3138/FM57-6770-U75U-7727},
  abstract = {All digitizing methods, as a general rule, record lines with far more data than is necessary for accurate graphic reproduction or for computer analysis. Two algorithms to reduce the number of points required to represent the line and, if desired, produce caricatures, are presented and compared with the most promising methods so far suggested. Line reduction will form a major part of automated generalization. R\`egle g\'en\'erale, les m\'ethodes num\'eriques enregistrent des lignes avec beaucoup plus de donn\'ees qu'il n'est n\'ecessaire \`a la reproduction graphique pr\'ecise ou \`a la recherche par ordinateur. L'auteur pr\'esente deux algorithmes pour r\'eduire le nombre de points n\'ecessaires pour repr\'esenter la ligne et produire des caricatures si d\'esir\'e, et les compare aux m\'ethodes les plus prometteuses sugg\'er\'ees jusqu'ici. La r\'eduction de la ligne constituera une partie importante de la g\'en\'eralisation automatique.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\AF99SDBI\\Douglas and Peucker - 1973 - Algorithms for the reduction of the number of poin.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\PZMFZQP4\\FM57-6770-U75U-7727.html},
  journal = {Cartographica: The International Journal for Geographic Information and Geovisualization},
  number = {2}
}

@article{duckham_efficient_2008,
  title = {Efficient Generation of Simple Polygons for Characterizing the Shape of a Set of Points in the Plane},
  author = {Duckham, Matt and Kulik, Lars and Worboys, Mike and Galton, Antony},
  year = {2008},
  month = oct,
  volume = {41},
  pages = {3224--3236},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2008.03.023},
  abstract = {This paper presents a simple, flexible, and efficient algorithm for constructing a possibly non-convex, simple polygon that characterizes the shape of a set of input points in the plane, termed a characteristic shape. The algorithm is based on the Delaunay triangulation of the points. The shape produced by the algorithm is controlled by a single normalized parameter, which can be used to generate a finite, totally ordered family of related characteristic shapes, varying between the convex hull at one extreme and a uniquely defined shape with minimum area. An optimal O(nlogn) algorithm for computing the shapes is presented. Characteristic shapes possess a number of desirable properties, and the paper includes an empirical investigation of the shapes produced by the algorithm. This investigation provides experimental evidence that with appropriate parameterization the algorithm is able to accurately characterize the shape of a wide range of different point distributions and densities. The experiments detail the effects of changing parameter values and provide an indication of some ``good'' parameter values to use in certain circumstances.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\4UFC4IP5\\Duckham et al. - 2008 - Efficient generation of simple polygons for charac.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\RNIN7HWG\\S0031320308001180.html},
  journal = {Pattern Recognition},
  keywords = {Alpha shape,Cartography,Convex hull,GIS,Shape analysis},
  language = {en},
  number = {10}
}

@article{edelsbrunner_shape_1983,
  title = {On the Shape of a Set of Points in the Plane},
  author = {Edelsbrunner, H. and Kirkpatrick, D. and Seidel, R.},
  year = {1983},
  month = jul,
  volume = {29},
  pages = {551--559},
  issn = {1557-9654},
  doi = {10.1109/TIT.1983.1056714},
  abstract = {A generalization of the convex hull of a finite set of points in the plane is introduced and analyzed. This generalization leads to a family of straight-line graphs, "\textbackslash alpha-shapes," which seem to capture the intuitive notions of "fine shape" and "crude shape" of point sets. It is shown that a-shapes are subgraphs of the closest point or furthest point Delaunay triangulation. Relying on this result an optimalO(n \textbackslash log n)algorithm that constructs\textbackslash alpha-shapes is developed.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\2FZWBPUC\\Edelsbrunner et al. - 1983 - On the shape of a set of points in the plane.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\93J7DGWY\\1056714.html},
  journal = {IEEE Transactions on Information Theory},
  keywords = {Geometry,Image analysis; shape,Image shape analysis},
  number = {4}
}

@misc{federal_aviation_administration_code_2016,
  title = {Code of {{Federal Regulations}} (14 {{CFR}}) {{Part}} 107.},
  author = {{Federal Aviation Administration}},
  year = {2016},
  publisher = {{FAA Washington, DC}}
}

@book{federal_aviation_administration_unmanned_2019,
  title = {Unmanned {{Aircraft Systems FY2019}}},
  author = {Federal Aviation Administration},
  year = {2019},
  publisher = {{FAA Washington, DC}},
  note = {Accessed: 2019-09-10},
  series = {The {{FAA Aerospace Forecast}}}
}

@inproceedings{feng_fast_2014,
  title = {Fast Plane Extraction in Organized Point Clouds Using Agglomerative Hierarchical Clustering},
  booktitle = {2014 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Feng, Chen and Taguchi, Yuichi and Kamat, Vineet R.},
  year = {2014},
  month = may,
  pages = {6218--6225},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2014.6907776},
  abstract = {Real-time plane extraction in 3D point clouds is crucial to many robotics applications. We present a novel algorithm for reliably detecting multiple planes in real time in organized point clouds obtained from devices such as Kinect sensors. By uniformly dividing such a point cloud into non-overlapping groups of points in the image space, we first construct a graph whose node and edge represent a group of points and their neighborhood respectively. We then perform an agglomerative hierarchical clustering on this graph to systematically merge nodes belonging to the same plane until the plane fitting mean squared error exceeds a threshold. Finally we refine the extracted planes using pixel-wise region growing. Our experiments demonstrate that the proposed algorithm can reliably detect all major planes in the scene at a frame rate of more than 35Hz for 640\texttimes 480 point clouds, which to the best of our knowledge is much faster than state-of-the-art algorithms.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\BGHHTBNB\\Feng et al. - 2014 - Fast plane extraction in organized point clouds us.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\7NH33W54\\6907776.html},
  keywords = {3D point clouds,agglomerative hierarchical clustering,clustering,Clustering algorithms,computer graphics,feature extraction,Image segmentation,image sensors,image space,Kinect sensors,mean square error methods,Merging,nonoverlapping groups,object detection,pattern clustering,pixel-wise region growing,plane,plane fitting mean squared error,planes detection,real-time plane extraction,Real-time systems,robotics applications,Robots,Sensors,Three-dimensional displays}
}

@inproceedings{forster_continuous_2015,
  title = {Continuous On-Board Monocular-Vision-Based Elevation Mapping Applied to Autonomous Landing of Micro Aerial Vehicles},
  booktitle = {2015 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Forster, C. and Faessler, M. and Fontana, F. and Werlberger, M. and Scaramuzza, D.},
  year = {2015},
  month = may,
  pages = {111--118},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2015.7138988},
  abstract = {In this paper, we propose a resource-efficient system for real-time 3D terrain reconstruction and landing-spot detection for micro aerial vehicles. The system runs on an on-board smartphone processor and requires only the input of a single downlooking camera and an inertial measurement unit. We generate a two-dimensional elevation map that is probabilistic, of fixed size, and robot-centric, thus, always covering the area immediately underneath the robot. The elevation map is continuously updated at a rate of 1 Hz with depth maps that are triangulated from multiple views using recursive Bayesian estimation. To highlight the usefulness of the proposed mapping framework for autonomous navigation of micro aerial vehicles, we successfully demonstrate fully autonomous landing including landing-spot detection in real-world experiments.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\U3ZCRU42\\Forster et al. - 2015 - Continuous on-board monocular-vision-based elevati.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\C3RGG23L\\7138988.html},
  keywords = {aerospace navigation,autonomous aerial vehicles,autonomous micro aerial vehicle landing,autonomous micro aerial vehicle navigation,Bayes methods,cameras,Cameras,continuous onboard monocular-vision-based elevation mapping,depth maps,Estimation,frequency 1 Hz,image reconstruction,inertial measurement unit,landing-spot detection,object detection,on-board smartphone processor,real-time 3D terrain reconstruction,Real-time systems,recursive Bayesian estimation,recursive estimation,resource-efficient system,robot vision,Robot vision systems,single downlooking camera,Three-dimensional displays,two-dimensional elevation map,Uncertainty}
}

@article{freeman_assessing_2013,
  title = {Assessing Bimodality to Detect the Presence of a Dual Cognitive Process},
  author = {Freeman, Jonathan B. and Dale, Rick},
  year = {2013},
  month = mar,
  volume = {45},
  pages = {83--97},
  issn = {1554-3528},
  doi = {10.3758/s13428-012-0225-x},
  abstract = {Researchers have long sought to distinguish between single-process and dual-process cognitive phenomena, using responses such as reaction times and, more recently, hand movements. Analysis of a response distribution's modality has been crucial in detecting the presence of dual processes, because they tend to introduce bimodal features. Rarely, however, have bimodality measures been systematically evaluated. We carried out tests of readily available bimodality measures that any researcher may easily employ: the bimodality coefficient (BC), Hartigan's dip statistic (HDS), and the difference in Akaike's information criterion between one-component and two-component distribution models (AICdiff). We simulated distributions containing two response populations and examined the influences of (1) the distances between populations, (2) proportions of responses, (3) the amount of positive skew present, and (4) sample size. Distance always had a stronger effect than did proportion, and the effects of proportion greatly differed across the measures. Skew biased the measures by increasing bimodality detection, in some cases leading to anomalous interactive effects. BC and HDS were generally convergent, but a number of important discrepancies were found. AICdiff was extremely sensitive to bimodality and identified nearly all distributions as bimodal. However, all measures served to detect the presence of bimodality in comparison to unimodal simulations. We provide a validation with experimental data, discuss methodological and theoretical implications, and make recommendations regarding the choice of analysis.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\6I4F9AW8\\Freeman and Dale - 2013 - Assessing bimodality to detect the presence of a d.pdf},
  journal = {Behavior Research Methods},
  language = {en},
  number = {1}
}

@misc{furieri_spatialite_2017,
  title = {Spatialite},
  author = {Furieri, A.},
  year = {2017},
  howpublished = {[Online] Available: \url{https://www.gaia-gis.it/fossil/libspatialite/index}},
  note = {Visited on 2017-01-14}
}

@article{garcia-castellanos_poles_2007,
  title = {Poles of Inaccessibility: {{A}} Calculation Algorithm for the Remotest Places on Earth},
  shorttitle = {Poles of Inaccessibility},
  author = {{Garcia-Castellanos}, Daniel and Lombardo, Umberto},
  year = {2007},
  month = sep,
  volume = {123},
  pages = {227--233},
  publisher = {{Routledge}},
  issn = {1470-2541},
  doi = {10.1080/14702540801897809},
  abstract = {An algorithm is presented to calculate the point on the surface of a sphere maximising the great-circle distance to a given spherical polygon. This is used to calculate the spots furthest from the sea in major land masses, also known as Poles of Inaccessibility (PIA), a concept that has raised the interest of explorers. For the Eurasian pole of inaccessibility (EPIA), the results reveal a misplacement in previous calculations ranging from 156 to 435 km. Although in general there is only one pole for a given coastline, the present calculations show that, within the error inherent to the definition of the coastline, two locations are candidates for EPIA, one equidistant from Gulf of Ob, Gulf of Bengal and Arabian Sea, and the other equidistant from Gulf of Ob, Gulf of Bengal and Gulf of Bohai, both poles being located in the north westernmost Chinese province of Xinjiang. The distance to the sea at these locations is 2510 and 2514 km respectively, about 120 km closer than generally thought.},
  annotation = {\_eprint: https://doi.org/10.1080/14702540801897809},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\4LAAKU5H\\Garcia-Castellanos and Lombardo - 2007 - Poles of inaccessibility A calculation algorithm .pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\LWQR4MUL\\14702540801897809.html},
  journal = {Scottish Geographical Journal},
  keywords = {computer method,Distance to sea},
  number = {3}
}

@article{geiger_vision_2013,
  title = {Vision Meets {{Robotics}}: {{The KITTI Dataset}}},
  author = {Geiger, Andreas and Lenz, Philip and Stiller, Christoph and Urtasun, Raquel},
  year = {2013},
  volume = {32},
  pages = {1231--1237},
  doi = {10.1177/0278364913491297},
  journal = {International Journal of Robotics Research (IJRR)},
  number = {11}
}

@misc{gillies_github_2020,
  title = {Github - {{Shapely}}: {{Manipulation}} and {{Analysis}} of {{Geometric Objects}}},
  author = {Gillies, Sean and others},
  year = {2020},
  howpublished = {[Online] Available: \url{https://github.com/Toblerity/Shapely}},
  note = {Accessed: 2020-06-05}
}

@misc{google_s2_2020,
  title = {S2 {{Geometry}}},
  author = {{Google}},
  year = {2020},
  howpublished = {[Online] Available: \url{https://github.com/google/s2geometry}},
  note = {Accessed: 2020-06-05}
}

@misc{graham_random_2014,
  title = {Random {{Points}}: {{How Dense Are You}}, {{Anyway}}?},
  shorttitle = {Random {{Points}}},
  author = {Graham, Lewis},
  year = {2014},
  month = jul,
  abstract = {A 1.023Mb PDF of this article as it appeared in the magazine complete with images is available by clicking HERE You may be...},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\62X8883R\\random-points-how-dense-are-you-anyway.html},
  howpublished = {http://localhost:10018/2014/07/05/random-points-how-dense-are-you-anyway/},
  journal = {LIDAR Magazine},
  keywords = {lidar},
  language = {en-US}
}

@article{haala_update_2010,
  title = {An Update on Automatic {{3D}} Building Reconstruction},
  author = {Haala, Norbert and Kada, Martin},
  year = {2010},
  month = nov,
  volume = {65},
  pages = {570--580},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2010.09.006},
  abstract = {The development of tools for the generation of 3D city models started almost two decades ago. From the beginning, fully automatic reconstruction systems were envisioned to fulfil the need for efficient data collection. However, research on automatic city modelling is still a very active area. The paper will review a number of current approaches in order to comprehensively elaborate the state of the art of reconstruction methods and their respective principles. Originally, automatic city modelling only aimed at polyhedral building objects, which mainly reflects the respective roof shapes and building footprints. For this purpose, airborne images or laser scans are used. In addition to these developments, the paper will also review current approaches for the generation of more detailed facade geometries from terrestrial data collection.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\EARXAH6F\\Haala and Kada - 2010 - An update on automatic 3D building reconstruction.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\74WWET3Z\\S0924271610000894.html},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  keywords = {Automation,Building,Reconstruction,Three-dimensional,Urban},
  language = {en},
  number = {6},
  series = {{{ISPRS Centenary Celebration Issue}}}
}

@article{hartigan_algorithm_1985,
  title = {Algorithm {{AS}} 217: {{Computation}} of the Dip Statistic to Test for Unimodality},
  author = {Hartigan, PM},
  year = {1985},
  volume = {34},
  pages = {320--325},
  publisher = {{JSTOR}},
  journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  number = {3}
}

@inproceedings{he_deep_2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, K. and Zhang, X. and Ren, S. and Sun, J.},
  year = {2016},
  month = jun,
  pages = {770--778},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\texttimes{} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\8MY688UV\\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\ZZ4E5ACP\\7780459.html},
  keywords = {CIFAR-10,COCO object detection dataset,COCO segmentation,Complexity theory,deep residual learning,deep residual nets,deeper neural network training,Degradation,ILSVRC \& COCO 2015 competitions,ILSVRC 2015 classification task,image classification,image recognition,Image recognition,Image segmentation,ImageNet dataset,ImageNet localization,ImageNet test set,learning (artificial intelligence),neural nets,Neural networks,object detection,residual function learning,residual nets,Training,VGG nets,visual recognition tasks,Visualization}
}

@article{herring_opengis_2006-1,
  title = {{{OpenGIS}} Implementation Specification for Geographic Information-{{Simple}} Feature Access- {{Part}} 1: {{Common}} Architecture},
  author = {Herring, John R},
  year = {2006},
  pages = {95},
  journal = {Open Geospatial Consortium}
}

@inproceedings{himmelsbach_fast_2010,
  title = {Fast Segmentation of {{3D}} Point Clouds for Ground Vehicles},
  booktitle = {2010 {{IEEE Intelligent Vehicles Symposium}}},
  author = {Himmelsbach, M. and v Hundelshausen, F. and Wuensche, H.-},
  year = {2010},
  month = jun,
  pages = {560--565},
  issn = {1931-0587},
  doi = {10.1109/IVS.2010.5548059},
  abstract = {This paper describes a fast method for segmentation of large-size long-range 3D point clouds that especially lends itself for later classification of objects. Our approach is targeted at high-speed autonomous ground robot mobility, so real-time performance of the segmentation method plays a critical role. This is especially true as segmentation is considered only a necessary preliminary for the more important task of object classification that is itself computationally very demanding. Efficiency is achieved in our approach by splitting the segmentation problem into two simpler subproblems of lower complexity: local ground plane estimation followed by fast 2D connected components labeling. The method's performance is evaluated on real data acquired in different outdoor scenes, and the results are compared to those of existing methods. We show that our method requires less runtime while at the same time yielding segmentation results that are better suited for later classification of the identified objects.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\XN5Z9RST\\Himmelsbach et al. - 2010 - Fast segmentation of 3D point clouds for ground ve.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\82CP8Q2S\\5548059.html},
  keywords = {3D point clouds,autonomous ground robot mobility,Clouds,computational complexity,computer graphics,fast segmentation,ground plane estimation,ground vehicles,image classification,image segmentation,Land vehicles,Laser radar,Layout,lower complexity,mobile robots,Mobile robots,object classification,Object detection,Real time systems,Remotely operated vehicles,road vehicles,Robot sensing systems,Sensor phenomena and characterization,traffic engineering computing}
}

@misc{hipp_sqlite_2020,
  title = {{{SQLite}} ({{Version}} 3.28) {{SQLite Development Team}}},
  author = {Hipp, D. R. and Kennedy, D. and Mistachkin, J.},
  year = {2020},
  howpublished = {[Online] Available: \url{https://www.sqlite.org/index.html}},
  note = {Visited on 2020-06-14}
}

@article{hoover_experimental_1996,
  title = {An Experimental Comparison of Range Image Segmentation Algorithms},
  author = {Hoover, A. and {Jean-Baptiste}, G. and Jiang, X. and Flynn, P.J. and Bunke, H. and Goldgof, D.B. and Bowyer, K. and Eggert, D.W. and Fitzgibbon, A. and Fisher, R.B.},
  year = {1996},
  month = jul,
  volume = {18},
  pages = {673--689},
  issn = {1939-3539},
  doi = {10.1109/34.506791},
  abstract = {A methodology for evaluating range image segmentation algorithms is proposed. This methodology involves (1) a common set of 40 laser range finder images and 40 structured light scanner images that have manually specified ground truth and (2) a set of defined performance metrics for instances of correctly segmented, missed, and noise regions, over- and under-segmentation, and accuracy of the recovered geometry. A tool is used to objectively compare a machine generated segmentation against the specified ground truth. Four research groups have contributed to evaluate their own algorithm for segmenting a range image into planar patches.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\5ED7CDWB\\Hoover et al. - 1996 - An experimental comparison of range image segmenta.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\TAJ3PHGA\\506791.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {Artificial intelligence,Computer vision,extraction,Geometrical optics,Geometry,image classification,image segmentation,Image segmentation,Laser noise,laser range finder images,laser ranging,Measurement standards,over-segmentation,performance metrics,Pixel,planar patches,plane,range,range image segmentation algorithms,recovered geometry,Shape,structured light scanner images,Testing,under-segmentation},
  number = {7}
}

@inproceedings{huang_cpp-taskflow_2019,
  title = {Cpp-{{Taskflow}}: {{Fast Task}}-{{Based Parallel Programming Using Modern C}}++},
  shorttitle = {Cpp-{{Taskflow}}},
  booktitle = {2019 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  author = {Huang, T. and Lin, C. and Guo, G. and Wong, M.},
  year = {2019},
  month = may,
  pages = {974--983},
  issn = {1530-2075},
  doi = {10.1109/IPDPS.2019.00105},
  abstract = {In this paper we introduce Cpp-Taskflow, a new C++ tasking library to help developers quickly write parallel programs using task dependency graphs. Cpp-Taskflow leverages the power of modern C++ and task-based approaches to enable efficient implementations of parallel decomposition strategies. Our programming model can quickly handle not only traditional loop-level parallelism, but also irregular patterns such as graph algorithms, incremental flows, and dynamic data structures. Compared with existing libraries, Cpp-Taskflow is more cost efficient in performance scaling and software integration. We have evaluated Cpp-Taskflow on both micro-benchmarks and real-world applications with million-scale tasking. In a machine learning example, Cpp-Taskflow achieved 1.5-2.7\texttimes{} less coding complexity and 14-38\% speed-up over two industrial-strength libraries OpenMP Tasking and Intel Threading Building Blocks (TBB).},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\STGXLQKX\\Huang et al. - 2019 - Cpp-Taskflow Fast Task-Based Parallel Programming.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\8IKGWEVC\\8821011.html},
  keywords = {C++ language,C++ languages,Cpp-Taskflow,data structures,Heuristic algorithms,industrial-strength libraries OpenMP tasking,Intel Threading Building Block,learning (artificial intelligence),Libraries,machine learning,million-scale tasking,multithreading,parallel decomposition strategies,parallel programming,parallel programming library and model,parallel programs,program compilers,Programming,Task analysis,task dependency graphs,task parallelism,task-based approaches,task-based parallel programming,TBB,Timing,traditional loop-level parallelism,Very large scale integration}
}

@book{iglewicz_how_1993,
  title = {How to {{Detect}} and {{Handle Outliers}}},
  author = {Iglewicz, Boris and Hoaglin, David Caster},
  year = {1993},
  publisher = {{ASQC Quality Press}},
  googlebooks = {siInAQAAIAAJ},
  isbn = {978-0-87389-247-6},
  keywords = {Technology \& Engineering / Reference},
  language = {en}
}

@misc{jeremy_castagno_polylidar3d_2020,
  title = {{{Polylidar3D Kitti Videos}}},
  author = {{Castagno, Jeremy}},
  year = {2020},
  howpublished = {[Online] Available: \url{https://drive.google.com/drive/folders/18R0alYprRYgwz5_MyzcdOQzf44496DOz}},
  note = {Accessed: 2020-06-30}
}

@misc{jeremy_castagno_polylidar3d_2020-1,
  title = {{{Polylidar3D RealSense Videos}}},
  author = {{Castagno, Jeremy}},
  year = {2020},
  howpublished = {[Online] Available: \url{https://drive.google.com/drive/folders/1t19uS9dh1azTA6drADThG1WTsX7NaflS}},
  note = {Accessed: 2020-06-30}
}

@article{jochem_automatic_2009,
  title = {Automatic {{Roof Plane Detection}} and {{Analysis}} in {{Airborne Lidar Point Clouds}} for {{Solar Potential Assessment}}},
  author = {Jochem, Andreas and H{\"o}fle, Bernhard and Rutzinger, Martin and Pfeifer, Norbert},
  year = {2009},
  month = jul,
  volume = {9},
  pages = {5241--5262},
  publisher = {{Molecular Diversity Preservation International}},
  doi = {10.3390/s90705241},
  abstract = {A relative height threshold is defined to separate potential roof points from the point cloud, followed by a segmentation of these points into homogeneous areas fulfilling the defined constraints of roof planes. The normal vector of each laser point is an excellent feature to decompose the point cloud into segments describing planar patches. An objectbased error assessment is performed to determine the accuracy of the presented classification. It results in 94.4\% completeness and 88.4\% correctness. Once all roof planes are detected in the 3D point cloud, solar potential analysis is performed for each point. Shadowing effects of nearby objects are taken into account by calculating the horizon of each point within the point cloud. Effects of cloud cover are also considered by using data from a nearby meteorological station. As a result the annual sum of the direct and diffuse radiation for each roof plane is derived. The presented method uses the full 3D information for both feature extraction and solar potential analysis, which offers a number of new applications in fields where natural processes are influenced by the incoming solar radiation (e.g., evapotranspiration, distribution of permafrost). The presented method detected fully automatically a subset of 809 out of 1,071 roof planes where the arithmetic mean of the annual incoming solar radiation is more than 700 kWh/m2.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\MBUZYB6W\\Jochem et al. - 2009 - Automatic Roof Plane Detection and Analysis in Air.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\PRZJR4FC\\5241.html},
  journal = {Sensors},
  keywords = {3D point cloud,airborne LiDAR,classification,clear sky index,roof plane detection,segmentation,solar radiation},
  language = {en},
  number = {7}
}

@article{kaiser_survey_2019,
  title = {A {{Survey}} of {{Simple Geometric Primitives Detection Methods}} for {{Captured 3D Data}}},
  author = {Kaiser, Adrien and Ybanez Zepeda, Jose Alonso and Boubekeur, Tamy},
  year = {2019},
  month = feb,
  volume = {38},
  pages = {167--196},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/cgf.13451},
  abstract = {The amount of captured 3D data is continuously increasing, with the democratization of consumer depth cameras, the development of modern multi-view stereo capture setups and the rise of single-view 3D capture based on machine learning. The analysis and representation of this ever growing volume of 3D data, often corrupted with acquisition noise and reconstruction artifacts, is a serious challenge at the frontier between computer graphics and computer vision. To that end, segmentation and optimization are crucial analysis components of the shape abstraction process, which can themselves be greatly simplified when performed on lightened geometric formats. In this survey, we review the algorithms which extract simple geometric primitives from raw dense 3D data. After giving an introduction to these techniques, from the acquisition modality to the underlying theoretical concepts, we propose an application-oriented characterization, designed to help select an appropriate method based on one's application needs, and compare recent approaches. We conclude by giving hints for how to evaluate these methods and a set of research challenges to be explored.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\5ZR2PW9M\\Kaiser et al. - 2019 - A Survey of Simple Geometric Primitives Detection .pdf},
  journal = {Computer Graphics Forum},
  language = {en},
  number = {1}
}

@article{kazhdan_screened_2013,
  title = {Screened Poisson Surface Reconstruction},
  author = {Kazhdan, Michael and Hoppe, Hugues},
  year = {2013},
  month = jul,
  volume = {32},
  pages = {29:1--29:13},
  issn = {0730-0301},
  doi = {10.1145/2487228.2487237},
  abstract = {Poisson surface reconstruction creates watertight surfaces from oriented point sets. In this work we extend the technique to explicitly incorporate the points as interpolation constraints. The extension can be interpreted as a generalization of the underlying mathematical framework to a screened Poisson equation. In contrast to other image and geometry processing techniques, the screening term is defined over a sparse set of points rather than over the full domain. We show that these sparse constraints can nonetheless be integrated efficiently. Because the modified linear system retains the same finite-element discretization, the sparsity structure is unchanged, and the system can still be solved using a multigrid approach. Moreover we present several algorithmic improvements that together reduce the time complexity of the solver to linear in the number of points, thereby enabling faster, higher-quality surface reconstructions.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\PSYWNASH\\Kazhdan and Hoppe - 2013 - Screened poisson surface reconstruction.pdf},
  journal = {ACM Transactions on Graphics},
  keywords = {adaptive octree,finite elements,mesh,Screened Poisson equation,surface fitting},
  number = {3}
}

@article{khuong_array_2017,
  title = {Array {{Layouts}} for {{Comparison}}-{{Based Searching}}},
  author = {Khuong, Paul-Virak and Morin, Pat},
  year = {2017},
  month = may,
  volume = {22},
  pages = {1.3:1--1.3:39},
  issn = {1084-6654},
  doi = {10.1145/3053370},
  abstract = {We attempt to determine the best order and search algorithm to store n comparable data items in an array, A, of length n so we can, for any query value, x, quickly find the smallest value in A that is greater than or equal to x. In particular, we consider the important case where there are many such queries to the same array, A, which resides entirely in RAM. In addition to the obvious sorted order/binary search combination we consider the Eytzinger breadth-first-search (BFS) layout normally used for heaps, an implicit B-tree layout that generalizes the Eytzinger layout, and the van Emde Boas layout commonly used in the cache-oblivious algorithms literature. After extensive testing and tuning on a wide variety of modern hardware, we arrive at the conclusion that, for small values of n, sorted order, combined with a good implementation of binary search, is best. For larger values of n, we arrive at the surprising conclusion that the Eytzinger layout is usually the fastest. The latter conclusion is unexpected and goes counter to earlier experimental work by Brodal, Fagerberg, and Jacob (SODA 2003), who concluded that both the B-tree and van Emde Boas layouts were faster than the Eytzinger layout for large values of n. Our fastest C++ implementations, when compiled, use conditional moves to avoid branch mispredictions and prefetching to reduce cache latency.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\ARB25WPM\\Khuong and Morin - 2017 - Array Layouts for Comparison-Based Searching.pdf},
  journal = {Journal of Experimental Algorithmics},
  keywords = {Binary search,caching,data layouts,microprocessor architecture,pipelining,search}
}

@article{kingma_adam_2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archivePrefix = {arXiv},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\LQNPX3UV\\Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\9637YAXE\\1412.html},
  journal = {arXiv:1412.6980 [cs]},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
  primaryClass = {cs}
}

@misc{land_nrw_open_2020,
  title = {Open {{Geo Data}}},
  author = {{Land NRW}},
  year = {2020},
  note = {Data licence: Germany - Zero - Version 2.0: https://www.govdata.de/dl-de/zero-2-0
\par
Satellite Images}
}

@incollection{lee_fast_2013,
  title = {Fast {{Range Image Segmentation}} and {{Smoothing Using Approximate Surface Reconstruction}} and {{Region Growing}}},
  booktitle = {Intelligent {{Autonomous Systems}} 12},
  author = {Holz, Dirk and Behnke, Sven},
  editor = {Lee, Sukhan and Cho, Hyungsuck and Yoon, Kwang-Joon and Lee, Jangmyung},
  year = {2013},
  volume = {194},
  pages = {61--73},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-33932-5_7},
  abstract = {Decomposing sensory measurements into relevant parts is a fundamental prerequisite for solving complex tasks, e.g., in the field of mobile manipulation in domestic environments. In this paper, we present a fast approach to surface reconstruction in range images by means of approximate polygonal meshing. The obtained local surface information and neighborhoods are then used to 1) smooth the underlying measurements, and 2) segment the image into planar regions and other geometric primitives. An evaluation using publicly available data sets shows that our approach does not rank behind state-of-the-art algorithms while allowing to process range images at high frame rates.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\FBTVFLW8\\Holz and Behnke - 2013 - Fast Range Image Segmentation and Smoothing Using .pdf},
  isbn = {978-3-642-33931-8 978-3-642-33932-5},
  language = {en}
}

@inproceedings{lee_indoor_2012-1,
  title = {Indoor Mapping Using Planes Extracted from Noisy {{RGB}}-{{D}} Sensors},
  booktitle = {2012 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Lee, T. and Lim, S. and Lee, S. and An, S. and Oh, S.},
  year = {2012},
  month = oct,
  pages = {1727--1733},
  issn = {2153-0866},
  doi = {10.1109/IROS.2012.6385909},
  abstract = {This paper presents a fast and robust plane feature extraction and matching technique for RGB-D type sensors. We propose three algorithm components required to utilize the plane features in an online Simultaneous Localization and Mapping (SLAM) problem: fast plane extraction, frame-to-frame constraint estimation, and plane merging. For the fast plane extraction, we estimate local surface normals and curvatures by a simple spherical model and then segment points using a modified flood fill algorithm. In plane parameter estimation, we suggest a new uncertainty estimation method which is robust against the measurement bias, and also introduce a fast boundary modeling method. We associate the plane features based on both the parameters and the spatial coverage, and estimate the stable constraints by the cost function with a regulation term. Also, our plane merging technique provides a way of generating local maps that are useful for estimating loop closure constraints. We have performed real-world experiments at our lab environment. The results demonstrate the efficiency and robustness of the proposed algorithm.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\ZKE2XMMN\\Lee et al. - 2012 - Indoor mapping using planes extracted from noisy R.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\4W38WEYM\\6385909.html},
  keywords = {Cameras,Estimation,feature extraction,Feature extraction,frame-to-frame constraint estimation,Image segmentation,indoor mapping,noisy RGB-D sensors,parameter estimation,plane merging,plane parameter estimation,robust control,robust plane feature extraction,robust plane feature matching,robustness,sensors,Simultaneous localization and mapping,simultaneous localization and mapping problem,SLAM (robots),SLAM problem,stable constraints,uncertainty estimation method,Vectors}
}

@article{lerma_terrestrial_2010,
  title = {Terrestrial Laser Scanning and Close Range Photogrammetry for {{3D}} Archaeological Documentation: The {{Upper Palaeolithic Cave}} of {{Parpall\'o}} as a Case Study},
  shorttitle = {Terrestrial Laser Scanning and Close Range Photogrammetry for {{3D}} Archaeological Documentation},
  author = {Lerma, Jos{\'e} Luis and Navarro, Santiago and Cabrelles, Miriam and Villaverde, Valent{\'i}n},
  year = {2010},
  month = mar,
  volume = {37},
  pages = {499--507},
  issn = {0305-4403},
  doi = {10.1016/j.jas.2009.10.011},
  abstract = {Graphic and metric archaeological documentation is an activity that requires the capture of information from different sources, accurate processing and comprehensive analysis. If monitoring of the state of conservation is required, this task has to be performed before intervention, during and after the completion of the works in a repetitive way. This paper presents the use of terrestrial laser scanning (TLS) in order to effectively produce, prior to intervention, accurate and high-resolution 3D models of a cave with engravings dating back to the Upper Palaeolithic era. The processing of the TLS data is discussed in detail in order to create digital surface models. The complexity of the cave required the integration of two techniques, TLS and close range photogrammetry to yield not only traditional drawings such as sections and elevations, but also photo-realistic perspective views and visual navigation worlds fully operational in 3D environments. This paper demonstrates the potential of integrating TLS and close range photogrammetry to provide both accurate digital surface models and photo-realistic outputs. This processed data can be used to systematically improve archaeological understanding of complex caves and relief panels of prehistoric art with tiny engravings.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\RGEVUWKJ\\Lerma et al. - 2010 - Terrestrial laser scanning and close range photogr.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\UACYLDHN\\S0305440309003781.html},
  journal = {Journal of Archaeological Science},
  keywords = {Analysis,Cave,Close range photogrammetry,Documentation,Interpretation,Rock art,Terrestrial laser scanning},
  language = {en},
  number = {3}
}

@inproceedings{lim_uninformed_2014,
  title = {Uninformed Multigoal Pathfinding on Grid Maps},
  booktitle = {2014 {{International Conference}} on {{Information Science}}, {{Electronics}} and {{Electrical Engineering}}},
  author = {Lim, K. L. and Yeong, L. S. and Ch'ng, S. I. and Seng, K. P. and Ang, L.},
  year = {2014},
  month = apr,
  volume = {3},
  pages = {1552--1556},
  doi = {10.1109/InfoSEEE.2014.6946181},
  abstract = {This paper proposes multigoal implementations of the Dijkstra's shortest path algorithm and the boundary iterative-deepening depth-first search (BIDDFS). The algorithms were modified to allow for the search of more than one goal in a single expansion pass. The aim of this is to reduce the operational redundancy and hence the time taken for calculating multiple start-goal node pairs. Simulations using multigoal algorithms on 250\texttimes{} 250 open grid maps with nine goals have shown up to a 458\% increase in time efficiency.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\69MHJVDT\\Lim et al. - 2014 - Uninformed multigoal pathfinding on grid maps.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\AKL4LEJR\\6946181.html},
  keywords = {Arrays,boundary iterative-deepening depth-first search,Buffer storage,Dijkstra shortest path algorithm,Educational institutions,Electronic mail,graph theory,multigoal algorithms,multiple start-goal node pairs,open grid maps,operational redundancy,Redundancy,Routing,Simulation,tree searching,uninformed multigoal pathfinding}
}

@article{limberger_real-time_2015,
  title = {Real-Time Detection of Planar Regions in Unorganized Point Clouds},
  author = {Limberger, Frederico A. and Oliveira, Manuel M.},
  year = {2015},
  month = jun,
  volume = {48},
  pages = {2043--2053},
  issn = {00313203},
  doi = {10.1016/j.patcog.2014.12.020},
  abstract = {Automatic detection of planar regions in point clouds is an important step for many graphics, image processing, and computer vision applications. While laser scanners and digital photography have allowed us to capture increasingly larger datasets, previous techniques are computationally expensive, being unable to achieve real-time performance for datasets containing tens of thousands of points, even when detection is performed in a non-deterministic way. We present a deterministic technique for plane detection in unorganized point clouds whose cost is O(n log n) in the number of input samples. It is based on an efficient Hough-transform voting scheme and works by clustering approximately co-planar points and by casting votes for these clusters on a spherical accumulator using a trivariate Gaussian kernel. A comparison with competing techniques shows that our approach is considerably faster and scales significantly better than previous ones, being the first practical solution for deterministic plane detection in large unorganized point clouds.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\THQMI7ZN\\Limberger and Oliveira - 2015 - Real-time detection of planar regions in unorganiz.pdf},
  journal = {Pattern Recognition},
  language = {en},
  number = {6}
}

@article{lindsay_whitebox_2016,
  title = {Whitebox {{GAT}}: {{A}} Case Study in Geomorphometric Analysis},
  shorttitle = {Whitebox {{GAT}}},
  author = {Lindsay, J. B.},
  year = {2016},
  month = oct,
  volume = {95},
  pages = {75--84},
  issn = {0098-3004},
  doi = {10.1016/j.cageo.2016.07.003},
  abstract = {This paper describes an open-source geographical information system (GIS) called Whitebox Geospatial Analysis Tools (Whitebox GAT). Whitebox GAT was designed to provide a platform for the rapid development and testing of experimental geospatial analysis methods, supported by its extensible design, integrated facilities for custom plug-in tool authoring, and its novel open-access design philosophy. One of the unique characteristics of Whitebox GAT is the ease with which users can inspect and modify the algorithms for individual geoprocessing tools. The open-access software model that Whitebox GAT adopts is designed to lessen the barriers that are often imposed on end-users when attempting to gain deeper understanding of how a specific function operates. While Whitebox GAT has an extensive range of GIS and remote sensing analytical capabilities, making it broadly suited for advanced scientific research applications in the Earth Sciences, this paper focusses on the software's application in the field of geomorphometry. An airborne LiDAR data set for a small headwater catchment of the Missisquoi River in northern Vermont, USA, was filtered to identify ground-points and then interpolated into a 2.0m resolution bare-Earth DEM. The DEM was processed to remove spurious off-ground objects (mainly buildings), to reduce surface roughness under heavy forest cover, and to hydrologically pre-condition the DEM. These data were then used to extract salient hydrological structures, i.e. the stream network and their associated sub-basins.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\E36VAXZK\\Lindsay - 2016 - Whitebox GAT A case study in geomorphometric anal.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\W29SFZG4\\S0098300416301820.html},
  journal = {Computers \& Geosciences},
  keywords = {Geographic information system,Geomorphometry,Open-source software,Remote sensing software},
  language = {en}
}

@inproceedings{malihi_3d_2016,
  title = {{{3D Building Reconstruction Using Dense Photogrammetric Point Cloud}}},
  booktitle = {{{ISPRS}} - {{International Archives}} of the {{Photogrammetry}}, {{Remote Sensing}} and {{Spatial Information Sciences}}},
  author = {Malihi, S. and Valadan Zoej, M. J. and Hahn, M. and Mokhtarzade, M. and Arefi, H.},
  year = {2016},
  month = jun,
  volume = {XLI-B3},
  pages = {71--74},
  publisher = {{Copernicus GmbH}},
  issn = {1682-1750},
  doi = {10.5194/isprs-archives-XLI-B3-71-2016},
  abstract = {{$<$}p{$><$}strong{$>$}Abstract.{$<$}/strong{$>$} Three dimensional models of urban areas play an important role in city planning, disaster management, city navigation and other applications. Reconstruction of 3D building models is still a challenging issue in 3D city modelling. Point clouds generated from multi view images of UAV is a novel source of spatial data, which is used in this research for building reconstruction. The process starts with the segmentation of point clouds of roofs and walls into planar groups. By generating related surfaces and using geometrical constraints plus considering symmetry, a 3d model of building is reconstructed. In a refinement step, dormers are extracted, and their models are reconstructed. The details of the 3d reconstructed model are in LoD3 level, with respect to modelling eaves, fractions of roof and dormers.{$<$}/p{$>$}},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\DQWJMTEH\\Malihi et al. - 2016 - 3D Building Reconstruction Using Dense Photogramme.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\YQI4UBWQ\\2016.html},
  language = {English}
}

@article{maset_photogrammetric_2017,
  title = {Photogrammetric {{3D Building Reconstruction From Thermal Images}}},
  author = {Maset, E. and Fusiello, A. and Crosilla, F. and Toldo, R. and Zorzetto, D.},
  year = {2017},
  volume = {IV-2/W3},
  pages = {25--32},
  doi = {10.5194/isprs-annals-IV-2-W3-25-2017},
  journal = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences}
}

@inproceedings{mcdonough_rangr_2018,
  title = {{{RANGR}}: {{Risk Aware Navigation}} and {{Gudiance Resilience}}},
  booktitle = {{{AUVSI Xponential}}},
  author = {McDonough, Kevin and Castagno, Jeremy and Player, Jennifer},
  year = {2018},
  month = may,
  address = {{Denver, CO, USA}},
  abstract = {Player}
}

@incollection{mejias_alvarez_forced_2009,
  title = {Forced Landing Technologies for Unmanned Aerial Vehicles: Towards Safer Operations},
  shorttitle = {Forced Landing Technologies for Unmanned Aerial Vehicles},
  booktitle = {Aerial {{Vehicles}}},
  author = {Mejias Alvarez, Luis and Fitzgerald, Daniel and Eng, Pillar and Liu, Xi},
  editor = {Lam, Than Mung},
  year = {2009},
  pages = {415--442},
  publisher = {{InTech}},
  address = {{Austria}},
  abstract = {While using unmanned systems in combat is not new, what will be new in the foreseeable future is how such systems are used and integrated in the civilian space. The potential use of Unmanned Aerial Vehicles in civil and commercial applications is becoming a fact, and is receiving considerable attention by industry and the research community. The majority of Unmanned Aerial Vehicles performing civilian tasks are restricted to flying only in segregated space, and not within the National Airspace. The areas that UAVs are restricted to flying in are typically not above populated areas, which in turn are the areas most useful for civilian applications. The reasoning behind the current restrictions is mainly due to the fact that current UAV technologies are not able to demonstrate an Equivalent Level of Safety to manned aircraft, particularly in the case of an engine failure which would require an emergency or forced landing. This chapter will preset and guide the reader through a number of developments that would facilitate the integration of UAVs into the National Airspace. Algorithms for UAV Sense-and-Avoid and Force Landings are recognized as two major enabling technologies that will allow the integration of UAVs in the civilian airspace. The following sections will describe some of the techniques that are currently being tested at the Australian Research Centre for Aerospace Automation (ARCAA), which places emphasis on the detection of candidate landing sites using computer vision, the planning of the descent path trajectory for the UAV, and the decision making process behind the selection of the final landing site.},
  copyright = {Copyright 2009 The Authors},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\PS8XDDAV\\Mejias Alvarez et al. - 2009 - Forced landing technologies for unmanned aerial ve.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\KGNDZTA7\\42556.html},
  isbn = {978-953-7619-41-1},
  language = {en}
}

@article{melnyk_third-party_2014-1,
  title = {A Third-Party Casualty Risk Model for Unmanned Aircraft System Operations},
  author = {Melnyk, Richard and Schrage, Daniel and Volovoi, Vitali and Jimenez, Hernando},
  year = {2014},
  month = apr,
  volume = {124},
  pages = {105--116},
  issn = {0951-8320},
  doi = {10.1016/j.ress.2013.11.016},
  abstract = {Unmanned Aircraft System (UAS) integration into the National Airspace System (NAS) is an important goal of many members of the Aerospace community including stakeholders such as the military, law enforcement and potential civil users of UAS. However, integration efforts have remained relatively limited due to safety concerns. Due to the nature of UAS, safety predictions must look beyond the system itself and take the operating environment into account. A framework that can link UAS reliability and physical characteristics to the effects on the bystander population is required. This study proposes using a Target Level of Safety approach and an event tree format, populated with data from existing studies that share characteristics of UAS crashes to enable casualty prediction for UAS operations.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\M3MVJQDN\\Melnyk et al. - 2014 - A third-party casualty risk model for unmanned air.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\WFGG4BDU\\S095183201300313X.html},
  journal = {Reliability Engineering \& System Safety},
  keywords = {Casualties,Reliability,Safety,Target Level of Safety (TLS),Third-party,Unmanned Aircraft Systems (UAS)},
  language = {en}
}

@article{meuleau_emergency_2009,
  title = {An {{Emergency Landing Planner}} for {{Damaged Aircraft}}},
  author = {Meuleau, N. and Plaunt, C. and Smith, D. E. and Smith, T.},
  year = {2009},
  pages = {71--80},
  journal = {Proc. of the 21st Innovative Applications of Artificial Intelligence Conf.}
}

@misc{microsoft_bing_2018,
  title = {Bing {{Maps}}},
  author = {{Microsoft}},
  year = {2018},
  note = {Accessed: 2018-09-05}
}

@article{mohajeri_city-scale_2018,
  title = {A City-Scale Roof Shape Classification Using Machine Learning for Solar Energy Applications},
  author = {Mohajeri, Nahid and Assouline, Dan and Guiboud, Berenice and Bill, Andreas and Gudmundsson, Agust and Scartezzini, Jean-Louis},
  year = {2018},
  month = jun,
  volume = {121},
  pages = {81--93},
  issn = {0960-1481},
  doi = {10.1016/j.renene.2017.12.096},
  abstract = {Solar energy deployment through PV installations in urban areas depends strongly on the shape, size, and orientation of available roofs. Here we use a machine learning approach, Support Vector Machine (SVM) classification, to classify 10,085 building roofs in relation to their received solar energy in the city of Geneva in Switzerland. The SVM correctly identifies six types of roof shapes in 66\% of cases, that is, flat \& shed, gable, hip, gambrel \& mansard, cross/corner gable \& hip, and complex roofs. We classify the roofs based on their useful area for PV installations and potential for receiving solar energy. For most roof shapes, the ratio between useful roof area and building footprint area is close to one, suggesting that footprint is a good measure of useful PV roof area. The main exception is the gable where this ratio is 1.18. The flat and shed roofs have the second highest useful roof area for PV (complex roof being the highest) and the highest PV potential (in GWh). By contrast, hip roof has the lowest PV potential. Solar roof-shape classification provides basic information for designing new buildings, retrofitting interventions on the building roofs, and efficient solar integration on the roofs of buildings.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\S78NMGNG\\Mohajeri et al. - 2018 - A city-scale roof shape classification using machi.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\ECSE58IZ\\S0960148117313009.html},
  journal = {Renewable Energy},
  keywords = {Machine learning,PV potential,Roof shape classification,Support Vector Machine},
  language = {en}
}

@incollection{mokbel_space-filling_2008,
  title = {Space-{{Filling Curves}}},
  booktitle = {Encyclopedia of {{GIS}}},
  author = {Mokbel, Mohamed F. and Aref, Walid G.},
  editor = {Shekhar, Shashi and Xiong, Hui},
  year = {2008},
  pages = {1068--1072},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-0-387-35973-1_1233},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\LGVLGKQM\\Mokbel and Aref - 2008 - Space-Filling Curves.pdf},
  isbn = {978-0-387-35973-1},
  keywords = {Consecutive Point,Geographic Information System,Hilbert Curve,Multimedia Server,Range Query,SFC},
  language = {en}
}

@article{nagle_dasymetric_2014-1,
  title = {Dasymetric {{Modeling}} and {{Uncertainty}}},
  author = {Nagle, Nicholas N. and Buttenfield, Barbara P. and Leyk, Stefan and Spielman, Seth},
  year = {2014},
  month = jan,
  volume = {104},
  pages = {80--95},
  publisher = {{Routledge}},
  issn = {0004-5608},
  doi = {10.1080/00045608.2013.843439},
  abstract = {Dasymetric models increase the spatial resolution of population data by incorporating related ancillary data layers. The role of uncertainty in dasymetric modeling has not been fully addressed as of yet. Uncertainty is usually present because most population data are themselves uncertain, or the geographic processes that connect population and the ancillary data layers are not precisely known. A new dasymetric methodology\textemdash the penalized maximum entropy dasymetric model (P\textendash MEDM)\textemdash is presented that enables these sources of uncertainty to be represented and modeled. The P\textendash MEDM propagates uncertainty through the model and yields fine-resolution population estimates with associated measures of uncertainty. This methodology contains a number of other benefits of theoretical and practical interest. In dasymetric modeling, researchers often struggle with identifying a relationship between population and ancillary data layers. The P\textendash MEDM model simplifies this step by unifying how ancillary data are included. The P\textendash MEDM also allows a rich array of data to be included, with disparate spatial resolutions, attribute resolutions, and uncertainties. Although the P\textendash MEDM does not necessarily produce more precise estimates than do existing approaches, it does help to unify how data enter the dasymetric model, it increases the types of data that can be used, and it allows geographers to characterize the quality of their dasymetric estimates. We present an application of the P\textendash MEDM that includes household-level survey data combined with higher spatial resolution data such as from census tracts, block groups, and land cover classifications.},
  annotation = {\_eprint: https://doi.org/10.1080/00045608.2013.843439},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\9YV8838R\\Nagle et al. - 2014 - Dasymetric Modeling and Uncertainty.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\YL7YPCHE\\00045608.2013.html},
  journal = {Annals of the Association of American Geographers},
  keywords = {dasymetric modeling,entropÃ­a mÃ¡xima,estimaciÃ³n de Ã¡rea pequeÃ±a,maximum entropy,modelaciÃ³n dasimÃ©trica,small area estimation,æœ€å¤§ç†µ,åˆ†åŒºå¯†åº¦æ¨¡åž‹,å°åŒºåŸŸä¼°ç®—},
  number = {1},
  pmid = {25067846}
}

@inproceedings{nash_lazy_2010,
  title = {Lazy Theta*: Any-Angle Path Planning and Path Length Analysis in {{3D}}},
  shorttitle = {Lazy Theta*},
  booktitle = {Proceedings of the {{Twenty}}-{{Fourth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Nash, Alex and Koenig, Sven and Tovey, Craig},
  year = {2010},
  month = jul,
  pages = {147--154},
  publisher = {{AAAI Press}},
  address = {{Atlanta, Georgia}},
  abstract = {Grids with blocked and unblocked cells are often used to represent continuous 2D and 3D environments in robotics and video games. The shortest paths formed by the edges of 8-neighbor 2D grids can be up to {$\approx$} 8\% longer than the shortest paths in the continuous environment. Theta* typically finds much shorter paths than that by propagating information along graph edges (to achieve short runtimes) without constraining paths to be formed by graph edges (to find short "any-angle" paths). We show in this paper that the shortest paths formed by the edges of 26-neighbor 3D grids can be {$\approx$} 13\% longer than the shortest paths in the continuous environment, which highlights the need for smart path planning algorithms in 3D. Theta* can be applied to 3D grids in a straight-forward manner, but it performs a line-of-sight check for each unexpanded visible neighbor of each expanded vertex and thus it performs many more line-of-sight checks per expanded vertex on a 26-neighbor 3D grid than on an 8-neighbor 2D grid. We therefore introduce Lazy Theta*, a variant of Theta* which uses lazy evaluation to perform only one line-of-sight check per expanded vertex (but with slightly more expanded vertices). We show experimentally that Lazy Theta* finds paths faster than Theta* on 26-neighbor 3D grids, with one order of magnitude fewer line-of-sight checks and without an increase in path length.},
  series = {{{AAAI}}'10}
}

@misc{new_york_state_2016_2018,
  title = {2016 {{Annual Lot New York County}}},
  author = {{New York State}},
  year = {2018},
  note = {Data was retrieved through ArcGIS World Imagery}
}

@inproceedings{ngatchou_pareto_2005,
  title = {Pareto {{Multi Objective Optimization}}},
  booktitle = {Proceedings of the 13th {{International Conference}} on, {{Intelligent Systems Application}} to {{Power Systems}}},
  author = {Ngatchou, P. and Zarei, A. and {El-Sharkawi}, A.},
  year = {2005},
  month = nov,
  pages = {84--91},
  doi = {10.1109/ISAP.2005.1599245},
  abstract = {The goal of this chapter is to give fundamental knowledge on solving multi-objective optimization problems. The focus is on the intelligent metaheuristic approaches (evolutionary algorithms or swarm-based techniques). The focus is on techniques for efficient generation of the Pareto frontier. A general formulation of MO optimization is given in this chapter, the Pareto optimality concepts introduced, and solution approaches with examples of MO problems in the power systems field are given},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\XE8Q5MB2\\Ngatchou et al. - 2005 - Pareto Multi Objective Optimization.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\RBA4IY3F\\1599245.html},
  keywords = {Constraint optimization,Cost function,Delta modulation,evolutionary algorithm,evolutionary computation,Evolutionary computation,intelligent metaheuristic approach,Pareto analysis,pareto multiobjective optimization,Pareto optimazation,Pareto optimisation,Pareto optimization,particle swarm optimisation,particle swarm optimization,Power engineering and energy,power systems,Stability,swarm-based technique,Systems engineering and theory,Voltage}
}

@misc{noauthor_github_2018,
  author = {Mapbox},
  title = {Github - {{Delaunator}}},
  year = {2018},
  howpublished = {[Online] Available: \url{https://github.com/mapbox/delaunator}},
  note = {Accessed: 2018-01-05}
}

@misc{noauthor_github_2018-1,
  author={Andrew Bell},
  title = {Github - {{Port}} of {{Delaunator}} to {{C}}++},
  year = {2018},
  howpublished = {[Online] Available: \url{https://github.com/abellgithub/delaunator-cpp}},
  note = {Accessed: 2018-01-05}
}

@misc{noauthor_github_2018-3,
  author={Mapbox},
  title = {Github - {{Polylabel}}},
  year = {2018},
  howpublished = {[Online] Available: \url{https://github.com/mapbox/polylabel}},
  note = {Accessed: 2018-01-05}
}

@misc{noauthor_github_2019-1,
  author={Intel},
  title = {Github - {{Intel RealSense Post Processing}}},
  year = {2019},
  howpublished = {[Online] Available: \url{https://github.com/IntelRealSense/librealsense/tree/master/examples/post-processing}},
  note = {Accessed: 2019-01-05}
}

@misc{noauthor_github_2020-4,
  author={Intel},
  title = {Github - {{Intel RealSense SDK}}},
  year = {2020},
  howpublished = {[Online] Available: \url{https://github.com/IntelRealSense/librealsense/}},
  note = {Accessed: 2020-06-05}
}


@misc{noauthor_github_2020-8,
  author={Google},
  title = {Github - {{Google Benchmark}}},
  year = {2020},
  howpublished = {[Online] Available: \url{https://github.com/google/benchmark}},
  note = {Accessed: 2020-06-05}
}

@inproceedings{samberg2007implementation,
  title={An implementation of the {ASPRS} {LAS} standard},
  author={Samberg, Andre},
  booktitle={ISPRS Workshop on Laser Scanning and SilviLaser},
  pages={363--372},
  year={2007}
}

@misc{nyc_open_data_building_2018,
  title = {Building {{Footprints}}},
  author = {{NYC Open Data}},
  year = {2018},
  howpublished = {[Online] Available: \url{https://data.cityofnewyork.us/Housing-Development/Building-Footprints/nqwf-w8eh}},
  note = {Visited on 2019-01-14}
}

@incollection{ochoa_fail-safe_2017,
  title = {Fail-{{Safe Navigation}} for {{Autonomous Urban Multicopter Flight}}},
  booktitle = {{{AIAA Information Systems}}-{{AIAA Infotech}} @ {{Aerospace}}},
  author = {Ochoa, Cosme A. and Atkins, Ella M.},
  year = {2017},
  month = jan,
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/6.2017-0222},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\BRHQIIRF\\Ochoa and Atkins - 2017 - Fail-Safe Navigation for Autonomous Urban Multicop.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\VBR2KL7E\\6.html},
  keywords = {planning},
  series = {{{AIAA SciTech Forum}}}
}

@article{oesau_planar_2016,
  title = {Planar {{Shape Detection}} and {{Regularization}} in {{Tandem}}: {{Planar Shape Detection}} and {{Regularization}} in {{Tandem}}},
  shorttitle = {Planar {{Shape Detection}} and {{Regularization}} in {{Tandem}}},
  author = {Oesau, Sven and Lafarge, Florent and Alliez, Pierre},
  year = {2016},
  month = feb,
  volume = {35},
  pages = {203--215},
  issn = {01677055},
  doi = {10.1111/cgf.12720},
  abstract = {We present a method for planar shape detection and regularization from raw point sets. The geometric modeling and processing of man-made environments from measurement data often relies upon robust detection of planar primitive shapes. In addition, the detection and reinforcement of regularities between planar parts is a means to increase resilience to missing or defect-laden data as well as to reduce the complexity of models and algorithms down the modeling pipeline. The main novelty behind our method is to perform detection and regularization in tandem. We first sample a sparse set of seeds uniformly on the input point set, then perform in parallel shape detection through region growing, interleaved with regularization through detection and reinforcement of regular relationships (coplanar, parallel and orthogonal). In addition to addressing the end goal of regularization, such reinforcement also improves data fitting and provides guidance for clustering small parts into larger planar parts. We evaluate our approach against a wide range of inputs and under four criteria: geometric fidelity, coverage, regularity and running times. Our approach compares well with available implementations such as the efficient RANSAC-based approach proposed by Schnabel and co-authors in 2007.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\X9A8D4YH\\Oesau et al. - 2016 - Planar Shape Detection and Regularization in Tande.pdf},
  journal = {Computer Graphics Forum},
  language = {en},
  number = {1}
}

@misc{open_nrw_open_2020,
  title = {Open {{Geo Data}}},
  author = {{Open NRW}},
  year = {2020},
  note = {Data licence: Germany - Zero - Version 2.0: https://www.govdata.de/dl-de/zero-2-0
\par
Lidar Data}
}

@misc{open_source_geospatial_foundation_postgis_2019,
  title = {{{PostGIS}}},
  author = {{Open Source Geospatial Foundation}},
  year = {2019},
  howpublished = {[Online] Available: \url{https://postgis.net/docs/ST_ConcaveHull.html}},
  note = {Visited on 2019-01-14}
}

@misc{openstreetmap_contributors_planet_2017,
  title = {Planet Dump Retrieved from {{https://planet.osm.org}}},
  author = {{OpenStreetMap contributors}},
  year = {2017},
  annotation = {Published:  https://www.openstreetmap.org}
}

@article{paris_modified_2013,
  title = {Modified Half-Edge Data Structure and Its Applications to {{3D}} Mesh Generation for Complex Tube Networks.},
  author = {Paris, Richard},
  year = {2013},
  month = may,
  doi = {10.18297/etd/1094},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\D5AJUDFX\\Paris - 2013 - Modified half-edge data structure and its applicat.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\8C9LNQPA\\1094.html},
  journal = {Electronic Theses and Dissertations},
  keywords = {half edge,mesh}
}

@article{partovi_roof_2017,
  title = {Roof {{Type Selection Based}} on {{Patch}}-Based {{Classification Using Deep Learning}} for {{High Resolution Satellite Imagery}}},
  author = {Partovi, T. and Fraundorfer, F. and Azimi, S. and Marmanis, D. and Reinartz, P.},
  year = {2017},
  volume = {XLII-1/W1},
  pages = {653--657},
  doi = {10.5194/isprs-archives-XLII-1-W1-653-2017},
  journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences}
}

@article{pathak_online_2010,
  title = {Online Three-Dimensional {{SLAM}} by Registration of Large Planar Surface Segments and Closed-Form Pose-Graph Relaxation},
  author = {Pathak, Kaustubh and Birk, Andreas and Vaskevicius, Narunas and Pfingsthorn, Max and Schwertfeger, S{\"o}ren and Poppinga, Jann},
  year = {2010},
  volume = {27},
  pages = {52--84},
  issn = {1556-4967},
  doi = {10.1002/rob.20322},
  abstract = {A fast pose-graph relaxation technique is presented for enhancing the consistency of three-dimensional (3D) maps created by registering large planar surface patches. The surface patches are extracted from point clouds sampled from a 3D range sensor. The plane-based registration method offers an alternative to the state-of-the-art algorithms and provides advantages in terms of robustness, speed, and storage. One of its features is that it results in an accurate determination of rotation, although a lack of predominant surfaces in certain directions may result in translational uncertainty in those directions. Hence, a loop-closing and relaxation problem is formulated that gains significant speed by relaxing only the translational errors and utilizes the full-translation covariance determined during pairwise registration. This leads to a fast 3D simultaneous localization and mapping suited for online operations. The approach is tested in two disaster scenarios that were mapped at the NIST 2008 Response Robot Evaluation Exercise in Disaster City, Texas. The two data sets from a collapsed car park and a flooding disaster consist of 26 and 70 3D scans, respectively. The results of these experiments show that our approach can generate 3D maps without motion estimates by odometry and that it outperforms iterative closest point\textendash based mapping with respect to speed and robustness. \textcopyright{} 2009 Wiley Periodicals, Inc.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.20322},
  copyright = {Copyright \textcopyright{} 2009 Wiley Periodicals, Inc.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\D4C258HC\\Pathak et al. - 2010 - Online three-dimensional SLAM by registration of l.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\BPMAEPT9\\rob.html},
  journal = {Journal of Field Robotics},
  keywords = {planes,SLAM},
  language = {en},
  number = {1}
}

@article{patterson_timely_2014,
  title = {Timely Autonomous Identification of {{UAV}} Safe Landing Zones},
  author = {Patterson, Timothy and McClean, Sally and Morrow, Philip and Parr, Gerard and Luo, Chunbo},
  year = {2014},
  month = sep,
  volume = {32},
  pages = {568--578},
  issn = {0262-8856},
  doi = {10.1016/j.imavis.2014.06.006},
  abstract = {For many applications such as environmental monitoring in the aftermath of a natural disaster and mountain search-and-rescue, swarms of autonomous Unmanned Aerial Vehicles (UAVs) have the potential to provide a highly versatile and often relatively inexpensive sensing platform. Their ability to operate as an `eye-in-the-sky', processing and relaying real-time colour imagery and other sensor readings facilitate the removal of humans from situations which may be considered dull, dangerous or dirty. However, as with manned aircraft they are likely to encounter errors, the most serious of which may require the UAV to land as quickly and safely as possible. Within this paper we therefore present novel work on autonomously identifying Safe Landing Zones (SLZs) which can be utilised upon occurrence of a safety critical event. Safe Landing Zones are detected and subsequently assigned a safety score either solely using multichannel aerial imagery or, whenever practicable by fusing knowledge in the form of Ordnance Survey (OS) map data with such imagery. Given the real-time nature of the problem we subsequently model two SLZ detection options one of which utilises knowledge enabling the UAV to choose an optimal, viable solution. Results are presented based on colour aerial imagery captured during manned flight demonstrating practical potential in the methods discussed.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\R42YKAW3\\Patterson et al. - 2014 - Timely autonomous identification of UAV safe landi.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\Y7S3ZIXR\\S026288561400105X.html},
  journal = {Image and Vision Computing},
  keywords = {Fuzzy logic,Terrain classification,UAV safe landing zone detection,UAV safety},
  language = {en},
  number = {9}
}

@article{paul_flight_2017,
  title = {Flight {{Trajectory Planning}} for {{Fixed}}-{{Wing Aircraft}} in {{Loss}} of {{Thrust Emergencies}}},
  author = {Paul, Saswata and Hole, Frederick and Zytek, Alexandra and Varela, Carlos A.},
  year = {2017},
  month = oct,
  abstract = {Loss of thrust emergencies-e.g., induced by bird/drone strikes or fuel exhaustion-create the need for dynamic data-driven flight trajectory planning to advise pilots or control UAVs. While total loss of thrust trajectories to nearby airports can be pre-computed for all initial points in a 3D flight plan, dynamic aspects such as partial power and airplane surface damage must be considered for accuracy. In this paper, we propose a new Dynamic Data-Driven Avionics Software (DDDAS) approach which during flight updates a damaged aircraft performance model, used in turn to generate plausible flight trajectories to a safe landing site. Our damaged aircraft model is parameterized on a baseline glide ratio for a clean aircraft configuration assuming best gliding airspeed on straight flight. The model predicts purely geometric criteria for flight trajectory generation, namely, glide ratio and turn radius for different bank angles and drag configurations. Given actual aircraft performance data, we dynamically infer the baseline glide ratio to update the damaged aircraft model. Our new flight trajectory generation algorithm thus can significantly improve upon prior Dubins based trajectory generation work by considering these data-driven geometric criteria. We further introduce a trajectory utility function to rank trajectories for safety. As a use case, we consider the Hudson River ditching of US Airways 1549 in January 2009 using a flight simulator to evaluate our trajectories and to get sensor data. In this case, a baseline glide ratio of 17.25:1 enabled us to generate trajectories up to 28 seconds after the birds strike, whereas, a 19:1 baseline glide ratio enabled us to generate trajectories up to 36 seconds after the birds strike. DDDAS can significantly improve the accuracy of generated flight trajectories thereby enabling better decision support systems for pilots in emergency conditions.},
  archivePrefix = {arXiv},
  eprint = {1711.00716},
  eprinttype = {arxiv},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\WUL227EX\\Paul et al. - 2017 - Flight Trajectory Planning for Fixed-Wing Aircraft.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\CRQ52WEG\\1711.html},
  journal = {arXiv:1711.00716 [cs]},
  keywords = {Electrical Engineering and Systems Science - Systems and Control},
  note = {Comment: This work was accepted as a full paper and presented in the Second International Conference on InfoSymbiotics / DDDAS (Dynamic Data Driven Applications Systems) held at MIT, Cambridge, Massachusetts in August, 2017},
  primaryClass = {cs}
}

@article{pedregosa_scikit-learn_2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  year = {2011},
  volume = {12},
  pages = {2825--2830},
  journal = {Journal of Machine Learning Research}
}

@inproceedings{pham_geometrically_2016-1,
  title = {Geometrically Consistent Plane Extraction for Dense Indoor {{3D}} Maps Segmentation},
  booktitle = {2016 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Pham, T. T. and Eich, M. and Reid, I. and Wyeth, G.},
  year = {2016},
  month = oct,
  pages = {4199--4204},
  issn = {2153-0866},
  doi = {10.1109/IROS.2016.7759618},
  abstract = {Modern SLAM systems with a depth sensor are able to reliably reconstruct dense 3D geometric maps of indoor scenes. Representing these maps in terms of meaningful entities is a step towards building semantic maps for autonomous robots. One approach is to segment the 3D maps into semantic objects using Conditional Random Fields (CRF), which requires large 3D ground truth datasets to train the classification model. Additionally, the CRF inference is often computationally expensive. In this paper, we present an unsupervised geometric-based approach for the segmentation of 3D point clouds into objects and meaningful scene structures. We approximate an input point cloud by an adjacency graph over surface patches, whose edges are then classified as being either on or off. We devise an effective classifier which utilises both global planar surfaces and local surface convexities for edge classification. More importantly, we propose a novel global plane extraction algorithm for robustly discovering the underlying planes in the scene. Our algorithm is able to enforce the extracted planes to be mutually orthogonal or parallel which conforms usually with human-made indoor environments. We reconstruct 654 3D indoor scenes from NYUv2 sequences to validate the efficiency and effectiveness of our segmentation method.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\VLMLIKNV\\Pham et al. - 2016 - Geometrically consistent plane extraction for dens.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\W2PYE5DQ\\7759618.html},
  keywords = {3D ground truth datasets,adjacency graph,autonomous robots,classification model,Clustering algorithms,conditional random fields,CRF,dense indoor 3D maps segmentation,edge classification,feature extraction,geometrically consistent plane extraction,global plane extraction algorithm,graph theory,image classification,Image reconstruction,image segmentation,Image segmentation,image sequences,NYUv2 sequences,robot vision,Robots,Semantics,simultaneous localization and mapping,SLAM (robots),SLAM systems,Surface reconstruction,Three-dimensional displays,unsupervised geometric-based approach}
}

@inproceedings{pham_scenecut_2018,
  title = {{{SceneCut}}: {{Joint Geometric}} and {{Object Segmentation}} for {{Indoor Scenes}}},
  shorttitle = {{{SceneCut}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Pham, Trung T. and Do, Thanh-Toan and S{\"u}nderhauf, Niko and Reid, Ian},
  year = {2018},
  month = may,
  pages = {3213--3220},
  issn = {2577-087X},
  doi = {10.1109/ICRA.2018.8461108},
  abstract = {This paper presents SceneCut, a novel approach to jointly discover previously unseen objects and non-object surfaces using a single RGB-D image. SceneCut's joint reasoning over scene semantics and geometry allows a robot to detect and segment object instances in complex scenes where modern deep learning-based methods either fail to separate object instances, or fail to detect objects that were not seen during training. SceneCut automatically decomposes a scene into meaningful regions which either represent objects or scene surfaces. The decomposition is qualified by an unified energy function over objectness and geometric fitting. We show how this energy function can be optimized efficiently by utilizing hierarchical segmentation trees. Moreover, we leverage a pre-trained convolutional oriented boundary network to predict accurate boundaries from images, which are used to construct high-quality region hierarchies. We evaluate SceneCut on several different indoor environments, and the results show that SceneCut significantly outperforms all the existing methods.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\9JLK2D3M\\Pham et al. - 2018 - SceneCut Joint Geometric and Object Segmentation .pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\BGCXSMJD\\8461108.html},
  keywords = {convolutional oriented boundary network,deep learning,deep learning-based methods,geometry,hierarchical segmentation trees,image colour analysis,image representation,image segmentation,Image segmentation,indoor scenes,joint geometric,learning (artificial intelligence),nonobject surfaces,object detection,object segmentation,Object segmentation,RGB-D image,Robots,scene semantics,scene surfaces,SceneCut,Semantics,Silicon,Three-dimensional displays,Training,unified energy function,unseen objects}
}

@inproceedings{poppinga_fast_2008,
  title = {Fast Plane Detection and Polygonalization in Noisy {{3D}} Range Images},
  booktitle = {2008 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Poppinga, J. and Vaskevicius, N. and Birk, A. and Pathak, K.},
  year = {2008},
  month = sep,
  pages = {3378--3383},
  publisher = {{IEEE}},
  address = {{Nice}},
  doi = {10.1109/IROS.2008.4650729},
  abstract = {A very fast but nevertheless accurate approach for surface extraction from noisy 3D point clouds is presented. It consists of two parts, namely a plane fitting and a polygonalization step. Both exploit the sequential nature of 3D data acquisition on mobile robots in form of range images. For the plane fitting, this is used to revise the standard mathematical formulation to an incremental version, which allows a linear computation. For the polygonalization, the neighborhood relation in range images is exploited. Experiments are presented using a time-of-flight range camera in form of a Swissranger SR-3000. Results include lab scenes as well as data from two runs of the rescue robot league at the RoboCup German Open 2007 with 1,414, respectively 2,343 sensor snapshots. The 36{$\cdot$}106, respectively 59{$\cdot$}106 points from the two point clouds are reduced to about 14{$\cdot$}103, respectively 23{$\cdot$}103 planes with only about 0.2 sec of total computation time per snapshot while the robot moves along.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\DUVYGNDB\\Poppinga et al. - 2008 - Fast plane detection and polygonalization in noisy.pdf},
  isbn = {978-1-4244-2057-5 978-1-4244-2058-2},
  language = {en}
}

@inproceedings{prevot_uas_2016,
  title = {{{UAS Traffic Management}} ({{UTM}}) {{Concept}} of {{Operations}} to {{Safely Enable Low Altitude Flight Operations}}},
  booktitle = {16th {{AIAA Aviation Technology}}, {{Integration}}, and {{Operations Conference}}},
  author = {Prevot, Thomas and Rios, Joseph and Kopardekar, Parimal and III, John E. Robinson and Johnson, Marcus and Jung, Jaewoo},
  year = {2016},
  month = jun,
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/6.2016-3292}
}

@article{primatesta_ground_2020,
  title = {Ground {{Risk Map}} for {{Unmanned Aircraft}} in {{Urban Environments}}},
  author = {Primatesta, Stefano and Rizzo, Alessandro and {la Cour-Harbo}, Anders},
  year = {2020},
  month = mar,
  volume = {97},
  pages = {489--509},
  issn = {1573-0409},
  doi = {10.1007/s10846-019-01015-z},
  abstract = {The large diversity of unmanned aircraft requires a suitable and proper risk assessment. In this paper, we propose the use of risk maps to define the risk associated to accidents with unmanned aircraft. It is a two-dimensional location-based map that quantifies the risk to the population on ground of flight operations over a specified area. The risk map is generated through a probabilistic approach and combines several layers, including population density, sheltering factor, no-fly zones, and obstacles. Each element of the risk map has associated a risk value that quantifies the risk of flying over a specific location. Risk values are defined by a risk assessment process using different uncontrolled descent events, drone parameters, environmental characteristics, as well as uncertainties on parameters. The risk map is able to quantify the risk of large areas, such as urban environments, and allows for easy identification of high and low-risk locations. The map is a tool for informed decision making, and our results report some examples of risk map with different aircraft in a realistic urban environment.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\947P9U9T\\Primatesta et al. - 2020 - Ground Risk Map for Unmanned Aircraft in Urban Env.pdf},
  journal = {Journal of Intelligent \& Robotic Systems},
  language = {en},
  number = {3}
}

@misc{proj_contributors_proj_2018,
  title = {{{PROJ}} Coordinate Transformation Software Library},
  author = {{PROJ contributors}},
  year = {2018},
  publisher = {{Open Source Geospatial Foundation}}
}

@incollection{qi_pointnet_2017-1,
  title = {{{PointNet}}++: {{Deep Hierarchical Feature Learning}} on {{Point Sets}} in a {{Metric Space}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {5099--5108},
  publisher = {{Curran Associates, Inc.}}
}

@article{richard_shewchuk_adaptive_1997,
  title = {Adaptive {{Precision Floating}}-{{Point Arithmetic}} and {{Fast Robust Geometric Predicates}}},
  author = {Richard Shewchuk, Jonathan},
  year = {1997},
  month = oct,
  volume = {18},
  pages = {305--363},
  issn = {1432-0444},
  doi = {10.1007/PL00009321},
  abstract = {. Exact computer arithmetic has a variety of uses, including the robust implementation of geometric algorithms. This article has three purposes. The first is to offer fast software-level algorithms for exact addition and multiplication of arbitrary precision floating-point values. The second is to propose a technique for adaptive precision arithmetic that can often speed these algorithms when they are used to perform multiprecision calculations that do not always require exact arithmetic, but must satisfy some error bound. The third is to use these techniques to develop implementations of several common geometric calculations whose required degree of accuracy depends on their inputs. These robust geometric predicates are adaptive; their running time depends on the degree of uncertainty of the result, and is usually small. These algorithms work on computers whose floating-point arithmetic uses radix two and exact rounding, including machines complying with the IEEE 754 standard. The inputs to the predicates may be arbitrary single or double precision floating-point numbers. C code is publicly available for the two-dimensional and three-dimensional orientation and incircle tests, and robust Delaunay triangulation using these tests. Timings of the implementations demonstrate their effectiveness.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\7TMT75CV\\Richard Shewchuk - 1997 - Adaptive Precision Floating-Point Arithmetic and F.pdf},
  journal = {Discrete \& Computational Geometry},
  keywords = {Computer Arithmetic,Delaunay Triangulation,Double Precision,Exact Computer,Geometric Calculation,geometry robust predicates},
  language = {en},
  number = {3}
}

@article{rottensteiner_results_2014,
  title = {Results of the {{ISPRS}} Benchmark on Urban Object Detection and {{3D}} Building Reconstruction},
  author = {Rottensteiner, Franz and Sohn, Gunho and Gerke, Markus and Wegner, Jan Dirk and Breitkopf, Uwe and Jung, Jaewook},
  year = {2014},
  month = jul,
  volume = {93},
  pages = {256--271},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2013.10.004},
  abstract = {For more than two decades, many efforts have been made to develop methods for extracting urban objects from data acquired by airborne sensors. In order to make the results of such algorithms more comparable, benchmarking data sets are of paramount importance. Such a data set, consisting of airborne image and laserscanner data, has been made available to the scientific community by ISPRS WGIII/4. Researchers were encouraged to submit their results of urban object detection and 3D building reconstruction, which were evaluated based on reference data. This paper presents the outcomes of the evaluation for building detection, tree detection, and 3D building reconstruction. The results achieved by different methods are compared and analysed to identify promising strategies for automatic urban object extraction from current airborne sensor data, but also common problems of state-of-the-art methods.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\HALUA2T7\\Rottensteiner et al. - 2014 - Results of the ISPRS benchmark on urban object det.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\55KZN6R7\\S0924271613002268.html},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  keywords = {3D building reconstruction,Aerial imagery,Automatic object extraction,Benchmarking test,Evaluation,Laser scanning},
  language = {en}
}

@inproceedings{rusinkiewicz_efficient_2001,
  title = {Efficient Variants of the {{ICP}} Algorithm},
  booktitle = {Proceedings {{Third International Conference}} on 3-{{D Digital Imaging}} and {{Modeling}}},
  author = {Rusinkiewicz, S. and Levoy, M.},
  year = {2001},
  month = may,
  pages = {145--152},
  doi = {10.1109/IM.2001.924423},
  abstract = {The ICP (Iterative Closest Point) algorithm is widely used for geometric alignment of three-dimensional models when an initial estimate of the relative pose is known. Many variants of ICP have been proposed, affecting all phases of the algorithm from the selection and matching of points to the minimization strategy. We enumerate and classify many of these variants, and evaluate their effect on the speed with which the correct alignment is reached. In order to improve convergence for nearly-flat meshes with small features, such as inscribed surfaces, we introduce a new variant based on uniform sampling of the space of normals. We conclude by proposing a combination of ICP variants optimized for high speed. We demonstrate an implementation that is able to align two range images in a few tens of milliseconds, assuming a good initial guess. This capability has potential application to real-time 3D model acquisition and model-based tracking.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\WBSNT6M6\\Rusinkiewicz and Levoy - 2001 - Efficient variants of the ICP algorithm.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\J5MVU7E2\\924423.html},
  keywords = {Convergence,distance measurement,geometric alignment,Geometry,icp,image processing,Image sampling,inscribed surfaces,Iterative algorithms,iterative closest point algorithm,Iterative closest point algorithm,Iterative methods,Layout,minimisation,Minimization methods,minimization strategy,model-based tracking,nearly-flat meshes,plane,range images,real-time 3D model acquisition,real-time systems,Rough surfaces,Solid modeling,three-dimensional models,uniform sampling}
}

@article{russakovsky_imagenet_2015,
  title = {{{ImageNet Large Scale Visual Recognition Challenge}}},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and {Fei-Fei}, Li},
  year = {2015},
  month = dec,
  volume = {115},
  pages = {211--252},
  issn = {1573-1405},
  doi = {10.1007/s11263-015-0816-y},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5~years of the challenge, and propose future directions and improvements.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\RQTFB8S7\\Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf},
  journal = {International Journal of Computer Vision},
  language = {en},
  number = {3}
}

@inproceedings{saha_planning_2003,
  title = {Planning Multi-Goal Tours for Robot Arms},
  booktitle = {2003 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{Cat}}. {{No}}.{{03CH37422}})},
  author = {Saha, M. and {Sanchez-Ante}, G. and Latombe, J.-},
  year = {2003},
  month = sep,
  volume = {3},
  pages = {3797-3803 vol.3},
  issn = {1050-4729},
  doi = {10.1109/ROBOT.2003.1242179},
  abstract = {This paper considers the following multi-goal motion planning problem: a robot arm must reach several goal configurations in some sequence, but this sequence is not given. Instead, the robot's planner must compute an optimal or near-optimal path through the goals. This problem occurs, for instance, in spot-welding, inspection, and measurement tasks. It combines two computationally hard sub-problems: the shortest-path and traveling-salesman problems. This paper describes a greedy algorithm that operates under the assumption that the number of goals is relatively small (a few dozen at most) and the computational cost of finding a good path between two goals dominates that of finding a good tour in a graph with edges of given costs. Although the algorithm computes a quadratic number of goal-to-goal paths in the worst case, it is much faster in practice.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\4BKFVS9V\\Saha et al. - 2003 - Planning multi-goal tours for robot arms.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\4EEAYM39\\1242179.html},
  keywords = {algorithm theory,Computational efficiency,Computer science,goal-to-goal paths,greedy algorithm,Greedy algorithms,industrial manipulators,Inspection,Manipulators,Motion planning,multigoal motion planning,optimal path planning,Orbital robotics,path planning,quadratic number,robot arms,Robotic assembly,Robots,shortest path problem,spot welding,traveling salesman problems,travelling salesman problems,Welding}
}

@inproceedings{salas-moreno_dense_2014,
  title = {Dense Planar {{SLAM}}},
  booktitle = {2014 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality}} ({{ISMAR}})},
  author = {{Salas-Moreno}, Renato F. and Glocken, Ben and Kelly, Paul H. J. and Davison, Andrew J.},
  year = {2014},
  month = sep,
  pages = {157--164},
  issn = {null},
  doi = {10.1109/ISMAR.2014.6948422},
  abstract = {Using higher-level entities during mapping has the potential to improve camera localisation performance and give substantial perception capabilities to real-time 3D SLAM systems. We present an efficient new real-time approach which densely maps an environment using bounded planes and surfels extracted from depth images (like those produced by RGB-D sensors or dense multi-view stereo reconstruction). Our method offers the every-pixel descriptive power of the latest dense SLAM approaches, but takes advantage directly of the planarity of many parts of real-world scenes via a data-driven process to directly regularize planar regions and represent their accurate extent efficiently using an occupancy approach with on-line compression. Large areas can be mapped efficiently and with useful semantic planar structure which enables intuitive and useful AR applications such as using any wall or other planar surface in a scene to display a user's content.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\VRVNENN8\\Salas-Moreno et al. - 2014 - Dense planar SLAM.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\GXTAAIDW\\6948422.html},
  keywords = {3D SLAM system,AR applications,Artificial,augmented,augmented reality,bounded planes,Cameras,Computing methodologies [Reconstruction]. Computing methodologies [Image Processing and Computer Vision]: Segmentation. Information Systems [Information Interfaces and Presentation],Computing methodologies [Scene understanding],dense multiview stereo reconstruction,dense planar SLAM,feature extraction,image reconstruction,Indexes,Noise,Real-time systems,red-green-blue-depth sensor,RGB-D sensors,Simultaneous localization and mapping,simultaneous localization and planning,slam,SLAM (robots),stereo image processing,surfel extraction,surfels,Three-dimensional displays,virtual realities}
}

@phdthesis{samosky_sectionviewsystem_1993,
  title = {{{SectionView}}\textendash{{A}} System for Interactively Specifying and Visualizing Sections through Three-Dimensional Medical Image Data},
  author = {Samosky, Joseph Thomas},
  year = {1993},
  school = {Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science},
  type = {{{PhD Thesis}}}
}

@incollection{sankararaman_towards_2017,
  title = {Towards {{A Computational Framework}} for {{Autonomous Decision}}-{{Making}} in {{Unmanned Aerial Vehicles}}},
  booktitle = {{{AIAA Information Systems}}-{{AIAA Infotech}} @ {{Aerospace}}},
  author = {Sankararaman, Shankar},
  year = {2017},
  month = jan,
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/6.2017-0446},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\2UVKWRHT\\Sankararaman - 2017 - Towards A Computational Framework for Autonomous D.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\KX75HFYD\\6.html},
  series = {{{AIAA SciTech Forum}}}
}

@article{sarne_multi-goal_2010,
  title = {Multi-Goal Economic Search Using Dynamic Search Structures},
  author = {Sarne, David and Manisterski, Efrat and Kraus, Sarit},
  year = {2010},
  month = sep,
  volume = {21},
  pages = {204--236},
  issn = {1573-7454},
  doi = {10.1007/s10458-009-9111-z},
  abstract = {This paper investigates cooperative search strategies for agents engaged in costly search in a complex environment. Searching cooperatively, several search goals can be satisfied within a single search effort. Given the searchers' preferences, the goal is to conduct a search in a way that the expected overall utility out of the set of opportunities found (e.g., products when operating in a market) minus the costs associated with finding that set is maximized. This search scheme, given in the context of a group search, applies also to scenarios where a single agent has to search for a set of items for satisfying several different goals. The uniqueness of the proposed mechanism is in the ability to partition the group of agents/goals into sub-groups where the search continues for each group autonomously. As we show throughout the paper, this strategy is favorable as it weakly dominates (i.e., can improve but never worsen) cooperative and autonomous search techniques. The paper presents a comprehensive analysis of the new search method and highlights the specific characteristics of the optimal search strategy. Furthermore, we introduce innovative algorithms for extracting the optimal search strategy in a range of common environments, that eliminates the computational overhead associated with the use of the partitioning technique.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\DIZUGDRA\\Sarne et al. - 2010 - Multi-goal economic search using dynamic search st.pdf},
  journal = {Autonomous Agents and Multi-Agent Systems},
  language = {en},
  number = {2}
}

@inproceedings{schaefer_maximum_2019,
  title = {A {{Maximum Likelihood Approach}} to {{Extract Finite Planes}} from 3-{{D Laser Scans}}},
  booktitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Schaefer, Alexander and Vertens, Johan and Buscher, Daniel and Burgard, Wolfram},
  year = {2019},
  month = may,
  pages = {72--78},
  publisher = {{IEEE}},
  address = {{Montreal, QC, Canada}},
  doi = {10.1109/ICRA.2019.8794318},
  abstract = {Whether it is object detection, model reconstruction, laser odometry, or point cloud registration: Plane extraction is a vital component of many robotic systems. In this paper, we propose a strictly probabilistic method to detect finite planes in organized 3-D laser range scans. An agglomerative hierarchical clustering technique, our algorithm builds planes from bottom up, always extending a plane by the point that decreases the measurement likelihood of the scan the least. In contrast to most related methods, which rely on heuristics like orthogonal point-to-plane distance, we leverage the ray path information to compute the measurement likelihood. We evaluate our approach not only on the popular SegComp benchmark, but also provide a challenging synthetic dataset that overcomes SegComp's deficiencies. Both our implementation and the suggested dataset are available at [1].},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\VES4XRC6\\Schaefer et al. - 2019 - A Maximum Likelihood Approach to Extract Finite Pl.pdf},
  isbn = {978-1-5386-6027-0},
  language = {en}
}

@article{schmidhuber_deep_2015,
  title = {Deep Learning in Neural Networks: {{An}} Overview},
  shorttitle = {Deep Learning in Neural Networks},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2015},
  month = jan,
  volume = {61},
  pages = {85--117},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2014.09.003},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\CDV3ELWC\\Schmidhuber - 2015 - Deep learning in neural networks An overview.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\IH7GX4QR\\S0893608014002135.html},
  journal = {Neural Networks},
  keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning},
  language = {en}
}

@article{stevenson_estimated_2015-1,
  title = {Estimated Levels of Safety for Small Unmanned Aerial Vehicles and Risk Mitigation Strategies},
  author = {Stevenson, Jonathan D. and O'Young, Siu and Rolland, Luc},
  year = {2015},
  month = sep,
  publisher = {{NRC Research Press  http://www.nrcresearchpress.com}},
  doi = {10.1139/juvs-2014-0016},
  abstract = {In this paper the risks posed by small unmanned aerial vehicles (UAV)1 are determined using quantitative methods, to calculate the estimated level of safety (ELS). The aim is to determine if the us...},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\QV6DRX77\\juvs-2014-0016.html},
  journal = {Journal of Unmanned Vehicle Systems},
  language = {en}
}

@article{sun_fast_2007,
  title = {Fast and {{Effective Feature}}-{{Preserving Mesh Denoising}}},
  author = {Sun, Xianfang and Rosin, Paul L. and Martin, Ralph and Langbein, Frank},
  year = {2007},
  month = sep,
  volume = {13},
  pages = {925--938},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10.1109/TVCG.2007.1065},
  abstract = {We present a simple and fast mesh denoising method, which can remove noise effectively, while preserving mesh features such as sharp edges and corners. The method consists of two stages. Firstly, noisy face normals are filtered iteratively by weighted averaging of neighboring face normals. Secondly, vertex positions are iteratively updated to agree with the denoised face normals. The weight function used during normal filtering is much simpler than that used in previous similar approaches, being simply a trimmed quadratic. This makes the algorithm both fast and simple to implement. Vertex position updating is based on the integration of surface normals using a least-squares error criterion. Like previous algorithms, we solve the least-squares problem by gradient descent, but whereas previous methods needed user input to determine the iteration step size, we determine it automatically. In addition, we prove the convergence of the vertex position updating approach. Analysis and experiments show the advantages of our proposed method over various earlier surface denoising methods.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\VA3TCEUT\\Sun et al. - 2007 - Fast and Effective Feature-Preserving Mesh Denoisi.pdf},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  language = {en},
  number = {5}
}

@article{suveg_reconstruction_2004,
  title = {Reconstruction of {{3D}} Building Models from Aerial Images and Maps},
  author = {Suveg, Ildiko and Vosselman, George},
  year = {2004},
  month = jan,
  volume = {58},
  pages = {202--224},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2003.09.006},
  abstract = {Automatic 3D building reconstruction has becoming increasingly important for a number of applications. The reconstruction of buildings using only aerial images as data source has been proven to be a very difficult problem. The complexity of the reconstruction can be greatly reduced by combining the aerial images with other data sources. In this paper, we describe a 3D building reconstruction method that integrates the aerial image analysis with information from large-scale 2D Geographic Information System (GIS) databases and domain knowledge. By combining the images with GIS data, the specific strengths of both the images (high resolution, accuracy, and large-information content) and the GIS data (relatively simple interpretation) are exploited.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\AD4QYXKE\\Suveg and Vosselman - 2004 - Reconstruction of 3D building models from aerial i.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\7UBEPY7Y\\S0924271603000583.html},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  keywords = {3D reconstruction,aerial images,domain knowledge,evaluation,fitting,GIS database,hypotheses generation},
  language = {en},
  number = {3},
  series = {Integration of {{Geodata}} and {{Imagery}} for {{Automated Refinement}} and {{Update}} of {{Spatial Databases}}}
}

@inproceedings{szegedy_inception-v4_2017,
  title = {Inception-v4, Inception-{{ResNet}} and the Impact of Residual Connections on Learning},
  booktitle = {Proceedings of the {{Thirty}}-{{First AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
  year = {2017},
  month = feb,
  pages = {4278--4284},
  publisher = {{AAAI Press}},
  address = {{San Francisco, California, USA}},
  abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08\% top-5 error on the test set of the ImageNet classification (CLS) challenge.},
  series = {{{AAAI}}'17}
}

@inproceedings{szegedy_rethinking_2016,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Szegedy, C. and Vanhoucke, V. and Ioffe, S. and Shlens, J. and Wojna, Z.},
  year = {2016},
  month = jun,
  pages = {2818--2826},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.308},
  abstract = {Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2\% top-1 and 5:6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5\% top-5 error and 17:3\% top-1 error on the validation set and 3:6\% top-5 error on the official test set.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\EYRL6HM9\\Szegedy et al. - 2016 - Rethinking the Inception Architecture for Computer.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\HS6UKQLD\\7780677.html},
  keywords = {Benchmark testing,Computational efficiency,Computational modeling,Computer architecture,computer vision,Computer vision,Convolution,deep convolutional networks,ILSVRC 2012 classification challenge validation set,image classification,inception architecture,neural nets,Training}
}

@article{tack_3d_2012,
  title = {{{3D}} Building Reconstruction Based on given Ground Plan Information and Surface Models Extracted from Spaceborne Imagery},
  author = {Tack, Frederik and Buyuksalih, Gurcan and Goossens, Rudi},
  year = {2012},
  month = jan,
  volume = {67},
  pages = {52--64},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2011.10.003},
  abstract = {3D surface models have gained field as an important tool for urban planning and mapping. However, urban environments have a complex nature to model and they provide a challenge to investigate the current limits of automatic digital surface modeling from high resolution satellite imagery. An approach is introduced to improve a 3D surface model, extracted photogrammetrically from satellite imagery, based on the geometric building information embodied in existing 2D ground plans. First buildings are clipped from the extracted DSM based on the 2D polygonal building ground plans. To generate prismatic shaped structures with vertical walls and flat roofs, building shape is retrieved from the cadastre database while elevation information is extracted from the DSM. Within each 2D building boundary, a constant roof height is extracted based on statistical calculations of the height values. After buildings are extracted from the initial surface model, the remaining DSM is further processed to simplify to a smooth DTM that reflects bare ground, without artifacts, local relief, vegetation, cars and city furniture. In a next phase, both models are merged to yield an integrated city model or generalized DSM. The accuracy of the generalized surface model is assessed according to a quantitative-statistical analysis by comparison with two different types of reference data.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\CUQV8ZBG\\Tack et al. - 2012 - 3D building reconstruction based on given ground p.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\RXQDSRDE\\S0924271611001134.html},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  keywords = {2D ground plans,3D city modeling,DSM,High resolution satellite imagery,Photogrammetry},
  language = {en}
}

@article{taillandier_automatic_2005,
  title = {Automatic Building Reconstruction from Cadastral Maps and Aerial Images},
  author = {Taillandier, Franck},
  year = {2005},
  volume = {36},
  pages = {W24},
  journal = {International Archives of Photogrammetry and Remote Sensing},
  number = {Part 3}
}

@article{tang_deep_2015,
  title = {Deep {{Learning}} Using {{Linear Support Vector Machines}}},
  author = {Tang, Yichuan},
  year = {2015},
  month = feb,
  abstract = {Recently, fully-connected and convolutional neural networks have been trained to achieve state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics. For classification tasks, most of these "deep learning" models employ the softmax activation function for prediction and minimize cross-entropy loss. In this paper, we demonstrate a small but consistent advantage of replacing the softmax layer with a linear support vector machine. Learning minimizes a margin-based loss instead of the cross-entropy loss. While there have been various combinations of neural nets and SVMs in prior art, our results using L2-SVMs show that by simply replacing softmax with linear SVMs gives significant gains on popular deep learning datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop's face expression recognition challenge.},
  archivePrefix = {arXiv},
  eprint = {1306.0239},
  eprinttype = {arxiv},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\LYKKBDTI\\Tang - 2015 - Deep Learning using Linear Support Vector Machines.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\NNLC5RI6\\1306.html},
  journal = {arXiv:1306.0239 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Contribution to the ICML 2013 Challenges in Representation Learning Workshop},
  primaryClass = {cs, stat}
}

@inproceedings{taubin_curve_1995,
  title = {Curve and Surface Smoothing without Shrinkage},
  booktitle = {Proceedings of {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Taubin, G.},
  year = {1995},
  month = jun,
  pages = {852--857},
  doi = {10.1109/ICCV.1995.466848},
  abstract = {For a number of computational purposes, including visualization of scientific data and registration of multimodal medical data, smooth curves must be approximated by polygonal curves, and surfaces by polyhedral surfaces. An inherent problem of these approximation algorithms is that the resulting curves and surfaces appear faceted. Boundary-following and iso-surface construction algorithms are typical examples. To reduce the apparent faceting, smoothing methods are used. In this paper, we introduce a new method for smoothing piecewise linear shapes of arbitrary dimension and topology. This new method is in fact a linear low-pass filter that removes high-curvature variations, and does not produce shrinkage. Its computational complexity is linear in the number of edges or faces of the shape, and the required storage is linear in the number of vertices.{$<>$}},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\EHTEXJNW\\Taubin - 1995 - Curve and surface smoothing without shrinkage.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\RJTP4AVH\\466848.html},
  keywords = {approximation algorithms,Approximation algorithms,boundary-following algorithms,computational complexity,Computational complexity,computational geometry,computer vision,curve fitting,curve smoothing,data visualisation,Data visualization,edges,faces,faceting,high curvature variations,image registration,iso-surface construction algorithms,linear low-pass filter,Low pass filters,medical image processing,multimodal medical data registration,Nonlinear filters,Piecewise linear approximation,piecewise linear shapes,Piecewise linear techniques,piecewise-linear techniques,polygonal curves,polyhedral surfaces,required storage,scientific data visualization,Shape,shrinkage,smoothing methods,Smoothing methods,surface smoothing,topology,Topology,vertices}
}

@article{ten_harmsel_emergency_2017,
  title = {Emergency {{Flight Planning}} for an {{Energy}}-{{Constrained Multicopter}}},
  author = {Ten Harmsel, Alec J. and Olson, Isaac J. and Atkins, Ella M.},
  year = {2017},
  month = jan,
  volume = {85},
  pages = {145--165},
  issn = {1573-0409},
  doi = {10.1007/s10846-016-0370-z},
  abstract = {Small Unmanned Aircraft Systems (UAS) have diverse commercial applications. Risk mitigation techniques must be developed to minimize the probability of harm to persons and property in the vicinity of the aircraft. This paper presents an emergency flight planner combining sensor-based and map-based elements to collectively plan a landing path for a UAS that experiences an unexpected low energy condition while flying over a populated area. Focus is placed in this work on the use of public databases of population distribution, structure locations, and terrain to create an efficient-to-access cost map of the data. Safe landing plans are generated with an A* search algorithm shown to be feasible for real-time use with the cost map. Simulation-based case studies are presented of a quadrotor UAS operating within New York City to illustrate how different cost terms impact optimal path characteristics.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\FLH6DRP5\\Ten Harmsel et al. - 2017 - Emergency Flight Planning for an Energy-Constraine.pdf},
  journal = {Journal of Intelligent \& Robotic Systems},
  language = {en},
  number = {1}
}

@book{the_cgal_project_cgal_2019,
  title = {{{CGAL User}} and {{Reference Manual}}},
  author = {{The CGAL Project}},
  year = {2019},
  edition = {4.14},
  publisher = {{CGAL Editorial Board}}
}

@inproceedings{theodore_flight_2006,
  title = {Flight {{Trials}} of a {{Rotorcraft Unmanned Aerial Vehicle Landing Autonomously}} at {{Unprepared Sites}}},
  booktitle = {Annual {{Forum Proc}}. {{American Helicopter Society}}},
  author = {Theodore, C. and Rowley, D. and Ansar, A. and Matthies, L. and Goldberg, S. and Hubbard, D. and Whalley, M.},
  year = {2006},
  volume = {62},
  pages = {1250}
}

@article{toony_describing_2015,
  title = {Describing {{3D Geometric Primitives Using}} the {{Gaussian Sphere}} and the {{Gaussian Accumulator}}},
  author = {Toony, Zahra and Laurendeau, Denis and Gagn{\'e}, Christian},
  year = {2015},
  month = nov,
  volume = {6},
  pages = {42},
  issn = {2092-6731},
  doi = {10.1007/s13319-015-0074-3},
  abstract = {Most complex object models are composed of basic parts or primitives. Being able to decompose a complex 3D model into such basic primitives is an important step in reverse engineering. Even when an algorithm can segment a complex model into its primitives, a description technique is still needed in order to identify the type of each primitive. Most feature extraction methods fail to describe these basic primitives or need a trained classifier on a database of prepared data to perform this identification. In this paper, we propose a method that can describe basic primitives such as planes, cones, cylinders, spheres, and tori as well as partial models of the latter four primitives. To achieve this task, we combine the concept of Gaussian sphere to a new concept introduced in this paper: the Gaussian accumulator. Comparison of the results of our method with other feature extractors reveals that our approach can distinguish all of these primitives from each other including partial models. Our method was also tested on real scanned data with noise and missing areas. The results show that our method is able to distinguish all of these models as well.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\W57YPWTT\\Toony et al. - 2015 - Describing 3D Geometric Primitives Using the Gauss.pdf},
  journal = {3D Research},
  keywords = {gaussian,sphere,triangle},
  language = {en},
  number = {4}
}

@article{torr_mlesac_2000,
  title = {{{MLESAC}}: {{A New Robust Estimator}} with {{Application}} to {{Estimating Image Geometry}}},
  shorttitle = {{{MLESAC}}},
  author = {Torr, P.H.S. and Zisserman, A.},
  year = {2000},
  month = apr,
  volume = {78},
  pages = {138--156},
  issn = {10773142},
  doi = {10.1006/cviu.1999.0832},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\PSC4X23V\\Torr and Zisserman - 2000 - MLESAC A New Robust Estimator with Application to.pdf},
  journal = {Computer Vision and Image Understanding},
  language = {en},
  number = {1}
}

@inproceedings{trevor2013efficient,
  title = {Efficient Organized Point Cloud Segmentation with Connected Components},
  booktitle = {3rd Workshop on Semantic Perception Mapping and Exploration ({{SPME}}), Karlsruhe, Germany},
  author = {Trevor, A and Gedikli, Suat and Rusu, R and Christensen, H},
  year = {2013}
}

@misc{usgs_lidar_2018_ny,
  title = {{{LIDAR Point Cloud NY CMPG}} 2013},
  author = {{USGS}},
  year = {2018},
  howpublished = {[Online] Available: \url{ftp://rockyftp.cr.usgs.gov/vdelivery/Datasets/Staged/Elevation/LPC/Projects/USGS\_Lidar\_Point\_Cloud\_NY\_CMPG\_2013\_LAS\_2015/metadata}},
  note = {Accessed: 2018-09-05}
}

@misc{usgs_lidar_2018-annarbor,
  title = {Lidar {{Point Cloud}}; {{Washtenaw County}}, {{MI}}},
  author = {{USGS}},
  year = {2018},
  howpublished = {[Online] Available: \url{ftp://rockyftp.cr.usgs.gov/vdelivery/Datasets/Staged/Elevation/LPC/Projects/MI\_WashtenawCo\_2009/laz}},
  note = {Accessed: 2018-09-05}
}

@misc{lidar_germany,
  author = {{Open NRW}},
  title = {{Open Geo Data}},
  year = {2017},
  howpublished = {[Online] Available: \url{https://www.opengeodata.nrw.de/produkte/geobasis/dom/dom1l/}},
  note = {Accessed: 2017-09-05}
}

@article{van_der_walt_scikit-image_2014,
  title = {Scikit-Image: Image Processing in {{Python}}},
  author = {{Van der Walt}, St{\'e}fan and Sch{\"o}nberger, Johannes L. and {Nunez-Iglesias}, Juan and Boulogne, Fran{\c c}ois and Warner, Joshua D. and Yager, Neil and Gouillart, Emmanuelle and Yu, Tony and scikit-image {contributors}, the},
  year = {2014},
  month = jun,
  volume = {2},
  pages = {e453},
  issn = {2167-8359},
  doi = {10.7717/peerj.453},
  journal = {PeerJ Inc.},
  keywords = {Education,Image processing,Open source,Python,Reproducible research,Scientific programming,Visualization}
}

@inproceedings{van_sandt_efficiently_2019,
  title = {Efficiently {{Searching In}}-{{Memory Sorted Arrays}}: {{Revenge}} of the {{Interpolation Search}}?},
  shorttitle = {Efficiently {{Searching In}}-{{Memory Sorted Arrays}}},
  booktitle = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Van Sandt, Peter and Chronis, Yannis and Patel, Jignesh M.},
  year = {2019},
  month = jun,
  pages = {36--53},
  publisher = {{Association for Computing Machinery}},
  address = {{Amsterdam, Netherlands}},
  doi = {10.1145/3299869.3300075},
  abstract = {In this paper, we focus on the problem of searching sorted, in-memory datasets. This is a key data operation, and Binary Search is the de facto algorithm that is used in practice. We consider an alternative, namely Interpolation Search, which can take advantage of hardware trends by using complex calculations to save memory accesses. Historically, Interpolation Search was found to underperform compared to other search algorithms in this setting, despite its superior asymptotic complexity. Also, Interpolation Search is known to perform poorly on non-uniform data. To address these issues, we introduce SIP (Slope reuse Interpolation), an optimized implementation of Interpolation Search, and TIP (Three point Interpolation), a new search algorithm that uses linear fractions to interpolate on non-uniform distributions. We evaluate these two algorithms against a similarly optimized Binary Search method using a variety of real and synthetic datasets. We show that SIP is up to 4 times faster on uniformly distributed data and TIP is 2-3 times faster on non-uniformly distributed data in some cases. We also design a meta-algorithm to switch between these different methods to automate picking the higher performing search algorithm, which depends on factors like data distribution.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\8VR4QHDZ\\Van Sandt et al. - 2019 - Efficiently Searching In-Memory Sorted Arrays Rev.pdf},
  isbn = {978-1-4503-5643-5},
  keywords = {binary search,in-memory search,interpolation,interpolation search,search},
  series = {{{SIGMOD}} '19}
}

@article{virtanen_scipy_2020,
  title = {{{SciPy}} 1.0: {{Fundamental Algorithms}} for {{Scientific Computing}} in {{Python}}},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, Ilhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  year = {2020},
  volume = {17},
  pages = {261--272},
  doi = {10.1038/s41592-019-0686-2},
  journal = {Nature Methods}
}

@incollection{warren_enabling_2015,
  title = {Enabling {{Aircraft Emergency Landings Using Active Visual Site Detection}}},
  booktitle = {Field and {{Service Robotics}}: {{Results}} of the 9th {{International Conference}}},
  author = {Warren, Michael and Mejias, Luis and Yang, Xilin and Arain, Bilal and Gonzalez, Felipe and Upcroft, Ben},
  editor = {Mejias, Luis and Corke, Peter and Roberts, Jonathan},
  year = {2015},
  pages = {167--181},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-07488-7_12},
  abstract = {The ability to automate forced landings in an emergency such as engine failure is an essential ability to improve the safety of Unmanned Aerial Vehicles operating in General Aviation airspace. By using active vision to detect safe landing zones below the aircraft, the reliability and safety of such systems is vastly improved by gathering up-to-the-minute information about the ground environment. This paper presents the Site Detection System, a methodology utilising a downward facing camera to analyse the ground environment in both 2D and 3D, detect safe landing sites and characterise them according to size, shape, slope and nearby obstacles. A methodology is presented showing the fusion of landing site detection from 2D imagery with a coarse Digital Elevation Map and dense 3D reconstructions using INS-aided Structure-from-Motion to improve accuracy. Results are presented from an experimental flight showing the precision/recall of landing sites in comparison to a hand-classified ground truth, and improved performance with the integration of 3D analysis from visual Structure-from-Motion.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\HBG9S8VV\\Warren et al. - 2015 - Enabling Aircraft Emergency Landings Using Active .pdf},
  isbn = {978-3-319-07488-7},
  keywords = {Autonomous Helicopter,Landing Site,Site Detection,Unmanned Aerial Vehicle,Visual Odometry},
  language = {en},
  series = {Springer {{Tracts}} in {{Advanced Robotics}}}
}

@article{weiss_survey_2016,
  title = {A Survey of Transfer Learning},
  author = {Weiss, Karl and Khoshgoftaar, Taghi M. and Wang, DingDing},
  year = {2016},
  month = may,
  volume = {3},
  pages = {9},
  issn = {2196-1115},
  doi = {10.1186/s40537-016-0043-6},
  abstract = {Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper formally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\4V3X3CMC\\Weiss et al. - 2016 - A survey of transfer learning.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\42842R2H\\s40537-016-0043-6.html},
  journal = {Journal of Big Data},
  number = {1}
}

@book{wenninger_spherical_1999,
  title = {Spherical Models},
  author = {Wenninger, Magnus J},
  year = {1999},
  volume = {3},
  publisher = {{Courier Corporation}}
}

@article{winnefeld_unmanned_2011,
  title = {Unmanned {{Systems Integrated Roadmap FY}} 2011-2036},
  author = {Winnefeld, J. A. and Kendall, F.},
  year = {2011},
  journal = {Office of the Secretary of Defense. US}
}

@inproceedings{xu_spidercnn_2018,
  title = {{{SpiderCNN}}: {{Deep Learning}} on {{Point Sets}} with {{Parameterized Convolutional Filters}}},
  shorttitle = {{{SpiderCNN}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2018},
  author = {Xu, Yifan and Fan, Tianqi and Xu, Mingye and Zeng, Long and Qiao, Yu},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  pages = {90--105},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-01237-3_6},
  abstract = {Deep neural networks have enjoyed remarkable success for various vision tasks, however it remains challenging to apply CNNs to domains lacking a regular underlying structures such as 3D point clouds. Towards this we propose a novel convolutional architecture, termed SpiderCNN, to efficiently extract geometric features from point clouds. SpiderCNN is comprised of units called SpiderConv, which extend convolutional operations from regular grids to irregular point sets that can be embedded in RnRn\textbackslash mathbb \{R\}\^n, by parametrizing a family of convolutional filters. We design the filter as a product of a simple step function that captures local geodesic information and a Taylor polynomial that ensures the expressiveness. SpiderCNN inherits the multi-scale hierarchical architecture from classical CNNs, which allows it to extract semantic deep features. Experiments on ModelNet40 demonstrate that SpiderCNN achieves state-of-the-art accuracy 92.4\%92.4\%92.4 \textbackslash\% on standard benchmarks, and shows competitive performance on segmentation task.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\Y8WDATMZ\\Xu et al. - 2018 - SpiderCNN Deep Learning on Point Sets with Parame.pdf},
  isbn = {978-3-030-01237-3},
  keywords = {Convolutional neural network,Parametrized convolutional filters,Point clouds},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{yan_dynamic_2017,
  title = {A {{Dynamic Multi}}-{{Projection}}-{{Contour Approximating Framework}} for the {{3D Reconstruction}} of {{Buildings}} by {{Super}}-{{Generalized Optical Stereo}}-{{Pairs}}},
  author = {Yan, Yiming and Su, Nan and Zhao, Chunhui and Wang, Liguo},
  year = {2017},
  month = sep,
  volume = {17},
  pages = {2153},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/s17092153},
  abstract = {In this paper, a novel framework of the 3D reconstruction of buildings is proposed, focusing on remote sensing super-generalized stereo-pairs (SGSPs). As we all know, 3D reconstruction cannot be well performed using nonstandard stereo pairs, since reliable stereo matching could not be achieved when the image-pairs are collected at a great difference of views, and we always failed to obtain dense 3D points for regions of buildings, and cannot do further 3D shape reconstruction. We defined SGSPs as two or more optical images collected in less constrained views but covering the same buildings. It is even more difficult to reconstruct the 3D shape of a building by SGSPs using traditional frameworks. As a result, a dynamic multi-projection-contour approximating (DMPCA) framework was introduced for SGSP-based 3D reconstruction. The key idea is that we do an optimization to find a group of parameters of a simulated 3D model and use a binary feature-image that minimizes the total differences between projection-contours of the building in the SGSPs and that in the simulated 3D model. Then, the simulated 3D model, defined by the group of parameters, could approximate the actual 3D shape of the building. Certain parameterized 3D basic-unit-models of typical buildings were designed, and a simulated projection system was established to obtain a simulated projection-contour in different views. Moreover, the artificial bee colony algorithm was employed to solve the optimization. With SGSPs collected by the satellite and our unmanned aerial vehicle, the DMPCA framework was verified by a group of experiments, which demonstrated the reliability and advantages of this work.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\2Z7P3RVG\\Yan et al. - 2017 - A Dynamic Multi-Projection-Contour Approximating F.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\HXRWEQL5\\2153.html},
  journal = {Sensors},
  keywords = {3D reconstruction,artificial bee colony algorithm,remote sensing,super-generalized stereo-pairs},
  language = {en},
  number = {9}
}

@article{yan_hierarchical_2017,
  title = {A {{Hierarchical Building Segmentation}} in {{Digital Surface Models}} for {{3D Reconstruction}}},
  author = {Yan, Yiming and Gao, Fengjiao and Deng, Shupei and Su, Nan},
  year = {2017},
  month = feb,
  volume = {17},
  pages = {222},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/s17020222},
  abstract = {In this study, a hierarchical method for segmenting buildings in a digital surface model (DSM), which is used in a novel framework for 3D reconstruction, is proposed. Most 3D reconstructions of buildings are model-based. However, the limitations of these methods are overreliance on completeness of the offline-constructed models of buildings, and the completeness is not easily guaranteed since in modern cities buildings can be of a variety of types. Therefore, a model-free framework using high precision DSM and texture-images buildings was introduced. There are two key problems with this framework. The first one is how to accurately extract the buildings from the DSM. Most segmentation methods are limited by either the terrain factors or the difficult choice of parameter-settings. A level-set method are employed to roughly find the building regions in the DSM, and then a recently proposed `occlusions of random textures model' are used to enhance the local segmentation of the buildings. The second problem is how to generate the facades of buildings. Synergizing with the corresponding texture-images, we propose a roof-contour guided interpolation of building facades. The 3D reconstruction results achieved by airborne-like images and satellites are compared. Experiments show that the segmentation method has good performance, and 3D reconstruction is easily performed by our framework, and better visualization results can be obtained by airborne-like images, which can be further replaced by UAV images.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\JXAEMK4A\\Yan et al. - 2017 - A Hierarchical Building Segmentation in Digital Su.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\JUR5KH3T\\222.html},
  journal = {Sensors},
  keywords = {3D reconstruction,building segmentation,digital surface model,remote sensing},
  language = {en},
  number = {2}
}

@article{zheng_bilateral_2011,
  title = {Bilateral Normal Filtering for Mesh Denoising},
  author = {Zheng, Youyi and Fu, Hongbo and Au, Oscar Kin-Chung and Tai, Chiew-Lan},
  year = {2011},
  month = oct,
  volume = {17},
  pages = {1521--1530},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2010.264},
  abstract = {Decoupling local geometric features from the spatial location of a mesh is crucial for feature-preserving mesh denoising. This paper focuses on first order features, i.e., facet normals, and presents a simple yet effective anisotropic mesh denoising framework via normal field denoising. Unlike previous denoising methods based on normal filtering, which process normals defined on the Gauss sphere, our method considers normals as a surface signal defined over the original mesh. This allows the design of a novel bilateral normal filter that depends on both spatial distance and signal distance. Our bilateral filter is a more natural extension of the elegant bilateral filter for image denoising than those used in previous bilateral mesh denoising methods. Besides applying this bilateral normal filter in a local, iterative scheme, as common in most of previous works, we present for the first time a global, noniterative scheme for an isotropic denoising. We show that the former scheme is faster and more effective for denoising extremely noisy meshes while the latter scheme is more robust to irregular surface sampling. We demonstrate that both our feature-preserving schemes generally produce visually and numerically better denoising results than previous methods, especially at challenging regions with sharp features or irregular sampling.},
  journal = {IEEE transactions on visualization and computer graphics},
  language = {eng},
  number = {10},
  pmid = {21173457}
}

@article{zhou_dense_2013,
  title = {Dense Scene Reconstruction with Points of Interest},
  author = {Zhou, Qian-Yi and Koltun, Vladlen},
  year = {2013},
  month = jul,
  volume = {32},
  pages = {112:1--112:8},
  issn = {0730-0301},
  doi = {10.1145/2461912.2461919},
  abstract = {We present an approach to detailed reconstruction of complex real-world scenes with a handheld commodity range sensor. The user moves the sensor freely through the environment and images the scene. An offline registration and integration pipeline produces a detailed scene model. To deal with the complex sensor trajectories required to produce detailed reconstructions with a consumer-grade sensor, our pipeline detects points of interest in the scene and preserves detailed geometry around them while a global optimization distributes residual registration errors through the environment. Our results demonstrate that detailed reconstructions of complex scenes can be obtained with a consumer-grade camera.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\PGGD8ZLQ\\Zhou and Koltun - 2013 - Dense scene reconstruction with points of interest.pdf},
  journal = {ACM Transactions on Graphics},
  keywords = {range imaging,scene reconstruction},
  number = {4}
}

@article{zhou_open3d_2018,
  title = {{{Open3D}}: {{A}} Modern Library for {{3D}} Data Processing},
  author = {Zhou, Qian-Yi and Park, Jaesik and Koltun, Vladlen},
  year = {2018},
  archivePrefix = {arXiv},
  eprint = {1801.09847},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1801.09847}
}

@article{zhou_seamless_2014,
  title = {Seamless {{Fusion}} of {{LiDAR}} and {{Aerial Imagery}} for {{Building Extraction}}},
  author = {Zhou, G. and Zhou, X.},
  year = {2014},
  month = nov,
  volume = {52},
  pages = {7393--7407},
  issn = {1558-0644},
  doi = {10.1109/TGRS.2014.2311991},
  abstract = {Although many efforts have been made on the fusion of Light Detection and Ranging (LiDAR) and aerial imagery for the extraction of houses, little research on taking advantage of a building's geometric features, properties, and structures for assisting the further fusion of the two types of data has been made. For this reason, this paper develops a seamless fusion between LiDAR and aerial imagery on the basis of aspect graphs, which utilize the features of houses, such as geometry, structures, and shapes. First, 3-D primitives, standing for houses, are chosen, and their projections are represented by the aspects. A hierarchical aspect graph is then constructed using aerial image processing in combination with the results of LiDAR data processing. In the aspect graph, the note represents the face aspect and the arc is described by attributes obtained by the formulated coding regulations, and the coregistration between the aspect and LiDAR data is implemented. As a consequence, the aspects and/or the aspect graph are interpreted for the extraction of houses, and then the houses are fitted using a planar equation for creating a digital building model (DBM). The experimental field, which is located in Wytheville, VA, is used to evaluate the proposed method. The experimental results demonstrated that the proposed method is capable of effectively extracting houses at a successful rate of 93\%, as compared with another method, which is 82\% effective when LiDAR spacing is approximately 7.3 by 7.3 ft2. The accuracy of 3-D DBM is higher than the method using only single LiDAR data.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\7HNQ4L7E\\Zhou and Zhou - 2014 - Seamless Fusion of LiDAR and Aerial Imagery for Bu.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\C97YPDLL\\6804760.html},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  keywords = {3D primitive,Aerial image,aerial image processing,building extraction,building geometric feature,Buildings,DBM,digital building model,Encoding,extraction,Face,Feature extraction,formulated coding regulation,geophysical image processing,graph theory,hierarchical aspect graph,house,house extraction,image coding,image coregistration,image fusion,image processing,image registration,image representation,Laser radar,LiDAR data processing,light detection and ranging,light detection and ranging (LiDAR),Merging,Optical imaging,optical radar,radar imaging,seamless fusion,urban},
  number = {11}
}


@misc{satellite_germany,
  author= {{Land NRW}},
  title = {{Witten County Portal}},
  howpublished = {[Online] Available: \url{https://www.land.nrw/de/tags/open-data}},
  note    = {Data was retrieved through ArcGIS World Imagery}, 
  year={2018}
}


@misc{satellite_newyork,
  author = {{New York State}},
  title = {{2016 Annual Lot New York County}},
  howpublished = {[Online] Available: \url{http://gis.ny.gov/gateway/orthoprogram/lot16/new-york.htm}},
  note    = {Data was retrieved through ArcGIS World Imagery}, 
  year={2018}
}

@misc{satellite_annarbor,
  author = {{Microsoft}},
  title = {{Bing Maps}},
  howpublished = {[Online] Available: \url{https://www.bing.com/maps}},
  note = {Accessed: 2018-09-05},
  year={2018}
}


@misc{polylidar_benchmark_concave,
  author={Jeremy Castagno},
  title = {{Github - Benchmark Concave Hull}},
  howpublished  = {[Online] Available: \url{https://github.com/JeremyBYU/concavehull-evaluation}},
  note = {Accessed: 2020-01-05},
  year={2020}
}

@misc{marl,
  author={Google},
  title = {{Github - A Hybrid Thread/Fiber Task Scheduler}},
  howpublished  = {[Online] Available: \url{https://github.com/google/marl}},
  note = {Accessed: 2020-06-05},
  year={2020}
}


@article{scherer_autonomous_2012,
	title = {Autonomous landing at unprepared sites by a full-scale helicopter},
	volume = {60},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S0921889012001509},
	doi = {10.1016/j.robot.2012.09.004},
	abstract = {Helicopters are valuable since they can land at unprepared sites; however, current unmanned helicopters are unable to select or validate landing zones (LZs) and approach paths. For operation in unknown terrain it is necessary to assess the safety of a LZ. In this paper, we describe a lidar-based perception system that enables a full-scale autonomous helicopter to identify and land in previously unmapped terrain with no human input. We describe the problem, real-time algorithms, perception hardware, and results. Our approach has extended the state of the art in terrain assessment by incorporating not only plane fitting, but by also considering factors such as terrain/skid interaction, rotor and tail clearance, wind direction, clear approach/abort paths, and ground paths. In results from urban and natural environments we were able to successfully classify LZs from point cloud maps. We also present results from 8 successful landing experiments with varying ground clutter and approach directions. The helicopter selected its own landing site, approaches, and then proceeds to land. To our knowledge, these experiments were the first demonstration of a full-scale autonomous helicopter that selected its own landing zones and landed.},
	language = {en},
	number = {12},
	urldate = {2021-05-11},
	journal = {Robotics and Autonomous Systems},
	author = {Scherer, Sebastian and Chamberlain, Lyle and Singh, Sanjiv},
	month = dec,
	year = {2012},
	keywords = {3D perception, Landing zone selection, Lidar, Rotorcraft, UAV},
	pages = {1545--1562},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\XT79I4W7\\Scherer et al. - 2012 - Autonomous landing at unprepared sites by a full-s.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Jerem\\Zotero\\storage\\BQ6CZZ5Y\\S0921889012001509.html:text/html},
}


@incollection{wickens_5_1988,
	address = {San Diego},
	series = {Cognition and {Perception}},
	title = {5 - {Information} {Processing}},
	isbn = {978-0-08-057090-7},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080570907500114},
	abstract = {This chapter reviews a representative research generated by the information processing approach to human cognition and presents some implications of this research for the design of safe, comfortable, and efficient aviation systems. The information processing approach has not been without its critics. It has been accused of playing 20 questions with nature and losing. The essence of this criticism is that the science has become too segmented along the line of specific experimental tasks, that is, one group studies the infinite permutations on choice reaction time tasks, another group studies memory search tasks, and another focuses only on tracking tasks. Thus, the science has produced an enormous catalog of information about a few rather esoteric laboratory tasks; yet it has contributed very little to the understanding of how humans function outside the laboratory in real-world settings. It is likely that the future success of the information processing approach will rest on its ability to deal with this criticism. There is a need for human factors specialists to widen their perspective beyond the relatively simple tasks that currently dominate research to increase the complexity of experimental tasks and to incorporate more ecologically valid sources of information. The information processing paradigm has contributed both knowledge and tools relevant for understanding human performance in aviation systems. The study of human performance in aviation systems provides an excellent opportunity to better understand general issues related to human cognition in complex environments.},
	language = {en},
	urldate = {2021-05-12},
	booktitle = {Human {Factors} in {Aviation}},
	publisher = {Academic Press},
	author = {Wickens, Christopher D. and Flach, John M.},
	editor = {Wiener, Earl L. and Nagel, David C.},
	month = jan,
	year = {1988},
	doi = {10.1016/B978-0-08-057090-7.50011-4},
	pages = {111--155},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\3G38XEV4\\Wickens and Flach - 1988 - 5 - Information Processing.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Jerem\\Zotero\\storage\\L6GXICAR\\B9780080570907500114.html:text/html},
}



@book{2003_technical_review_human_error,
publisher = {{European Organisation for the Safety of Air Navigation}},
title = "Technical review of human performance models and taxonomies of human error in ATM (HERA)",
keywords = "2-E tekno",
author = "A. Isaac and S.T. Shorrock and R. Kennedy and B. Kirwan and Andersen, {Henning Boje} and Thomas Bove",
year = "2003",
language = "English",
series = "HRS/HSP-002-REP-01",
}

@book{Wickens2015,
  doi = {10.4324/9781315665177},
  url = {https://doi.org/10.4324/9781315665177},
  year = {2015},
  month = aug,
  publisher = {Psychology Press},
  author = {Christopher D. Wickens and Justin G. Hollands and Simon Banbury and Raja Parasuraman},
  title = {Engineering Psychology and Human Performance}
}


@inproceedings{liu_dynamic_2019,
	title = {Dynamic {Points} {Agglomeration} for {Hierarchical} {Point} {Sets} {Learning}},
	doi = {10.1109/ICCV.2019.00764},
	abstract = {Many previous works on point sets learning achieve excellent performance with hierarchical architecture. Their strategies towards points agglomeration, however, only perform points sampling and grouping in original Euclidean space in a fixed way. These heuristic and task-irrelevant strategies severely limit their ability to adapt to more varied scenarios. To this end, we develop a novel hierarchical point sets learning architecture, with dynamic points agglomeration. By exploiting the relation of points in semantic space, a module based on graph convolution network is designed to learn a soft points cluster agglomeration. We construct a hierarchical architecture that gradually agglomerates points by stacking this learnable and lightweight module. In contrast to fixed points agglomeration strategy, our method can handle more diverse situations robustly and efficiently. Moreover, we propose a parameter sharing scheme for reducing memory usage and computational burden induced by the agglomeration module. Extensive experimental results on several point cloud analytic tasks, including classification and segmentation, well demonstrate the superior performance of our dynamic hierarchical learning framework over current state-of-the-art methods.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Liu, Jinxian and Ni, Bingbing and Li, Caiyuan and Yang, Jiancheng and Tian, Qi},
	month = oct,
	year = {2019},
	note = {ISSN: 2380-7504},
	keywords = {Computer architecture, Convolution, Machine learning, Semantics, Task analysis, Three-dimensional displays, Two dimensional displays},
	pages = {7545--7554},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\959LUFP7\\Liu et al. - 2019 - Dynamic Points Agglomeration for Hierarchical Poin.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Jerem\\Zotero\\storage\\KPXJSYDA\\9010020.html:text/html},
}


@article{guo_deep_2020,
	title = {Deep {Learning} for {3D} {Point} {Clouds}: {A} {Survey}},
	issn = {1939-3539},
	shorttitle = {Deep {Learning} for {3D} {Point} {Clouds}},
	doi = {10.1109/TPAMI.2020.3005434},
	abstract = {Point cloud learning has lately attracted increasing attention due to its wide applications in many areas, such as computer vision, autonomous driving, and robotics. As a dominating technique in AI, deep learning has been successfully used to solve various 2D vision problems. However, deep learning on point clouds is still in its infancy due to the unique challenges faced by the processing of point clouds with deep neural networks. Recently, deep learning on point clouds has become even thriving, with numerous methods being proposed to address different problems in this area. To stimulate future research, this paper presents a comprehensive review of recent progress in deep learning methods for point clouds. It covers three major tasks, including 3D shape classification, 3D object detection and tracking, and 3D point cloud segmentation. It also presents comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Guo, Yulan and Wang, Hanyun and Hu, Qingyong and Liu, Hao and Liu, Li and Bennamoun, Mohammed},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {3D data, deep learning, Deep learning, instance segmentation, Laser radar, object detection, Object detection, object tracking, part segmentation, point clouds, scene flow, semantic segmentation, Sensors, Shape, shape classification, shape retrieval, Task analysis, Three-dimensional displays},
	pages = {1--1},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\CHGB3YD3\\Guo et al. - 2020 - Deep Learning for 3D Point Clouds A Survey.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Jerem\\Zotero\\storage\\A5LPDB25\\9127813.html:text/html},
}


@inproceedings{mols_highly_2020,
	title = {Highly {Parallelizable} {Plane} {Extraction} for {Organized} {Point} {Clouds} {Using} {Spherical} {Convex} {Hulls}},
	doi = {10.1109/ICRA40945.2020.9197139},
	abstract = {We present a novel region growing algorithm for plane extraction of organized point clouds using the spherical convex hull. Instead of explicit plane parameterization, our approach interprets potential underlying planes as a series of geometric constraints on the sphere that are refined during region growing. Unlike existing schemes relying on downsampling for sequential execution in real time, our approach enables pixelwise plane extraction that is highly parallelizable. We further test the proposed approach with a fully parallel implementation on a GPU. Evaluation based on public data sets has shown state-of-the-art extraction accuracy and superior speed compared to existing approaches, while guaranteeing real-time processing at full input resolution of a typical RGB-D camera.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {MÃ¶ls, Hannes and Li, Kailai and Hanebeck, Uwe D.},
	month = may,
	year = {2020},
	note = {ISSN: 2577-087X},
	keywords = {Clustering algorithms, Data mining, Feature extraction, Real-time systems, Robot sensing systems, Three-dimensional displays},
	pages = {7920--7926},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\ZKQTKJ7V\\MÃ¶ls et al. - 2020 - Highly Parallelizable Plane Extraction for Organiz.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Jerem\\Zotero\\storage\\HY7FBTZ5\\citations.html:text/html},
}



@article{cohen_digital_2020,
	title = {Digital {Smartphone} {Tracking} for {COVID}-19: {Public} {Health} and {Civil} {Liberties} in {Tension}},
	volume = {323},
	issn = {0098-7484},
	shorttitle = {Digital {Smartphone} {Tracking} for {COVID}-19},
	url = {https://doi.org/10.1001/jama.2020.8570},
	doi = {10.1001/jama.2020.8570},
	abstract = {Contact investigations have been a vital public health strategy, most recently in controlling tuberculosis and sexually transmitted infections including HIV. Yet, the sheer scale of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infections poses major challenges to contact investigations. Strategies in China, Singapore, South Korea, and Taiwan have supplemented traditional manual approaches with digital surveillance through smartphone applications.The US has not used digital surveillance as a tool, but Google, Apple, the Massachusetts Institute of Technology (MIT), as well as 2 pan-European consortia and a variety of independent efforts are developing Bluetooth smartphone technology to enable rapid notification of users that they have had a close exposure to individuals diagnosed with medically verified coronavirus disease 2019 (COVID-19). How does digital tracking differ from manual tracing? Although digital surveillance has the distinct advantages of scale and speed, does it confer sufficient public health benefit to justify adoption given privacy concerns? How do the design choices of digital contact tracing systems affect public health and privacy?},
	number = {23},
	urldate = {2021-05-14},
	journal = {JAMA},
	author = {Cohen, I. Glenn and Gostin, Lawrence O. and Weitzner, Daniel J.},
	month = jun,
	year = {2020},
	pages = {2371--2372},
	file = {Full Text:C\:\\Users\\Jerem\\Zotero\\storage\\TLVG8NFS\\Cohen et al. - 2020 - Digital Smartphone Tracking for COVID-19 Public H.pdf:application/pdf;Snapshot:C\:\\Users\\Jerem\\Zotero\\storage\\YHPRLA7H\\2766675.html:text/html},
}


@article{lee_benefits_2021,
	title = {Benefits of {Mobile} {Contact} {Tracing} on {COVID}-19: {Tracing} {Capacity} {Perspectives}},
	volume = {9},
	issn = {2296-2565},
	shorttitle = {Benefits of {Mobile} {Contact} {Tracing} on {COVID}-19},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8013704/},
	doi = {10.3389/fpubh.2021.586615},
	urldate = {2021-05-14},
	journal = {Frontiers in Public Health},
	author = {Lee, Uichin and Kim, Auk},
	month = mar,
	year = {2021},
	pmid = {33816413},
	pmcid = {PMC8013704},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\CKC5XSVG\\Lee and Kim - 2021 - Benefits of Mobile Contact Tracing on COVID-19 Tr.pdf:application/pdf},
}


%% New Chapter 6 Unreal


@article{lee_uav_2019,
	title = {{UAV} {Flight} and {Landing} {Guidance} {System} for {Emergency} {Situations} â€ },
	volume = {19},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/19/20/4468},
	doi = {10.3390/s19204468},
	abstract = {Unmanned aerial vehicles (UAVs) with high mobility can perform various roles such as delivering goods, collecting information, recording videos and more. However, there are many elements in the city that disturb the flight of the UAVs, such as various obstacles and urban canyons which can cause a multi-path effect of GPS signals, which degrades the accuracy of GPS-based localization. In order to empower the safety of the UAVs flying in urban areas, UAVs should be guided to a safe area even in a GPS-denied or network-disconnected environment. Also, UAVs must be able to avoid obstacles while landing in an urban area. For this purpose, we present the UAV detour system for operating UAV in an urban area. The UAV detour system includes a highly reliable laser guidance system to guide the UAVs to a point where they can land, and optical flow magnitude map to avoid obstacles for a safe landing.},
	language = {en},
	number = {20},
	urldate = {2021-03-18},
	journal = {Sensors},
	author = {Lee, Joon Yeop and Chung, Albert Y. and Shim, Hooyeop and Joe, Changhwan and Park, Seongjoon and Kim, Hwangnam},
	month = jan,
	year = {2019},
	note = {Number: 20
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {emergency landing, laser guidance, optical flow, particle filter, UAV},
	pages = {4468},
	file = {Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\828M3D9K\\Lee et al. - 2019 - UAV Flight and Landing Guidance System for Emergen.pdf:application/pdf;Snapshot:C\:\\Users\\Jerem\\Zotero\\storage\\9XKKZEPX\\4468.html:text/html},
}

@article{wei_enhancing_2021,
	title = {Enhancing {Solid} {State} {LiDAR} {Mapping} with a {2D} {Spinning} {LiDAR} in {Urban} {Scenario} {SLAM} on {Ground} {Vehicles}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/21/5/1773},
	doi = {10.3390/s21051773},
	abstract = {Solid-State LiDAR (SSL) takes an increasing share of the LiDAR market. Compared with traditional spinning LiDAR, SSLs are more compact, energy-efficient and cost-effective. Generally, the current study of SSL mapping is limited to adapting existing SLAM algorithms to an SSL sensor. However, compared with spinning LiDARs, SSLs are different in terms of their irregular scan patterns and limited FOV. Directly applying existing SLAM approaches on them often increase the instability of a mapping process. This study proposes a systematic design, which consists of a dual-LiDAR mapping system and a three DOF interpolated six DOF odometry. For dual-LiDAR mapping, this work uses a 2D LiDAR to enhance a 3D SSL performance on a ground vehicle platform. The proposed system takes a 2D LiDAR to preprocess the scanning field into a number of feature sections according to the curvatures on the 2D fraction. Subsequently, this section information is passed to 3D SSL for direction feature selection. Additionally, this work proposes an odometry interpolation method which uses both LiDARs to generate two separated odometries. The proposed odometry interpolation method selectively determines the appropriate odometry information to update the system state under challenging conditions. Experiments are conducted in different scenarios. The results proves that the proposed approach is able to utilise 12 times more corner features from the environment than the comparied method, thus results in a demonstrable improvement in its absolute position error.},
	language = {en},
	number = {5},
	urldate = {2021-03-16},
	journal = {Sensors},
	author = {Wei, Weichen and Shirinzadeh, Bijan and Nowell, Rohan and Ghafarian, Mohammadali and Ammar, Mohamed M. A. and Shen, Tianyao},
	month = jan,
	year = {2021},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {LiDAR, localisation, SLAM, UGV},
	pages = {1773},
	file = {Snapshot:C\:\\Users\\Jerem\\Zotero\\storage\\NJC6FHPX\\1773.html:text/html},
}


@inproceedings{nam_solid-state_2021,
	title = {Solid-{State} {LiDAR} based-{SLAM}: {A} {Concise} {Review} and {Application}},
	shorttitle = {Solid-{State} {LiDAR} based-{SLAM}},
	doi = {10.1109/BigComp51126.2021.00064},
	abstract = {An efficient navigation system for the autonomous mobile robot demands the integration of the simultaneous localization and mapping system (SLAM). In general, the SLAM framework employs the camera, IMU, wheel encoder and can not lack the combination of the Light Detection and Ranging (LiDAR) sensors for superior accuracy for the navigation system. Nevertheless, the price of mechanical LiDAR is exceptionally high compared with other sensors, which is the primary reason to introduce a new generation of laser called Solid-State LiDAR (SSL). In this paper, we introduce a back-ground of SSL and several types of commercial SSL and also their characteristic. For developing SLAM framework, we discuss several successful SLAM architectures, which can be capable of integrating into a navigation system for the mobile robot. Last, we propose a low-cost SLAM framework using the visual-inertial-SSL, that can not only reduce the price of a mobile industrial robot but also provide excellent performance.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Big} {Data} and {Smart} {Computing} ({BigComp})},
	author = {Nam, D. V. and Gon-Woo, K.},
	month = jan,
	year = {2021},
	note = {ISSN: 2375-9356},
	keywords = {Computer architecture, laser localization, Laser radar, Mechanical sensors, Mobile robots, Navigation, navigation system, Sensor phenomena and characterization, Simultaneous localization and mapping, SLAM, solid-ltate LiDAR},
	pages = {302--305},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\5CIBKCD3\\Nam and Gon-Woo - 2021 - Solid-State LiDAR based-SLAM A Concise Review and.pdf:application/pdf},
}

@misc{Castagno_Github_UnrealLanding,
	author = {Jeremy Castagno},
	title = {Github - {Unreal} {Landing}},
	year = {2021},
	howpublished = {[Online] Available: \url{https://github.com/JeremyBYU/UnrealRooftopLanding/}},
	note = {Accessed: 2021-06-01}
}

@techreport{hoh_decision-height_1991,
	title = {Decision-{Height} {Windows} for {Decelerating} {Approaches} in {Helicopters} - {Pilot}/{Vehicle} {Factors} and {Limitations}},
	url = {https://apps.dtic.mil/sti/citations/ADA239610},
	abstract = {A combined analysis and flight test program was conducted to investigate the characteristics of the decision-height DH window for helicopter decelerating instrument approaches. The concept of an effective flight path angle has been employed to define the DH window in terms of basic rotorcraft performance data. Exploratory flight tests were conducted to validate this approach and to define the approximate dimensions of the DH window 50 feet above ground level. The flight test experiment included an instrument meteorological conditions IMC decelerating instrument approach with errors built into the flight director to cause the helicopter to arrive at the decision-height with some glideslope and groundspeed errors. The pilots were required to visually maneuver the rotorcraft from decision-height to a steady hover over the helipad. The decision-height window was formulated on a grid of glideslope error versus the groundspeed at decision-height. The results indicate that the high speed boundary of the DH window is a function of the minimum usable torque, and related to maximum acceptable pitch attitude during deceleration. Some margin is required to account for pilot delay or control misapplication after breakout. The upper glideslope error boundary is based on the maximum negative aerodynamic flight path angle that can be flown at low airspeeds. Poor visual cuing after breakout tends to emphasize the need for margins from the helicopter performance. The low speed boundary of the DH window is based on rotorcraft handling qualities at very low airspeeds. The low glideslope boundary is dependent on obstruction avoidance and ability to see the heliport environment upon breakout at decision-height.},
	language = {en},
	number = {ADA239610},
	urldate = {2021-03-01},
	institution = {SYSTEMS CONTROL TECHNOLOGY INC ARLINGTON VA},
	author = {Hoh, R. H. and Baillie, S. and Kereliuk, S. and Traybar, J. J.},
	month = apr,
	year = {1991},
	note = {Section: Technical Reports},
	file = {Snapshot:C\:\\Users\\Jerem\\Zotero\\storage\\X6P29JJW\\ADA239610.html:text/html;Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\9QB8QNZX\\Hoh et al. - 1991 - Decision-Height Windows for Decelerating Approache.pdf:application/pdf},
}

@article{chua_pilot_2013,
	title = {Pilot decision making during landing point designation},
	volume = {15},
	issn = {1435-5566},
	url = {https://doi.org/10.1007/s10111-012-0233-8},
	doi = {10.1007/s10111-012-0233-8},
	abstract = {Predicting astronaut performance when choosing where to land on the Moon, or the landing point designation task, is difficult. Human judgment and decision making for this application is not well-characterized. To address this knowledge deficiency, an experiment was designed to replicate the landing point designation task in both ideal and poor landing conditions. Fifteen helicopter pilots were observed to use one of two strategy types, with both strategies resulting in equatable performance. Regardless of strategy use, terrain contrast conditions were observed to have a significant impact on the selection of safe landing sites. Other aspects of performance, such as task completion time, were unaffected by terrain contrast. Pilots were also observed to be influenced by multiple terrain factors. Researchers noted twenty different types of actions stemming from nine categories of scenario cues. When these cueâ€“action relationships are used in conjunction with pilotsâ€™ evaluation criteria and a known strategy, models can be developed to predict decision making.},
	language = {en},
	number = {3},
	urldate = {2021-03-01},
	journal = {Cognition, Technology \& Work},
	author = {Chua, Zarrin K. and Feigh, Karen M.},
	month = aug,
	year = {2013},
	pages = {297--311},
	file = {Springer Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\8U8HMUKI\\Chua and Feigh - 2013 - Pilot decision making during landing point designa.pdf:application/pdf},
}

@article{garcia-pardo_towards_2002,
	title = {Towards vision-based safe landing for an autonomous helicopter},
	volume = {38},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S092188900100166X},
	doi = {10.1016/S0921-8890(01)00166-X},
	abstract = {Autonomous landing is a challenging problem for aerial robots. An autonomous landing manoeuver depends largely on two capabilities: the decision of where to land and the generation of control signals to guide the vehicle to a safe landing. We focus on the first capability here by presenting a strategy and an underlying fast algorithm as the computer vision basis to make a safe landing decision. The experimental results obtained from real test flights on a helicopter testbed demonstrate the robustness of the approach under widely different light, altitude and background texture conditions, as well as its feasibility for limited-performance embedded computers.},
	language = {en},
	number = {1},
	urldate = {2021-03-01},
	journal = {Robotics and Autonomous Systems},
	author = {Garcia-Pardo, Pedro J. and Sukhatme, Gaurav S. and Montgomery, James F.},
	month = jan,
	year = {2002},
	keywords = {Autonomous helicopter, Contrast measures, Safe landing, Texture scale, Vision-based landing},
	pages = {19--29},
	file = {ScienceDirect Snapshot:C\:\\Users\\Jerem\\Zotero\\storage\\B4GGXB63\\S092188900100166X.html:text/html;ScienceDirect Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\5C5Q7SPS\\Garcia-Pardo et al. - 2002 - Towards vision-based safe landing for an autonomou.pdf:application/pdf},
}

@inproceedings{pluckter_precision_2020,
	address = {Cham},
	series = {Springer {Proceedings} in {Advanced} {Robotics}},
	title = {Precision {UAV} {Landing} in {Unstructured} {Environments}},
	isbn = {978-3-030-33950-0},
	doi = {10.1007/978-3-030-33950-0_16},
	abstract = {Autonomous landing of a drone is a necessary part of autonomous flight. One way to have high certainty of safety in landing is to return to the same location the drone took-off from. Implementations of return-to-home functionality fall short when relying solely on GPS or odometry as inaccuracies in the measurements and drift in the state estimate guides the drone to a position with a large offset from the initial position. This can be particularly dangerous if the drone took-off next to something like a body of water. Current work on precision landing relies on localizing to a known landing pattern, which requires the pilot to carry a landing pattern with them. We propose a method using a downward facing fisheye lens camera to accurately land a UAV from where it took off on an unstructured surface, without a landing pattern. Specifically, this approach uses a position estimate relative to the take-off path of the drone to guide the drone back. With the large Field-of-View provided by the fisheye lens, our algorithm can provide visual feedback starting with a large position error at the beginning of the landing, until 25 cm above the ground at the end of the landing. This algorithm empirically shows it can correct the drift error in the state estimation and land with an accuracy of 40 cm.},
	language = {en},
	booktitle = {Proceedings of the 2018 {International} {Symposium} on {Experimental} {Robotics}},
	publisher = {Springer International Publishing},
	author = {Pluckter, Kevin and Scherer, Sebastian},
	editor = {Xiao, Jing and KrÃ¶ger, Torsten and Khatib, Oussama},
	year = {2020},
	keywords = {Fisheye lens, Precision landing, UAV},
	pages = {177--187},
	file = {Springer Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\LBBW32VH\\Pluckter and Scherer - 2020 - Precision UAV Landing in Unstructured Environments.pdf:application/pdf},
}

@inproceedings{kalinov_high-precision_2019,
	title = {High-{Precision} {UAV} {Localization} {System} for {Landing} on a {Mobile} {Collaborative} {Robot} {Based} on an {IR} {Marker} {Pattern} {Recognition}},
	doi = {10.1109/VTCSpring.2019.8746668},
	abstract = {We present a novel high-precision UAV localization system for interconnection between two collaborative robots, i.e., unmanned ground robot (UGR) and unmanned aerial vehicle (UAV) capable of autonomous navigation and precise localization in an indoor environment. Based on our localization system we have achieved robust UAV landing on the moving robot using a fusion of 2D LIDAR sensors, camera, and ultrasonic system for localization. In addition, UAV is capable of accurate high-altitude indoor flights (up to 15 m) relative to the ground robot. Localization of UAV is based on the developed adaptive active IR marker system to achieve reliable flight on different altitudes and light conditions. In this paper, we describe the operating principle of the system and present the results of UAV flight experiments. One of promising applications of the developed system is automated inventory management of warehouses.},
	booktitle = {2019 {IEEE} 89th {Vehicular} {Technology} {Conference} ({VTC2019}-{Spring})},
	author = {Kalinov, I. and Safronov, E. and Agishev, R. and Kurenkov, M. and Tsetserukou, D.},
	month = apr,
	year = {2019},
	note = {ISSN: 2577-2465},
	keywords = {2D LIDAR sensors, adaptive active IR marker system, aircraft landing guidance, attitude control, automated inventory management, autonomous aerial vehicles, autonomous navigation, cameras, Cameras, high-altitude indoor flights, high-precision UAV localization system, image recognition, IR marker pattern recognition, mobile collaborative robot, mobile robots, optical radar, path planning, Robot kinematics, robot vision, Robot vision systems, robust UAV landing, Task analysis, UAV flight experiments, ultrasonic system, unmanned aerial vehicle capable, Unmanned aerial vehicles, unmanned ground robot},
	pages = {1--6},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\VV9FSRSK\\Kalinov et al. - 2019 - High-Precision UAV Localization System for Landing.pdf:application/pdf},
}


@inproceedings{schonberger_structure--motion_2016,
	title = {Structure-from-{Motion} {Revisited}},
	doi = {10.1109/CVPR.2016.445},
	abstract = {Incremental Structure-from-Motion is a prevalent strategy for 3D reconstruction from unordered image collections. While incremental reconstruction systems have tremendously advanced in all regards, robustness, accuracy, completeness, and scalability remain the key problems towards building a truly general-purpose pipeline. We propose a new SfM technique that improves upon the state of the art to make a further step towards this ultimate goal. The full reconstruction pipeline is released to the public as an open-source implementation.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {SchÃ¶nberger, J. L. and Frahm, J.},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {3D image reconstruction, Cameras, computer vision, image reconstruction, Image reconstruction, Image registration, incremental reconstruction system, Internet, open-source implementation, Pipelines, public domain software, Robustness, SfM, stereo image processing, structure-from-motion, Transmission line matrix methods},
	pages = {4104--4113},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jerem\\Zotero\\storage\\DDALPKSR\\7780814.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\PUJB6SRU\\SchÃ¶nberger and Frahm - 2016 - Structure-from-Motion Revisited.pdf:application/pdf},
}

@inproceedings{koenig_design_2004,
	title = {Design and use paradigms for {Gazebo}, an open-source multi-robot simulator},
	volume = {3},
	doi = {10.1109/IROS.2004.1389727},
	abstract = {Simulators have played a critical role in robotics research as tools for quick and efficient testing of new concepts, strategies, and algorithms. To date, most simulators have been restricted to 2D worlds, and few have matured to the point where they are both highly capable and easily adaptable. Gazebo is designed to fill this niche by creating a 3D dynamic multi-robot environment capable of recreating the complex worlds that would be encountered by the next generation of mobile robots. Its open source status, fine grained control, and high fidelity place Gazebo in a unique position to become more than just a stepping stone between the drawing board and real hardware: data visualization, simulation of remote environments, and even reverse engineering of blackbox systems are all possible applications. Gazebo is developed in cooperation with the Player and Stage projects (Gerkey, B. P., et al., July 2003), (Gerkey, B. P., et al., May 2001), (Vaughan, R. T., et al., Oct. 2003), and is available from http://playerstage.sourceforge.net/gazebo/ gazebo.html.},
	booktitle = {2004 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS}) ({IEEE} {Cat}. {No}.{04CH37566})},
	author = {Koenig, N. and Howard, A.},
	month = sep,
	year = {2004},
	keywords = {application program interfaces, Computational modeling, control engineering computing, digital simulation, Educational robots, Friction, Gazebo, grained control, mobile robots, Mobile robots, multi-robot systems, Open source software, open-source multi-robot simulator, Packaging, Player project, Robot sensing systems, Service robots, Stage project, Testing, Vehicle dynamics},
	pages = {2149--2154 vol.3},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jerem\\Zotero\\storage\\X9WNXXBA\\1389727.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\BMIY49KV\\Koenig and Howard - 2004 - Design and use paradigms for Gazebo, an open-sourc.pdf:application/pdf},
}

@incollection{Efraimidis2008,
	address = {Boston, MA},
	title = {Weighted random sampling},
	isbn = {978-0-387-30162-4},
	url = {https://doi.org/10.1007/978-0-387-30162-4_478},
	booktitle = {Encyclopedia of algorithms},
	publisher = {Springer US},
	author = {Efraimidis, Pavlos and Spirakis, Paul},
	editor = {Kao, Ming-Yang},
	year = {2008},
	doi = {10.1007/978-0-387-30162-4â‚„78},
	pages = {1024--1027},
}

@misc{unrealengine,
  title = {Unreal Engine},
  author = {{Epic Games}},
  year = {2019},
  month = apr
}


@misc{urbancity,
  author ={PolyPixel},
  title = {Unreal Marketplace - Urban City},
  howpublished  = {[Online] Available: \url{https://www.unrealengine.com/marketplace/urban-city}},
  note = {Accessed: 2017-01-05},
  year={2018}
}


@misc{urbanrooftop,
  author= {Croissant},
  title = {Unreal Marketplace - Urban Rooftop},
  howpublished  = {[Online] Available: \url{https://www.unrealengine.com/marketplace/rooftop-pack}},
  note = {Accessed: 2017-01-05},
  year={2018}
}


@article{mohamed_survey_2019,
	title = {A {Survey} on {Odometry} for {Autonomous} {Navigation} {Systems}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2929133},
	abstract = {The development of a navigation system is one of the major challenges in building a fully autonomous platform. Full autonomy requires a dependable navigation capability not only in a perfect situation with clear GPS signals but also in situations, where the GPS is unreliable. Therefore, self-contained odometry systems have attracted much attention recently. This paper provides a general and comprehensive overview of the state of the art in the field of self-contained, i.e., GPS denied odometry systems, and identifies the out-coming challenges that demand further research in future. Self-contained odometry methods are categorized into five main types, i.e., wheel, inertial, laser, radar, and visual, where such categorization is based on the type of the sensor data being used for the odometry. Most of the research in the field is focused on analyzing the sensor data exhaustively or partially to extract the vehicle pose. Different combinations and fusions of sensor data in a tightly/loosely coupled manner and with filtering or optimizing fusion method have been investigated. We analyze the advantages and weaknesses of each approach in terms of different evaluation metrics, such as performance, response time, energy efficiency, and accuracy, which can be a useful guideline for researchers and engineers in the field. In the end, some future research challenges in the field are discussed.},
	journal = {IEEE Access},
	author = {Mohamed, S. A. S. and Haghbayan, M. and Westerlund, T. and Heikkonen, J. and Tenhunen, H. and Plosila, J.},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {autonomous navigation systems, distance measurement, filter-based, fully autonomous platform, fusion method filtering, Global Positioning System, GPS signals, GPS-denied, inertial odoemtry, laser odometry, Laser radar, Lasers, loosely-coupled, mobile robots, navigation, optimization-based, Satellite broadcasting, Self-contained localization, self-contained odometry methods, self-contained odometry systems, sensor data analysis, sensor data fusions, sensor fusion, Simultaneous localization and mapping, tightly-coupled, visual-inertial odometry, wheel odometry, Wheels},
	pages = {97466--97486},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jerem\\Zotero\\storage\\Z5TJMLDC\\8764393.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\43F77HTI\\Mohamed et al. - 2019 - A Survey on Odometry for Autonomous Navigation Sys.pdf:application/pdf},
}

@article{zhang_low-drift_2017,
	title = {Low-drift and real-time lidar odometry and mapping},
	volume = {41},
	issn = {1573-7527},
	url = {https://doi.org/10.1007/s10514-016-9548-2},
	doi = {10.1007/s10514-016-9548-2},
	abstract = {Here we propose a real-time method for low-drift odometry and mapping using range measurements from a 3D laser scanner moving in 6-DOF. The problem is hard because the range measurements are received at different times, and errors in motion estimation (especially without an external reference such as GPS) cause mis-registration of the resulting point cloud. To date, coherent 3D maps have been built by off-line batch methods, often using loop closure to correct for drift over time. Our method achieves both low-drift in motion estimation and low-computational complexity. The key idea that makes this level of performance possible is the division of the complex problem of Simultaneous Localization and Mapping, which seeks to optimize a large number of variables simultaneously, into two algorithms. One algorithm performs odometry at a high-frequency but at low fidelity to estimate velocity of the laser scanner. Although not necessary, if an IMU is available, it can provide a motion prior and mitigate for gross, high-frequency motion. A second algorithm runs at an order of magnitude lower frequency for fine matching and registration of the point cloud. Combination of the two algorithms allows map creation in real-time. Our method has been evaluated by indoor and outdoor experiments as well as the KITTI odometry benchmark. The results indicate that the proposed method can achieve accuracy comparable to the state of the art offline, batch methods.},
	language = {en},
	number = {2},
	urldate = {2021-02-09},
	journal = {Autonomous Robots},
	author = {Zhang, Ji and Singh, Sanjiv},
	month = feb,
	year = {2017},
	pages = {401--416},
	file = {Springer Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\GFBIK6E9\\Zhang and Singh - 2017 - Low-drift and real-time lidar odometry and mapping.pdf:application/pdf},
}


@inproceedings{yan_loose_2017,
	title = {Loose coupling visual-lidar odometry by combining {VISO2} and {LOAM}},
	doi = {10.23919/ChiCC.2017.8028435},
	abstract = {Odometry is a very popular research direction in recent decades and plays a very critical role in the autonomous navigation system. VISO2, as a relatively robust and mature visual odometry algorithm, becomes unreliable when distinct visual features are insufficiently or when the texture of the environment is repetitive. Even if not in such a challenging environment, the drift is still a big problem for the practical application. LOAM, a lidar odometry algorithm, has been leading the way for the performance on KITTI data set. In fact, it also has limitations in shape features not prominent environment, like a highway with few buildings around. Considering that the current mobile unmanned platforms (e.g., unmanned vehicles) are almost equipped with both lidar and cameras, this paper proposes a loose coupling visual-lidar odometry by combining VISO2 and LOAM to achieve the advantages of both camera and lidar and reduce the number of restricted scenarios. The algorithm proposed in this paper is tested on the raw data set of KITTI. The average translation error is about 1.37\% on 10 sequences, and the accuracy is improved with respect to LOAM with the same parameters. With the help of VISO2, the algorithm also performs well in shape features not prominent environment (e.g., highway), and from the experiment results, we confirmed the efficient strategy to combine VISO2 and LOAM to adapt to challenging environment and to keep the accuracy of the algorithm.},
	booktitle = {2017 36th {Chinese} {Control} {Conference} ({CCC})},
	author = {Yan, M. and Wang, J. and Li, J. and Zhang, C.},
	month = jul,
	year = {2017},
	note = {ISSN: 1934-1768},
	keywords = {autonomous navigation system, cameras, Cameras, distance measurement, distinct visual features, Distortion, Ego-motion Estimation, KITTI data set, Laser radar, Lidar Odometry, lidar odometry algorithm, LOAM, loose coupling visual-lidar odometry, mobile robots, optical radar, Simultaneous localization and mapping, SLAM, Three-dimensional displays, Transforms, VISO2, Visual Odometry, Visualization},
	pages = {6841--6846},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jerem\\Zotero\\storage\\89P3PVWD\\8028435.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\P7PRI37I\\Yan et al. - 2017 - Loose coupling visual-lidar odometry by combining .pdf:application/pdf},
}

@inproceedings{zhang_point_2019,
	title = {A {Point} {Cloud} {Distortion} {Removing} and {Mapping} {Algorithm} based on {Lidar} and {IMU} {UKF} {Fusion}},
	doi = {10.1109/AIM.2019.8868647},
	abstract = {When the lidar motion is rapid, point cloud distortion is an important factor affecting the accuracy of lidar odometry. In this paper, an effective lidar and IMU fusion based point cloud distortion removing and mapping algorithm is proposed. UKF is used to fuse the angular velocity, linear acceleration of the IMU and the pose of the lidar. It can get the angular velocity bias, linear acceleration bias of the IMU and the linear velocity of the lidar. Then the pose of the lidar at each time in a scanning period is obtained, therefore the distortion of the point cloud can be removed. UKF can also estimate the pose of the lidar, which can be used as the initial value of the point cloud registration algorithm in lidar odometry, therefore the computation time can be reduced. The rapid rotation and rapid translation motion experiments show that the proposed method can deal with the rapid motion of lidar, and achieve high efficiency and precision mapping.},
	booktitle = {2019 {IEEE}/{ASME} {International} {Conference} on {Advanced} {Intelligent} {Mechatronics} ({AIM})},
	author = {Zhang, B. and Zhang, X. and Wei, B. and Qi, C.},
	month = jul,
	year = {2019},
	note = {ISSN: 2159-6255},
	keywords = {Acceleration, Angular velocity, angular velocity bias, Coordinate measuring machines, distance measurement, image registration, IMU UKF fusion, Kalman filters, Laser radar, lidar motion, lidar odometry, linear acceleration bias, linear velocity, Nonlinear distortion, nonlinear filters, optical radar, point cloud distortion mapping algorithm, point cloud distortion removing algorithm, point cloud registration algorithm, Three-dimensional displays},
	pages = {966--971},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jerem\\Zotero\\storage\\U6X3B5IY\\8868647.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\WSDJUBK4\\Zhang et al. - 2019 - A Point Cloud Distortion Removing and Mapping Algo.pdf:application/pdf},
}

@inproceedings{zhang_visual-lidar_2015,
	title = {Visual-lidar odometry and mapping: low-drift, robust, and fast},
	shorttitle = {Visual-lidar odometry and mapping},
	doi = {10.1109/ICRA.2015.7139486},
	abstract = {Here, we present a general framework for combining visual odometry and lidar odometry in a fundamental and first principle method. The method shows improvements in performance over the state of the art, particularly in robustness to aggressive motion and temporary lack of visual features. The proposed on-line method starts with visual odometry to estimate the ego-motion and to register point clouds from a scanning lidar at a high frequency but low fidelity. Then, scan matching based lidar odometry refines the motion estimation and point cloud registration simultaneously.We show results with datasets collected in our own experiments as well as using the KITTI odometry benchmark. Our proposed method is ranked \#1 on the benchmark in terms of average translation and rotation errors, with a 0.75\% of relative position drift. In addition to comparison of the motion estimation accuracy, we evaluate robustness of the method when the sensor suite moves at a high speed and is subject to significant ambient lighting changes.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Zhang, J. and Singh, S.},
	month = may,
	year = {2015},
	note = {ISSN: 1050-4729},
	keywords = {aggressive motion, ambient lighting changes, Cameras, distance measurement, Distortion, ego-motion estimation, Feature extraction, first principle method, image matching, image registration, KITTI odometry benchmark, Laser radar, motion estimation, Motion estimation, optical radar, point cloud registration, scan matching based lidar odometry, temporary lack of visual features, Three-dimensional displays, visual-lidar mapping, visual-lidar odometry, Visualization},
	pages = {2174--2181},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jerem\\Zotero\\storage\\ILJL6Y5Q\\7139486.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\NVBQPN7N\\Zhang and Singh - 2015 - Visual-lidar odometry and mapping low-drift, robu.pdf:application/pdf},
}

@inproceedings{zhang_shufflenet_2018,
	title = {{ShuffleNet}: {An} {Extremely} {Efficient} {Convolutional} {Neural} {Network} for {Mobile} {Devices}},
	shorttitle = {{ShuffleNet}},
	doi = {10.1109/CVPR.2018.00716},
	abstract = {We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8\%) than recent MobileNet [12] on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves 13Ã— actual speedup over AlexNet while maintaining comparable accuracy.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zhang, X. and Zhou, X. and Lin, M. and Sun, J.},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {ARM-based mobile device, Complexity theory, computation-efficient CNN architecture, Computational modeling, Computer architecture, convolution, Convolution, extremely efficient convolutional neural network, feedforward neural nets, image classification, ImageNet classification task, mobile computing, Mobile handsets, MS COCO object detection, Neural networks, object detection, pointwise group convolution, ShuffleNet, Task analysis},
	pages = {6848--6856},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jerem\\Zotero\\storage\\KNPDB4XB\\8578814.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\PTMGZRV7\\Zhang et al. - 2018 - ShuffleNet An Extremely Efficient Convolutional N.pdf:application/pdf},
}

@article{howard_mobilenets_2017,
	title = {{MobileNets}: {Efficient} {Convolutional} {Neural} {Networks} for {Mobile} {Vision} {Applications}},
	shorttitle = {{MobileNets}},
	url = {http://arxiv.org/abs/1704.04861},
	abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
	urldate = {2021-02-08},
	journal = {arXiv:1704.04861 [cs]},
	author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.04861},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\Jerem\\Zotero\\storage\\U4M7CVW7\\1704.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Jerem\\Zotero\\storage\\DGNN5PNY\\Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf:application/pdf},
}

@inproceedings{siam_comparative_2018,
	title = {A {Comparative} {Study} of {Real}-{Time} {Semantic} {Segmentation} for {Autonomous} {Driving}},
	doi = {10.1109/CVPRW.2018.00101},
	abstract = {Semantic segmentation is a critical module in robotics related applications, especially autonomous driving. Most of the research on semantic segmentation is focused on improving the accuracy with less attention paid to computationally efficient solutions. Majority of the efficient semantic segmentation algorithms have customized optimizations without scalability and there is no systematic way to compare them. In this paper, we present a real-time segmentation benchmarking framework and study various segmentation algorithms for autonomous driving. We implemented a generic meta-architecture via a decoupled design where different types of encoders and decoders can be plugged in independently. We provide several example encoders including VGG16, Resnet18, MobileNet, and ShuffleNet and decoders including SkipNet, UNet and Dilation Frontend. The framework is scalable for addition of new encoders and decoders developed in the community for other vision tasks. We performed detailed experimental analysis on cityscapes dataset for various combinations of encoder and decoder. The modular framework enabled rapid prototyping of a custom efficient architecture which provides x143 GFLOPs reduction compared to SegNet and runs real-time at 15 fps on NVIDIA Jetson TX2. The source code of the framework is publicly available.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Siam, M. and Gamal, M. and Abdel-Razek, M. and Yogamani, S. and Jagersand, M. and Zhang, H.},
	month = jun,
	year = {2018},
	note = {ISSN: 2160-7516},
	keywords = {autonomous driving, Computer architecture, computer vision, Context modeling, Convolution, critical module, custom efficient architecture, customized optimizations, decoder, Decoding, decoupled design, efficient semantic segmentation algorithms, encoder, generic meta-architecture, image segmentation, Image segmentation, optimisation, rapid prototyping, real-time segmentation benchmarking framework, real-time semantic segmentation, real-time systems, Real-time systems, robotics related applications, Semantics, various segmentation algorithms},
	pages = {700--70010},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jerem\\Zotero\\storage\\MXZVUX27\\8575250.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\L8SZRDH5\\Siam et al. - 2018 - A Comparative Study of Real-Time Semantic Segmenta.pdf:application/pdf},
}

@article{chen_deeplab_2018,
	title = {{DeepLab}: {Semantic} {Image} {Segmentation} with {Deep} {Convolutional} {Nets}, {Atrous} {Convolution}, and {Fully} {Connected} {CRFs}},
	volume = {40},
	issn = {1939-3539},
	shorttitle = {{DeepLab}},
	doi = {10.1109/TPAMI.2017.2699184},
	abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or `atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed â€œDeepLabâ€ system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Chen, L. and Papandreou, G. and Kokkinos, I. and Murphy, K. and Yuille, A. L.},
	month = apr,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {atrous convolution, atrous spatial pyramid pooling, Computational modeling, conditional random fields, Context, convolution, Convolution, Convolutional neural networks, deep convolutional nets, Deep Convolutional Neural Networks, Deep Learning, DeepLab, feature extraction, feedforward neural nets, fully connected Conditional Random Field, highlight convolution, image context, Image resolution, image segmentation, Image segmentation, learning (artificial intelligence), Neural networks, PASCAL VOC-2012 semantic image segmentation task, probabilistic graphical models, random processes, semantic image segmentation, semantic segmentation, Semantics},
	pages = {834--848},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jerem\\Zotero\\storage\\FERRNWDN\\7913730.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\XP5YVZLX\\Chen et al. - 2018 - DeepLab Semantic Image Segmentation with Deep Con.pdf:application/pdf},
}

@article{chen_rethinking_2017,
	title = {Rethinking {Atrous} {Convolution} for {Semantic} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1706.05587},
	abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
	urldate = {2021-02-08},
	journal = {arXiv:1706.05587 [cs]},
	author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.05587},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Add more experimental results},
	file = {arXiv.org Snapshot:C\:\\Users\\Jerem\\Zotero\\storage\\XXHKBF2I\\1706.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Jerem\\Zotero\\storage\\AS3EUTRD\\Chen et al. - 2017 - Rethinking Atrous Convolution for Semantic Image S.pdf:application/pdf},
}

@inproceedings{kendall_bayesian_2017,
	address = {London, UK},
	title = {Bayesian {SegNet}: {Model} {Uncertainty} in {Deep} {Convolutional} {Encoder}-{Decoder} {Architectures} for {Scene} {Understanding}},
	isbn = {978-1-901725-60-5},
	shorttitle = {Bayesian {SegNet}},
	url = {http://www.bmva.org/bmvc/2017/papers/paper057/index.html},
	doi = {10.5244/C.31.57},
	language = {en},
	urldate = {2021-02-08},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2017},
	publisher = {British Machine Vision Association},
	author = {Kendall, Alex and Badrinarayanan, Vijay and Cipolla, Roberto},
	year = {2017},
	pages = {57},
	file = {Submitted Version:C\:\\Users\\Jerem\\Zotero\\storage\\QJWJQS3P\\Kendall et al. - 2017 - Bayesian SegNet Model Uncertainty in Deep Convolu.pdf:application/pdf},
}

@article{badrinarayanan_segnet_2017,
	title = {{SegNet}: {A} {Deep} {Convolutional} {Encoder}-{Decoder} {Architecture} for {Image} {Segmentation}},
	volume = {39},
	issn = {1939-3539},
	shorttitle = {{SegNet}},
	doi = {10.1109/TPAMI.2016.2644615},
	abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
	number = {12},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Badrinarayanan, V. and Kendall, A. and Cipolla, R.},
	month = dec,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Caffe implementation, competitive inference time, Computer architecture, Convolutional codes, convolutional layers, core trainable segmentation engine, decoder, decoder network, Decoding, DeconvNet architectures, deep convolutional encoder-decoder architecture, Deep convolutional neural networks, DeepLab-LargeFOV, dense feature maps, encoder, encoder network, FCN, feature extraction, gradient methods, image classification, image colour analysis, image representation, image resolution, image segmentation, Image segmentation, indoor scenes, inference mechanisms, learning (artificial intelligence), low resolution encoder feature maps, lower resolution input feature map, max-pooling step, Neural networks, nonlinear upsampling, pixel-wise classification layer, pixel-wise segmentation, pooling, practical deep fully convolutional neural network architecture, road scenes, SegNet, self-organising feature maps, semantic pixel-wise segmentation, Semantics, stochastic gradient descent, SUN RGB-D indoor scene segmentation, SUN RGB-D indoor scene segmentation tasks, topology, Training, upsampling, VGG16 network},
	pages = {2481--2495},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jerem\\Zotero\\storage\\AG7HHMK3\\7803544.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\3QLNBQII\\Badrinarayanan et al. - 2017 - SegNet A Deep Convolutional Encoder-Decoder Archi.pdf:application/pdf},
}

@inproceedings{ronneberger_u-net_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	isbn = {978-3-319-24574-4},
	shorttitle = {U-{Net}},
	doi = {10.1007/978-3-319-24574-4_28},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} â€“ {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	year = {2015},
	keywords = {Convolutional Layer, Data Augmentation, Deep Network, Ground Truth Segmentation, Training Image},
	pages = {234--241},
	file = {Springer Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\54Q3JVY9\\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf},
}

@article{shelhamer_fully_2017,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	volume = {39},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2016.2572683},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build â€œfully convolutionalâ€ networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30\% relative improvement to 67.2\% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Shelhamer, E. and Long, J. and Darrell, T.},
	month = apr,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {coarse layer, Computer architecture, contemporary classification networks, Convolution, Convolutional Networks, correspondingly-sized output, Deep Learning, feedforward neural nets, fine layer, fully convolutional networks, Fuses, image classification, image representation, image resolution, image segmentation, Image segmentation, learned representations, learning (artificial intelligence), NYUDv2, PASCAL VOC, PASCAL-Context, Proposals, semantic segmentation, Semantic Segmentation, Semantics, SIFT Flow, spatially dense prediction tasks, Training, Transfer Learning, transforms, visual models},
	pages = {640--651},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jerem\\Zotero\\storage\\Z4UFFERX\\7478072.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\RZLXYUJ8\\Shelhamer et al. - 2017 - Fully Convolutional Networks for Semantic Segmenta.pdf:application/pdf},
}


@article{yang_autonomous_2014,
	title = {Autonomous {Landing} of {MAVs} on an {Arbitrarily} {Textured} {Landing} {Site} {Using} {Onboard} {Monocular} {Vision}},
	volume = {74},
	issn = {1573-0409},
	url = {https://doi.org/10.1007/s10846-013-9906-7},
	doi = {10.1007/s10846-013-9906-7},
	abstract = {This paper presents a novel solution for micro aerial vehicles (MAVs) to autonomously search for and land on an arbitrary landing site using real-time monocular vision. The autonomous MAV is provided with only one single reference image of the landing site with an unknown size before initiating this task. We extend a well-known monocular visual SLAM algorithm that enables autonomous navigation of the MAV in unknown environments, in order to search for such landing sites. Furthermore, a multi-scale ORB feature based method is implemented and integrated into the SLAM framework for landing site detection. We use a RANSAC-based method to locate the landing site within the map of the SLAM system, taking advantage of those map points associated with the detected landing site. We demonstrate the efficiency of the presented vision system in autonomous flights, both indoor and in challenging outdoor environment.},
	language = {en},
	number = {1},
	urldate = {2021-02-08},
	journal = {Journal of Intelligent \& Robotic Systems},
	author = {Yang, Shaowu and Scherer, Sebastian A. and Schauwecker, Konstantin and Zell, Andreas},
	month = apr,
	year = {2014},
	pages = {27--43},
	file = {Springer Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\URR8KL92\\Yang et al. - 2014 - Autonomous Landing of MAVs on an Arbitrarily Textu.pdf:application/pdf},
}

@inproceedings{shuo_yang_precise_2015,
	title = {Precise quadrotor autonomous landing with {SRUKF} vision perception},
	doi = {10.1109/ICRA.2015.7139489},
	abstract = {We present an autonomous quadrotor system that is able to perform high precision landing on small platform in both indoor and outdoor environment. Its taking off and landing processes are fully autonomous. We use vision sensor to detect the landing platform, and the vision measurement is enhanced by IMU with SRUKF based sensor fusion method. All computation are done real-time and on-board. We implement the system and carry a series of experiments under various environmental conditions. The experiment results confirm the robustness and precision of our system in real use cases.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {{Shuo Yang} and {Jiahang Ying} and {Yang Lu} and {Zexiang Li}},
	month = may,
	year = {2015},
	note = {ISSN: 1050-4729},
	keywords = {autonomous aerial vehicles, autonomous quadrotor system, Cameras, Estimation, helicopters, high precision landing, IMU, Kalman filters, mobile robots, Noise, nonlinear filters, Optical sensors, position control, quadrotor autonomous landing, Robot sensing systems, robot vision, Rotation measurement, sensor fusion, sensor fusion method, Software, SRUKF vision perception, telerobotics, unscented Kalman filter, vision sensor},
	pages = {2196--2201},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jerem\\Zotero\\storage\\SD99CHPR\\7139489.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\75ADT8IY\\Shuo Yang et al. - 2015 - Precise quadrotor autonomous landing with SRUKF vi.pdf:application/pdf},
}

@inproceedings{dotenco_autonomous_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Autonomous {Approach} and {Landing} for a {Low}-{Cost} {Quadrotor} {Using} {Monocular} {Cameras}},
	isbn = {978-3-319-16178-5},
	doi = {10.1007/978-3-319-16178-5_14},
	abstract = {In this paper, we propose a monocular vision system for approach and landing using a low-cost micro aerial vehicle (MAV). The system enables an off-the-shelf Parrot AR.Drone 2.0 quadrotor MAV to autonomously detect a landpad, approach it, and land on it. Particularly, we exploit geometric properties of a circular landpad marker in order to estimate the exact flight distance between the quadrotor and the landing spot. We then employ monocular simultaneous localization and mapping (SLAM) to fly towards the landpad while accurately following a trajectory. Notably, our system does not require the landpad to be located directly underneath the MAV.},
	language = {en},
	booktitle = {Computer {Vision} - {ECCV} 2014 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Dotenco, Sergiu and Gallwitz, Florian and Angelopoulou, Elli},
	editor = {Agapito, Lourdes and Bronstein, Michael M. and Rother, Carsten},
	year = {2015},
	keywords = {Approach and landing, Conic sections, Ellipse detection, MAV, Pose estimation, PTAM, SLAM},
	pages = {209--222},
	file = {Springer Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\R2IXFXV8\\Dotenco et al. - 2015 - Autonomous Approach and Landing for a Low-Cost Qua.pdf:application/pdf},
}

@inproceedings{jin_-board_2016,
	title = {On-board vision autonomous landing techniques for quadrotor: {A} survey},
	shorttitle = {On-board vision autonomous landing techniques for quadrotor},
	doi = {10.1109/ChiCC.2016.7554984},
	abstract = {Nowadays, quadrotor is playing a growing number of critical roles in a great many of aspects. A crucial ability is the autonomous landing ability onto a target such as a fixed platform, moving platform, or ship deck platform, employing on-board vision. Researching autonomous landing technologies employing vision for quadrotor have grown up to be the second active area of research over the past years. This article primarily gives the foremost research groups involved in the development of on-board vision autonomous landing technologies for quadrotor, then it discusses the specifics of two factors are taken into account for a smooth landing: sensing and control. The aim of this article is to examine the state-of-the art autonomous landing technologies employing on-board vision that capture entire milestones and seminal works. Last but not least, the article stands out three challenging parts in this research field.},
	booktitle = {2016 35th {Chinese} {Control} {Conference} ({CCC})},
	author = {Jin, S. and Zhang, J. and Shen, L. and Li, T.},
	month = jul,
	year = {2016},
	note = {ISSN: 1934-1768},
	keywords = {autonomous landing, autonomous landing ability, Cameras, computer vision, crucial ability, entry, descent and landing (spacecraft), fixed platform, Global Positioning System, helicopters, Machine vision, Marine vehicles, moving platform, on-board vision, on-board vision autonomous landing techniques, on-board vision autonomous landing technologies, quadrotor, Sensors, ship deck platform, smooth landing, Unmanned aerial vehicles, Visualization},
	pages = {10284--10289},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jerem\\Zotero\\storage\\FW9Z78GN\\7554984.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\4RXNBC2J\\Jin et al. - 2016 - On-board vision autonomous landing techniques for .pdf:application/pdf},
}

@inproceedings{shah_airsim_2018,
	address = {Cham},
	series = {Springer {Proceedings} in {Advanced} {Robotics}},
	title = {{AirSim}: {High}-{Fidelity} {Visual} and {Physical} {Simulation} for {Autonomous} {Vehicles}},
	isbn = {978-3-319-67361-5},
	shorttitle = {{AirSim}},
	doi = {10.1007/978-3-319-67361-5_40},
	abstract = {Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process. Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments. We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals. Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop (HITL) simulations with support for popular protocols (e.g. MavLink). The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols. In addition, the modular design enables various components to be easily usable independently in other projects. We demonstrate the simulator by first implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world flights.},
	language = {en},
	booktitle = {Field and {Service} {Robotics}},
	publisher = {Springer International Publishing},
	author = {Shah, Shital and Dey, Debadeepta and Lovett, Chris and Kapoor, Ashish},
	editor = {Hutter, Marco and Siegwart, Roland},
	year = {2018},
	pages = {621--635},
	file = {Springer Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\72JD4928\\Shah et al. - 2018 - AirSim High-Fidelity Visual and Physical Simulati.pdf:application/pdf},
}

@article{lambert_performance_2020,
	title = {Performance {Analysis} of 10 {Models} of {3D} {LiDARs} for {Automated} {Driving}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3009680},
	abstract = {Automated vehicle technology has recently become reliant on 3D LiDAR sensing for perception tasks such as mapping, localization and object detection. This has led to a rapid growth in the LiDAR manufacturing industry with several competing makers releasing new sensors regularly. With this increased variety of LiDARs, each with different properties such as number of laser emitters, resolution, field-of-view, and price tags, a more in-depth comparison of their characteristics and performance is required. This work compares 10 commonly used 3D LiDARs, establishing several metrics to assess their performance. Various outstanding issues with specific LiDARs were qualitatively identified. The accuracy and precision of individual LiDAR beams and accumulated point clouds are evaluated in a controlled environment at distances from 5 to 180 meters. Reflective targets were used to characterize intensity patterns and quantify the impact of surface reflectivity on accuracy and precision. A vehicle and pedestrian mannequin were also used as additional targets of interest. A thorough assessment of these LiDARs is given with their potential applicability for automated driving tasks. The data collected in these experiments and analysis tools are all shared openly.},
	journal = {IEEE Access},
	author = {Lambert, Jacob and Carballo, Alexander and Cano, Abraham Monrroy and Narksri, Patiphon and Wong, David and Takeuchi, Eijiro and Takeda, Kazuya},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {3D LiDAR, 3D LiDAR sensing, 3D LiDARs, 3D sensing, Analytical models, automated driving, automated driving tasks, automated vehicle technology, autonomous driving, benchmark, competing makers, individual LiDAR beams, Laser radar, LiDAR manufacturing industry, localization, mobile robots, object detection, Object detection, optical radar, pedestrian mannequin, perception tasks, performance analysis, robot vision, Sensor phenomena and characterization, sensors, SLAM (robots), Task analysis, Three-dimensional displays},
	pages = {131699--131722},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jerem\\Zotero\\storage\\8PSHJM2J\\9142208.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\ZPVPH9AC\\Lambert et al. - 2020 - Performance Analysis of 10 Models of 3D LiDARs for.pdf:application/pdf},
}

@article{glennie_static_2010,
	title = {Static {Calibration} and {Analysis} of the {Velodyne} {HDL}-{64E} {S2} for {High} {Accuracy} {Mobile} {Scanning}},
	volume = {2},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/2/6/1610},
	doi = {10.3390/rs2061610},
	abstract = {The static calibration and analysis of the Velodyne HDL-64E S2 scanning LiDAR system is presented and analyzed. The mathematical model for measurements for the HDL-64E S2 scanner is derived and discussed. A planar feature based least squares adjustment approach is presented and utilized in a minimally constrained network in order to derive an optimal solution for the laserâ€™s internal calibration parameters. Finally, the results of the adjustment along with a detailed examination of the adjustment residuals are given. A three-fold improvement in the planar misclosure residual RMSE over the standard factory calibration model was achieved by the proposed calibration. Results also suggest that there may still be some unmodelled distortions in the range measurements from the scanner. However, despite this, the overall precision of the adjusted laser scanner data appears to make it a viable choice for high accuracy mobile scanning applications.},
	language = {en},
	number = {6},
	urldate = {2020-09-15},
	journal = {Remote Sensing},
	author = {Glennie, Craig and Lichti, Derek D.},
	month = jun,
	year = {2010},
	note = {Number: 6
Publisher: Molecular Diversity Preservation International},
	keywords = {accuracy, error analysis, LiDAR, system calibration},
	pages = {1610--1624},
	file = {Snapshot:C\:\\Users\\Jerem\\Zotero\\storage\\TZ46NHWS\\1610.html:text/html;Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\VTUS9AKA\\Glennie and Lichti - 2010 - Static Calibration and Analysis of the Velodyne HD.pdf:application/pdf},
}

@article{kaasalainen_analysis_2011,
	title = {Analysis of {Incidence} {Angle} and {Distance} {Effects} on {Terrestrial} {Laser} {Scanner} {Intensity}: {Search} for {Correction} {Methods}},
	volume = {3},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Analysis of {Incidence} {Angle} and {Distance} {Effects} on {Terrestrial} {Laser} {Scanner} {Intensity}},
	url = {https://www.mdpi.com/2072-4292/3/10/2207},
	doi = {10.3390/rs3102207},
	abstract = {The intensity information from terrestrial laser scanners (TLS) has become an important object of study in recent years, and there are an increasing number of applications that would benefit from the addition of calibrated intensity data to the topographic information. In this paper, we study the range and incidence angle effects on the intensity measurements and search for practical correction methods for different TLS instruments and targets. We find that the range (distance) effect is strongly dominated by instrumental factors, whereas the incidence angle effect is mainly caused by the target surface properties. Correction for both effects is possible, but more studies are needed for physical interpretation and more efficient use of intensity data for target characterization.},
	language = {en},
	number = {10},
	urldate = {2020-09-04},
	journal = {Remote Sensing},
	author = {Kaasalainen, Sanna and Jaakkola, Anttoni and Kaasalainen, Mikko and Krooks, Anssi and Kukko, Antero},
	month = oct,
	year = {2011},
	note = {Number: 10
Publisher: Molecular Diversity Preservation International},
	keywords = {incidence angle, intensity, LiDAR, radiometric calibration, terrestrial laser scanning},
	pages = {2207--2221},
	file = {Snapshot:C\:\\Users\\Jerem\\Zotero\\storage\\AASLXKYP\\2207.html:text/html;Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\DP54B5R5\\Kaasalainen et al. - 2011 - Analysis of Incidence Angle and Distance Effects o.pdf:application/pdf},
}


@misc{Castagno_Github_realsensepackage,
  title = {Github - {RealSense Sensor Package}},
  author = {Castagno, Jeremy},
  year = {2020},
  howpublished = {[Online] Available: \url{https://github.com/JeremyBYU/realsense-tracking}},
  note = {Accessed: 2020-06-05}
}

@misc{Continental_Github_ecal,
  title = {Github - {Enhanced Communcaton and Abstraction Library}},
  author = {Continental},
  year = {2021},
  howpublished = {[Online] Available: \url{https://github.com/continental/ecal}},
  note = {Accessed: 2021-06-05}
}



@book{antenucci_geographic_1991,
	title = {{GEOGRAPHIC} {INFORMATION} {SYSTEMS}: {A} {GUIDE} {TO} {THE} {TECHNOLOGY}},
	isbn = {978-0-442-00756-0},
	publisher={Springer US},
	shorttitle = {{GEOGRAPHIC} {INFORMATION} {SYSTEMS}},
	url = {https://trid.trb.org/view/368512},
	urldate = {2021-05-28},
	author = {Antenucci, J. C. and Brown, K. and Croswell, P. L. and Kevany, M. J. and Archer, H.},
	year = {1991},
	file = {Snapshot:C\:\\Users\\Jerem\\Zotero\\storage\\WI8V5DXB\\368512.html:text/html},
}



@book{GIS_demystified,
Author = {Galati, Stephen R.},
ISBN = {9781580535335},
Publisher = {Artech House, Inc},
Series = {Artech House Mobile Communications Series},
Title = {Geographic Information Systems Demystified.},
Year = {2006},
}


@article{wgs84,
author = { Muneendra   Kumar },
title = {World geodetic system 1984: A modern and accurate global reference frame},
journal = {Marine Geodesy},
volume = {12},
number = {2},
pages = {117-126},
year  = {1988},
publisher = {Taylor & Francis},
doi = {10.1080/15210608809379580},

URL = { 
        https://doi.org/10.1080/15210608809379580
    
},
eprint = { 
        https://doi.org/10.1080/15210608809379580
    
}

}



@inproceedings{frantis_emergency_2011,
	title = {Emergency and precautionary landing assistant},
	doi = {10.1109/DASC.2011.6096111},
	abstract = {According to statistics, there are many emergency landing into field due to engine failure. This situation is very stressful for the pilot in command and in general aviation airplanes (mainly in the LSA and small aircraft category) there are no instruments that would help pilot to deal with such a situation. Modern synthetic vision system with built-in 3D terrain database provides very little assistance or they are not helping at all with handling emergency landing in a field. This article deals with development and testing of emergency and precautionary landing assistant system based on synthetic vision system with detailed 3D terrain database. New functions that could help the pilot to deal with the difficult task are described. In conclusion the results of flight tests are discussed.},
	booktitle = {2011 {IEEE}/{AIAA} 30th {Digital} {Avionics} {Systems} {Conference}},
	author = {Frantis, Petr},
	month = oct,
	year = {2011},
	note = {ISSN: 2155-7209},
	keywords = {Airplanes, Airports, Instruments, Machine vision, Three dimensional displays, Vectors},
	pages = {6E2--1--6E2--6},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\CSQVZTU7\\Frantis - 2011 - Emergency and precautionary landing assistant.pdf:application/pdf},
}



@article{loureiro_emergency_2021,
	title = {Emergency {Landing} {Spot} {Detection} {Algorithm} for {Unmanned} {Aerial} {Vehicles}},
	volume = {13},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/13/10/1930},
	doi = {10.3390/rs13101930},
	abstract = {The use and research of Unmanned Aerial Vehicle (UAV) have been increasing over the years due to the applicability in several operations such as search and rescue, delivery, surveillance, and others. Considering the increased presence of these vehicles in the airspace, it becomes necessary to reflect on the safety issues or failures that the UAVs may have and the appropriate action. Moreover, in many missions, the vehicle will not return to its original location. If it fails to arrive at the landing spot, it needs to have the onboard capability to estimate the best area to safely land. This paper addresses the scenario of detecting a safe landing spot during operation. The algorithm classifies the incoming Light Detection and Ranging (LiDAR) data and store the location of suitable areas. The developed method analyses geometric features on point cloud data and detects potential right spots. The algorithm uses the Principal Component Analysis (PCA) to find planes in point cloud clusters. The areas that have a slope less than a threshold are considered potential landing spots. These spots are evaluated regarding ground and vehicle conditions such as the distance to the UAV, the presence of obstacles, the areaâ€™s roughness, and the spotâ€™s slope. Finally, the output of the algorithm is the optimum spot to land and can vary during operation. The proposed approach evaluates the algorithm in simulated scenarios and an experimental dataset presenting suitability to be applied in real-time operations.},
	number = {10},
	journal = {Remote Sensing},
	author = {Loureiro, Gabriel and Dias, AndrÃ© and Martins, Alfredo and Almeida, JosÃ©},
	year = {2021},
}


@inproceedings{yan_safe_2020,
	title = {A {Safe} {Landing} {Site} {Selection} {Method} of {UAVs} {Based} on {LiDAR} {Point} {Clouds}},
	doi = {10.23919/CCC50068.2020.9189499},
	abstract = {Landing site selection is one of the most important tasks in autonomous landing of unmanned aerial vehicles (UAVs). In this paper, a new method to autonomously select safe landing sites from LiDAR point clouds is proposed. Principal component analysis CPCA) and an improved region growing algorithm are utilized to detect flat regions, where plane fitting is executed afterwards for terrain complexity assessment using an improved progressive sample consensus (PROSAC) algorithm. The most suitable landing site of a landing zone is selected according to the terrain complexity. Terrain point clouds of urban and natural scenes are used for simulation experiments. Experimental results show that the landing zones can be classified successfully and the selected landing sitescan meet the safety requirements, which demonstrate the effectiveness and feasibility of our proposed method.},
	booktitle = {2020 39th {Chinese} {Control} {Conference} ({CCC})},
	author = {Yan, Lu and Qi, Juntong and Wang, Mingming and Wu, Chong and Xin, Ji},
	month = jul,
	year = {2020},
	note = {ISSN: 1934-1768},
	keywords = {Classification algorithms, Complexity theory, Eigenvalues and eigenfunctions, Fitting, Laser radar, LiDAR, Mathematical model, point clouds, progressive sample consensus, region growing, safe landing site selection, Three-dimensional displays, UAV},
	pages = {6497--6502},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\D8ZPZFH5\\Yan et al. - 2020 - A Safe Landing Site Selection Method of UAVs Based.pdf:application/pdf},
}


@inproceedings{maturana_3d_2015,
	title = {{3D} {Convolutional} {Neural} {Networks} for landing zone detection from {LiDAR}},
	doi = {10.1109/ICRA.2015.7139679},
	abstract = {We present a system for the detection of small and potentially obscured obstacles in vegetated terrain. The key novelty of this system is the coupling of a volumetric occupancy map with a 3D Convolutional Neural Network (CNN), which to the best of our knowledge has not been previously done. This architecture allows us to train an extremely efficient and highly accurate system for detection tasks from raw occupancy data. We apply this method to the problem of detecting safe landing zones for autonomous helicopters from LiDAR point clouds. Current methods for this problem rely on heuristic rules and use simple geometric features. These heuristics break down in the presence of low vegetation, as they do not distinguish between vegetation that may be landed on and solid objects that should be avoided. We evaluate the system with a combination of real and synthetic range data. We show our system outperforms various benchmarks, including a system integrating various hand-crafted point cloud features from the literature.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Maturana, Daniel and Scherer, Sebastian},
	month = may,
	year = {2015},
	note = {ISSN: 1050-4729},
	keywords = {Blades, Laser radar, Neural networks, Safety, Solid modeling, Three-dimensional displays, Vegetation mapping},
	pages = {3471--3478},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\R4LADSR2\\Maturana and Scherer - 2015 - 3D Convolutional Neural Networks for landing zone .pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Jerem\\Zotero\\storage\\EA2SR6VR\\7139679.html:text/html},
}



@article{poissant_mitigation_2020,
	title = {Mitigation of {Ground} {Impact} {Hazard} for {Safe} {Unmanned} {Aerial} {Vehicle} {Operations}},
	volume = {17},
	url = {https://doi.org/10.2514/1.I010797},
	doi = {10.2514/1.I010797},
	abstract = {Autonomous operation of unmanned aerial vehicles (UAVs) requires development of technologies that allow for safer flight control and response to various flight anomalies. Software for autonomous control should allow the UAV to detect and avoid potential hazards, as well as respond to critical failures midflight without input from a human operator. This paper develops a ground impact and hazard mitigation (GIHM) module that integrates the following: 1) consideration of engine and control surface failure flight modes, 2) generation of feasible ground impact footprints based on glide equations, 3) selection of safest response ground impact sites based on a high-resolution LandScan USA population dataset, and 4) controlled descent to a selected site. For a sample population distribution, integration of GIHM with standard UAV flight software shows a maximum casualty expectation reduction of 97\% compared to the flight software without GIHM near highly populated areas. Incorporation of this hazard mitigation module is successful in reducing fatalities per flight hour, bringing UAVs closer to being safe enough for integration into the National Airspace System.},
	number = {12},
	journal = {Journal of Aerospace Information Systems},
	author = {Poissant, Andrew and Castano, Lina and Xu, Huan},
	year = {2020},
	note = {\_eprint: https://doi.org/10.2514/1.I010797},
	pages = {647--658},
}



@inproceedings{garg_terrain-based_2015,
	title = {Terrain-based landing site selection and path planning for fixed-wing {UAVs}},
	doi = {10.1109/ICUAS.2015.7152297},
	abstract = {UAVs are used in several remote applications. While performing a mission, electro-mechanical faults can occur which requires the UAVs to land quickly and safely. Landing in emergency conditions is a time critical problem, where, the UAV needs to find a landing site and plan a path to the selected site that satisfies some safety criterion in a given time. In this paper, we use terrain information from digital elevation maps to find a list of potential landing sites, selecting a site from the list and determine a path to the selected site in a given time. We use two any time path planners - RRT and PSO for path planning in limited time and compare their performance. Simulation results are presented to validate our approach and an analysis on the effect of path cost, computational time and site selection is presented.},
	booktitle = {2015 {International} {Conference} on {Unmanned} {Aircraft} {Systems} ({ICUAS})},
	author = {Garg, Mayank and Kumar, Abhishek and Sujit, P.B.},
	month = jun,
	year = {2015},
	keywords = {Automation, Conferences, Helicopters, Path planning, Sensors, Time factors, Vegetation},
	pages = {246--251},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\ECGY9Y53\\Garg et al. - 2015 - Terrain-based landing site selection and path plan.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Jerem\\Zotero\\storage\\SL98BNYU\\7152297.html:text/html},
}




@inproceedings{li_development_2015,
	title = {Development of an unmanned aerial vehicle for rooftop landing and surveillance},
	doi = {10.1109/ICUAS.2015.7152368},
	abstract = {This paper presents a novel solution for a typical small-scaled quad-rotor UAV to autonomously search for and land on a pre-designed marker placed on a rooftop and to observe a digit panel on the opposite building. The UAV is equipped with a downward-facing monocular camera for landing target detection and pose estimation, as well as a front-facing camera for digit panel observation and surveillance. The UAV navigates to the landing site based on GPS and transits to vision-guided navigation once the UAV identifies the targeted marker. The target searching and observation thread will take over once the UAV successfully lands on the rooftop. The presented vision-based navigation and detection algorithms were demonstrated with fully autonomous flight carried out during the competition IMAV 2014.},
	booktitle = {2015 {International} {Conference} on {Unmanned} {Aircraft} {Systems} ({ICUAS})},
	author = {Li, Kun and Liu, Peidong and Pang, Tao and Yang, Zhaolin and Chen, Ben M.},
	month = jun,
	year = {2015},
	keywords = {Buildings, Cameras, Estimation, Image color analysis, Navigation, Object detection, Surveillance},
	pages = {832--838},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\Q5NPKPWY\\Li et al. - 2015 - Development of an unmanned aerial vehicle for roof.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Jerem\\Zotero\\storage\\YRMRF9PW\\7152368.html:text/html},
}


@article{2041-210X.12973,
  title = {How Do You Find the Green Sheep? {{A}} Critical Review of the Use of Remotely Sensed Imagery to Detect and Count Animals},
  author = {Hollings, Tracey and Burgman, Mark and {van Andel}, Mary and Gilbert, Marius and Robinson, Timothy and Robinson, Andrew},
  year = {2018},
  volume = {9},
  pages = {881--892},
  doi = {10.1111/2041-210X.12973},
  abstract = {Abstract Animal abundance data are essential for endangered species conservation, tracking invasive species spread, biosecurity, agricultural applications and wildlife monitoring; however, obtaining abundance data are a perennial challenge. Recent improvements in the resolution of remotely sensed imagery, and image-processing tools and software have facilitated improvement of methods for the detection of individual, generally large-bodied animals. The potential to monitor and survey populations from remotely sensed imagery is an exciting new development in animal ecology. We review the methods used to analyse remotely sensed imagery for their potential to estimate the abundance of wild and domestic animal populations by directly detecting, identifying and counting individuals. Despite many illustrative studies using a variety of methods for detecting animals from remotely sensed imagery, it remains problematic in many situations. Studies that demonstrated reasonably high accuracy using automated and semi-automated techniques have been undertaken on small spatial scales relative to the geographical range of the species of interest and/or in homogenous environments such as sea ice. The major limitations are the relatively low accuracy of automated detection techniques across large spatial extents, false detections and the cost of high-resolution data. Future developments in the analysis of remotely sensed data for population surveys will improve detection capabilities, including the advancement of algorithms, the crossover of software and technology from other disciplines, and improved availability, accessibility, cost and resolution of data.},
  eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.12973},
  journal = {Methods in Ecology and Evolution},
  keywords = {agriculture,animals,detection,population survey,remote sensing,satellite imagery,wildlife},
  number = {4}
}


@article{SHAO2021126954,
  title = {Assessing City-Scale Green Roof Development Potential Using {{Unmanned Aerial Vehicle}} ({{UAV}}) Imagery},
  author = {Shao, Huamei and Song, Peihao and Mu, Bo and Tian, Guohang and Chen, Qian and He, Ruizhen and Kim, Gunwoo},
  year = {2021},
  volume = {57},
  pages = {126954},
  issn = {1618-8667},
  doi = {10.1016/j.ufug.2020.126954},
  abstract = {While green roofs have been deemed promising in mitigating environmental issues caused by rapid urban development, city-scale green roof studies have faced various obstacles, especially difficulties in obtaining accurate data for analysis. This study developed a new, cost-effective approach to assessing green roof development potential by using ultra-high-resolution (UHR) (0.09 m) Unmanned Aerial Vehicle (UAV) imagery in a case study site (Central Luohe with an area of 158 km2) in China. Specifically, the data was processed, interpreted, and classified to create highly accurate land-use and building roof spatial resources databases. A decision-making flowchart was developed for preliminary determination of a building stock's suitability for green roof implementation and the preferred type based on the five influencing factors and building roof classification. Subsequently, a two-stage strategy for large-scale green roof development was proposed. The approach demonstrated in this research greatly improves the accuracy of city-scale studies on roof spatial resources and enables better planning and development of urban green spaces at the local level.},
  journal = {Urban Forestry \& Urban Greening},
  keywords = {China,City-scale,Green roof,Green spaces,Roof spatial resources,Unmanned Aerial Vehicle (UAV)}
}


@incollection{Stansbury2015,
  title = {Technology Surveys and Regulatory Gap Analyses of {{UAS}} Subsystems toward Access to the {{NAS}}},
  booktitle = {Handbook of Unmanned Aerial Vehicles},
  author = {Stansbury, Richard S. and Wilson, Timothy A.},
  editor = {Valavanis, Kimon P. and Vachtsevanos, George J.},
  year = {2015},
  pages = {2293--2338},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-90-481-9707-1â‚†2},
  abstract = {To make a safe transition of UAS into the National Airspace System, new regulations must be developed by the Federal Aviation Administration. The technologies employed by UAS are in many circumstances fundamentally different than those of traditional manned aircraft. The regulations written to support the airworthiness certification, operations, maintenance, etc. of manned aircraft often do not apply as written without interpretation, revision, and/or deletion. This chapter provides the necessary details on how to conduct a technology survey and regulatory gap analysis of UAS technology subsystems. Four past studies performed by Embry-Riddle Aeronautical University for the FAA's William J. Hughes Technology Center are discussed. These studies address UAS propulsion systems, sense-and-avoid technologies and procedures, command control and communication, and emergency recovery and flight termination systems. Each study will be discussed in this chapter, and a recommended process for future studies is provided.},
  isbn = {978-90-481-9707-1}
}



@article{chen_integrated_2021,
  title = {Integrated {{Air}}-{{Ground Vehicles}} for {{UAV Emergency Landing}} Based on {{Graph Convolution Network}}},
  author = {Chen, Jie and Zhang, Yifan and Li, Jianqiang and Du, Weiming and Chen, Zhuangzhuang and Liu, Zun and Wang, Huihui and Leung, Victor C.M.},
  year = {2021},
  pages = {1--1},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2021.3058192},
  abstract = {With unmanned aerial vehicle (UAV) technologies advanced rapidly, many applications have emerged in cities. However, those applications do not widely spread as the safety consideration hinders the UAV from integrating into civilian environment. This work focuses on investigating the UAV emergency landing problem which is a critical safety functionality of UAV. This work proposed a Graph Convolution Networkbased decision network to learn by imitating the human pilots' landing strategy. To alleviate the needs of a large amount of real-world data for model training, the proposed model allows to be trained in a simulated environment and then transferred to the real-world scenario due to the separation of domain-specific terrain classes and domain-independent topological structures among down-looking camera images. The Graph Convolution Network-based decision network can be coupled with a topological heuristic to improve the performance of action prediction in emergency situation. To evaluate the proposed method, this work implemented a simulation environment for collecting data and testing the UAV emergency landing. The empirical results in both simulated and real-world scenario show that the proposed methods can outperform the state-of-the-art counterparts in terms of predictive accuracy and success landing rate.},
  journal = {IEEE Internet of Things Journal},
  keywords = {Cameras,Convolution,Data models,Emergency landing,Global Positioning System,Graph Convolution Network.,Internet of Things,Task analysis,Unmanned Aerial Vehicle,Unmanned aerial vehicles}
}


@inproceedings{Wu2018,
  title = {Towards a Consequences-Aware Emergency Landing System for Unmanned Aerial Systems},
  booktitle = {2018 International Conference on Unmanned Aircraft Systems ({{ICUAS}})},
  author = {Wu, Xiangyu and Mueller, Mark},
  year = {2018},
  month = jun,
  publisher = {{IEEE}},
  doi = {10.1109/icuas.2018.8453347}
}


@article{YuFeiShen2013,
  title = {A Vision-Based Automatic Safe Landing-Site Detection System},
  author = {Shen, Yu-Fei and Rahman, Z. and Krusienski, D. and Li, Jiang},
  year = {2013},
  month = jan,
  volume = {49},
  pages = {294--311},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/taes.2013.6404104},
  journal = {IEEE Transactions on Aerospace and Electronic Systems},
  number = {1}
}

@inbook{doi:10.2514/6.2016-1137,
author = {Justin Mackay and Gary Ellingson and Tim W. McLain},
title = {Landing Zone Determination for Autonomous Rotorcraft in Surveillance Applications},
booktitle = {AIAA Guidance, Navigation, and Control Conference},
chapter = {},
pages = {},
doi = {10.2514/6.2016-1137},
URL = {https://arc.aiaa.org/doi/abs/10.2514/6.2016-1137},
eprint = {https://arc.aiaa.org/doi/pdf/10.2514/6.2016-1137}
}


@inproceedings{papa_uas_2018,
  title = {{{UAS Aided Landing}} and {{Obstacle Detection Through LIDAR}}-{{Sonar}} Data},
  booktitle = {2018 5th {{IEEE International Workshop}} on {{Metrology}} for {{AeroSpace}} ({{MetroAeroSpace}})},
  author = {Papa, Umberto and Ariante, Gennaro and Del Core, Giuseppe},
  year = {2018},
  month = jun,
  pages = {478--483},
  issn = {2575-7490},
  doi = {10.1109/MetroAeroSpace.2018.8453594},
  abstract = {Unmanned Aircraft Systems (UASs) have gained large consensus during the last decade, due to their versatility in military and civilian tasks. For some operations, it is necessary to have a good sensor embedded platform that allows navigation in unknown environment. This paper shows the use of low-cost sensors fusion for obstacle detection and avoidance: Sonic ranger (SR) and Light detection and ranging (LIDAR). Further sensors are also installed on the UAS (e.g. ADXL335 3-axis accelerometer), in order to have redundant information for the relative position evaluation. Simulation and experimental results are provided to determine feasibility of the proposed approach to detection and avoidance applications on a small rotary wing UAS.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\83NAPCUF\\Papa et al. - 2018 - UAS Aided Landing and Obstacle Detection Through L.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\M4JPCJ69\\8453594.html},
  keywords = {Accelerometers,Aided Landing,Data collection,Distance measurement,Laser radar,LIDAR,Obstacle Detection,Sensors Fusion,SR,Temperature measurement,Temperature sensors,UAS,Unmanned aerial vehicles}
}



@inproceedings{romano_experimental_2019,
  title = {Experimental {{Evaluation}} of {{Continuum Deformation}} with a {{Five Quadrotor Team}}},
  booktitle = {2019 {{American Control Conference}} ({{ACC}})},
  author = {Romano, Matthew and Kuevor, Prince and Lukacs, Derek and Marshall, Owen and Stevens, Mia and Rastgoftar, Hossein and Cutler, James and Atkins, Ella},
  year = {2019},
  month = jul,
  pages = {2023--2029},
  issn = {2378-5861},
  doi = {10.23919/ACC.2019.8815266},
  abstract = {This paper experimentally evaluates continuum deformation cooperative control for the first time. Theoretical results are expanded to place a bounding triangle on the leader-follower system such that the team is contained despite nontrivial tracking error. Flight tests were conducted with custom quadrotors running a modified version of ArduPilot on a BeagleBone Blue in M-Air, an outdoor netted flight facility. Motion capture and an onboard inertial measurement unit were used for state estimation. Position error was characterized in single vehicle tests using quintic spline trajectories and different reference velocities. Five-quadrotor leader trajectories were generated, and followers executed the continuum deformation control law in-flight. Flight tests successfully demonstrated continuum deformation; future work in characterizing error propagation from leaders to followers is discussed.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\4KXNISQY\\Romano et al. - 2019 - Experimental Evaluation of Continuum Deformation w.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\SKEHZIFK\\8815266.html},
  keywords = {Collision avoidance,Nickel,Propellers,Strain,Tracking,Trajectory,Two dimensional displays}
}



@inproceedings{ouerghi:hal-02567816,
  title = {Comparative {{Study}} of a Commercial Tracking Camera and {{ORB}}-{{SLAM2}} for Person Localization},
  booktitle = {15th International Conference on Computer Vision Theory and Applications},
  author = {Ouerghi, Safa and Ragot, Nicolas and Boutteau, R{\'e}mi and Savatier, Xavier},
  year = {2020},
  month = feb,
  pages = {357--364},
  publisher = {{SCITEPRESS - Science and Technology Publications}},
  address = {{Valletta, France}},
  doi = {10.5220/0008980703570364},
  hal_id = {hal-02567816},
  hal_version = {v1},
  keywords = {benchmarking,Intel T265,ORB-SLAM2,person localization},
  pdf = {https://hal.archives-ouvertes.fr/hal-02567816/file/VISAPP2020.pdf}
}


@inproceedings{10.1007/978-3-030-70740-8_14,
  title = {Vision-Based Localization for Multi-Rotor Aerial Vehicle in Outdoor Scenarios},
  booktitle = {Modelling and Simulation for Autonomous Systems},
  author = {Bayer, Jan and Faigl, Jan},
  editor = {Mazal, Jan and Fagiolini, Adriano and Vasik, Petr and Turi, Michele},
  year = {2021},
  pages = {217--228},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  abstract = {In this paper, we report on the experimental evaluation of the embedded visual localization system, the Intel RealSense T265, deployed on a multi-rotor unmanned aerial vehicle. The performed evaluation is targeted to examine the limits of the localization system and discover its weak points. The system has been deployed in outdoor rural scenarios at altitudes up to 20 m. The Absolute trajectory error measures the accuracy of the localization with the reference provided by the differential GPS with centimeter precision. Besides, the localization performance is compared to the state-of-the-art feature-based visual localization ORB-SLAM2 utilizing the Intel RealSense D435 depth camera. In both types of experimental scenarios, with the teleoperated and autonomous vehicle, the identified weak point of the system is a translation drift. However, taking into account all experimental trials, both examined localization systems provide competitive results.},
  isbn = {978-3-030-70740-8}
}



@inproceedings{zhang_tutorial_2018,
  title = {A {{Tutorial}} on {{Quantitative Trajectory Evaluation}} for {{Visual}}(-{{Inertial}}) {{Odometry}}},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Zhang, Zichao and Scaramuzza, Davide},
  year = {2018},
  month = oct,
  pages = {7244--7251},
  issn = {2153-0866},
  doi = {10.1109/IROS.2018.8593941},
  abstract = {In this tutorial, we provide principled methods to quantitatively evaluate the quality of an estimated trajectory from visual(-inertial) odometry (VO/VIO), which is the foundation of benchmarking the accuracy of different algorithms. First, we show how to determine the transformation type to use in trajectory alignment based on the specific sensing modality (i.e., monocular, stereo and visual-inertial). Second, we describe commonly used error metrics (i.e., the absolute trajectory error and the relative error) and their strengths and weaknesses. To make the methodology presented for VO/VIO applicable to other setups, we also generalize our formulation to any given sensing modality. To facilitate the reproducibility of related research, we publicly release our implementation of the methods described in this tutorial.},
  file = {C\:\\Users\\Jeremy\\Zotero\\storage\\N6PG58J6\\Zhang and Scaramuzza - 2018 - A Tutorial on Quantitative Trajectory Evaluation f.pdf;C\:\\Users\\Jeremy\\Zotero\\storage\\PV3GXTL6\\8593941.html},
  keywords = {Cameras,Noise measurement,Sensors,Trajectory,Tutorials,Visualization}
}

@manual{nxp:tja1043,
    organization  = "Intel",
    title         = "Intel\textsuperscript{\textregistered} RealSense\textsuperscript{TM} LiDAR Camera L515 Datasheet",
    year          =  2020,
    month         =  6,    
    note          = "Rev. 002. [Online] Available: \url{https://dev.intelrealsense.com/docs/lidar-camera-l515-datasheet}",
    howpublished = {}
}

@misc{realsense_github_t265,
  author={Intel},
  title = {Github - {{Intel RealSense T265 Tracking Camera Documentation}}},
  year = {2020},
  howpublished = {[Online] Available: \url{https://github.com/IntelRealSense/librealsense/blob/master/doc/t265.md}},
  note = {Accessed: 2021-06-05}
}



@inproceedings{whalley2009field,
  title={Field-testing of a helicopter UAV obstacle field navigation and landing system},
  author={Whalley, Matt and Takahashi, Marc and Tsenkov, P and Schulein, G and Goerzen, C},
  booktitle={65th Annual Forum of the American Helicopter Society, Grapevine, TX},
  year={2009}
}


@manual{ouster_os0,
    organization  = "Ouster",
    title         = "OS0 - Ultra-Wide View High-Resolution Imaging Lidar",
    year          =  2020,
    month         =  2,    
    note          = "Rev. 02/23/20220 [Online] Available: \url{https://data.ouster.io/downloads/OS0-lidar-sensor-datasheet.pdf}",
    howpublished = {}
}


@inproceedings{chitanvis_collision_2019,
	title = {Collision avoidance and {Drone} surveillance using {Thread} protocol in {V2V} and {V2I} communications},
	doi = {10.1109/NAECON46414.2019.9058170},
	abstract = {According to the World Health Organizations (WHO) report nearly 1.25 million people die in road accidents every year. This creates a need for Advanced Driver Assist Systems (ADAS) which can ensure safe travel. To tackle the above challenge in existing the ADAS, Intra-vehicular communications (V2V) and vehicle to infrastructure communications (V2I) has been one of the predominant research topics nowadays due to the rapid growth of automobile industries and ideology of producing autonomous cars in the near future. The key feature of V2V communication is vehicle to vehicle collision detection by transmitting information like vehicle speed and position of a vehicle to other vehicles in the same location using wireless sensor networks (WSN). On the other hand, Unmanned Aerial Vehicle (UAV) systems are growing at a rapid rate in various aspects of life including dispatch of medicines and undergo video surveillance during an emergency due to less air traffic. This paper demonstrates the practice of integrating V2V communication with Thread, one of the low power WSN for data transmission, to initiate adaptive cruise control in a vehicle during a crisis. Also, UAV systems are employed as a part of V2I system to provide aerial view video surveillance if any accident occurs.},
	booktitle = {2019 {IEEE} {National} {Aerospace} and {Electronics} {Conference} ({NAECON})},
	author = {Chitanvis, Rajas and Ravi, Niranjan and Zantye, Tanmay and El-Sharkawy, Mohamed},
	month = jul,
	year = {2019},
	note = {ISSN: 2379-2027},
	keywords = {Cloud computing, IBM Watson, IEEE 802.15.4, Instruction sets, MBDT, Mesh networks, Message systems, Protocols, PX4, Thread, UAV, V2I, V2V, Wireless communication, Wireless sensor networks},
	pages = {406--411},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\RI4XFV3Q\\Chitanvis et al. - 2019 - Collision avoidance and Drone surveillance using T.pdf:application/pdf},
}


@inproceedings {raft_concensus,
author = {Diego Ongaro and John Ousterhout},
title = {In Search of an Understandable Consensus Algorithm},
booktitle = {2014 {USENIX} Annual Technical Conference ({USENIX} {ATC} 14)},
year = {2014},
isbn = {978-1-931971-10-2},
address = {Philadelphia, PA},
pages = {305--319},
url = {https://www.usenix.org/conference/atc14/technical-sessions/presentation/ongaro},
publisher = {{USENIX} Association},
month = jun,
}



@article{molano_finding_2012,
	title = {Finding the largest area rectangle of arbitrary orientation in a closed contour},
	volume = {218},
	issn = {0096-3003},
	url = {https://www.sciencedirect.com/science/article/pii/S0096300312003207},
	doi = {10.1016/j.amc.2012.03.063},
	abstract = {For many software applications, it is sometimes necessary to find the rectangle of largest area inscribed in a polygon, in any possible direction. Thus, given a closed contour C, we consider approximation algorithms for the problem of finding the largest area rectangle of arbitrary orientation that is fully contained in C. Furthermore, we compute the largest area rectangle of arbitrary orientation in a quasi-lattice polygon, which models the C contour. In this paper, we propose an approximation algorithm that solves this problem with an O(n3) computational cost, where n is the number of vertices of the polygon. There is no other algorithm having lower computational complexity regardless of any constraints. In addition, we have developed a web application that uses the proposed algorithm.},
	language = {en},
	number = {19},
	urldate = {2021-07-17},
	journal = {Applied Mathematics and Computation},
	author = {Molano, RubÃ©n and RodrÃ­guez, Pablo G. and Caro, AndrÃ©s and DurÃ¡n, M. Luisa},
	month = jun,
	year = {2012},
	keywords = {Polygon, Rectangle, Region of Interest (ROI)},
	pages = {9866--9874},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\AKMWWB3Q\\Molano et al. - 2012 - Finding the largest area rectangle of arbitrary or.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Jerem\\Zotero\\storage\\8LL5D9HZ\\S0096300312003207.html:text/html},
}

@book{docherty2012losing,
  title={{Losing humanity: The case against killer robots}},
  publisher={Human Rights Watch},
  author={Docherty, Bonnie},
  year={2012}
}



@inproceedings{gutzwiller_human-computer_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Human-{Computer} {Collaboration} in {Adaptive} {Supervisory} {Control} and {Function} {Allocation} of {Autonomous} {System} {Teams}},
	isbn = {978-3-319-21067-4},
	doi = {10.1007/978-3-319-21067-4_46},
	abstract = {The foundation for a collaborative, man-machine system for adaptive performance of tasks in a multiple, heterogeneous unmanned system teaming environment is discussed. An autonomics system is proposed to monitor missions and overall system attributes, including those of the operator, autonomy, states of the world, and the mission. These variables are compared within a model of the global system, and strategies that re-allocate tasks can be executed based on a mission-health perspective (such as relieving an overloaded user by taking over incoming tasks). Operators still have control over the allocation via a task manager, which also provides a function allocation interface, and accomplishes an initial attempt at transparency. We plan to learn about configurations of function allocation from human-in-the-loop experiments, using machine learning and operator feedback. Integrating autonomics, machine learning, and operator feedback is expected to improve collaboration, transparency, and human-machine performance.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Gutzwiller, Robert S. and Lange, Douglas S. and Reeder, John and Morris, Rob L. and Rodas, Olinda},
	editor = {Shumaker, Randall and Lackey, Stephanie},
	year = {2015},
	keywords = {Autonomics, Autonomous systems, Supervisory control, Task models},
	pages = {447--456},
}


@article{saltzer_origin_2020,
	title = {The {Origin} of the â€œ{MIT} {License}â€},
	volume = {42},
	issn = {1934-1547},
	url = {https://muse.jhu.edu/article/786273},
	number = {4},
	urldate = {2021-07-23},
	journal = {IEEE Annals of the History of Computing},
	author = {Saltzer, Jerome H.},
	year = {2020},
	note = {Publisher: IEEE Computer Society},
	pages = {94--98},
	file = {Full Text PDF:C\:\\Users\\Jerem\\Zotero\\storage\\EEIMP3SB\\Saltzer - 2020 - The Origin of the â€œMIT Licenseâ€.pdf:application/pdf},
}


















































